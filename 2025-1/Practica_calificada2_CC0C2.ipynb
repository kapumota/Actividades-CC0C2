{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c3fbffa-bfc7-4133-a358-2464054258b6",
   "metadata": {},
   "source": [
    "## **Práctica calificada 2 CC0C2**\n",
    "\n",
    "#### **Indicaciones generales**\n",
    "\n",
    "Cada estudiante debe:\n",
    "1. Elegir **uno de los proyectos propuestos (no se admite proyectos repetidos)** y registrar su elección. **Fecha de entrega 24 de mayo hasta las 14:00 PM** \n",
    "2. Crear un **repositorio público en GitHub** con el nombre `nlp-proyectoXX`, donde `XX` es el número del proyecto.\n",
    "3. Actualizar dicho repositorio diariamente (mínimo un `commit` con mensaje descriptivo) con:\n",
    "   - Código fuente\n",
    "   - Notebooks de prueba\n",
    "   - Métricas intermedias\n",
    "   - Registro de decisiones técnicas\n",
    "4. Presentar el trabajo en la **sesión final obligatoria de exposición**, en la que se evaluará:\n",
    "   - Entendimiento técnico\n",
    "   - Justificación de decisiones\n",
    "   - Análisis de resultados\n",
    "   - Calidad de visualización y comunicación\n",
    "\n",
    "\n",
    "#### **Cronograma sugerido de avance**\n",
    "\n",
    "| Día | Actividades esperadas | Verificable en GitHub |\n",
    "|-----|------------------------|------------------------|\n",
    "| 1 | Registro del proyecto elegido y estructura base del repositorio | `README.md`, carpeta `src/` y `notebooks/`, primer commit |\n",
    "| 2 | Primer prototipo o pruebas preliminares con datos | código de carga y procesado inicial |\n",
    "| 3 | Implementación parcial de componentes clave | archivo funcional + evidencia de ejecución |\n",
    "| 4 | Primeros resultados / métricas básicas | gráficos, tablas de salida, resultados intermedios |\n",
    "| 5 | Feedback intermedio (revisión cruzada) entre equipos | issues/comentarios cruzados en GitHub |\n",
    "| 6 | Mejoras / refactorización / experimentos adicionales | scripts refactorizados y limpieza de logs |\n",
    "| 7 | Documentación de decisiones, generación de reportes | notebooks explicativos o `doc/` con comparativas |\n",
    "| 8 | Implementación final y validación de resultados | ejecución reproducible + versión final `main.ipynb` |\n",
    "| 9 | Preparación de presentación (slides, notebook narrado) | `presentation.ipynb` o `slides.pdf` |\n",
    "| 10 | **Presentación en clase obligatoria** | defensa oral, sesión de preguntas, revisión del repo |\n",
    "\n",
    "\n",
    "#### **Criterios de evaluación**\n",
    "\n",
    "| Criterio | Peso (%) |\n",
    "|---------|----------|\n",
    "| **Presentación oral** (defensa técnica, comprensión profunda, dominio del contenido) | **60%** |\n",
    "| **Repositorio de GitHub** (progreso continuo, calidad del código, documentación) | 30% |\n",
    "| **Presentación del trabajo final** (visualización de resultados, storytelling técnico) | 10% |\n",
    "\n",
    "> **Importante:** La presentación del trabajo **no se puede reemplazar**. Si el estudiante no se presenta o presenta un trabajo no comprendido, la calificación será de  **0**. Si se presenta un trabajo al final sin mostrar seguimiento continuo, el puntaje por el repositorio de trabajo sera de 0%. \n",
    "\n",
    "#### **Requisitos del repositorio**\n",
    "\n",
    "Debe incluir al menos:\n",
    "- `README.md` con descripción clara del proyecto, dataset y enfoque\n",
    "- Carpeta `src/` con los módulos desarrollados\n",
    "- Carpeta `notebooks/` con pruebas y visualizaciones\n",
    "- Archivo `requirements.txt` o `environment.yml`\n",
    "- Comandos reproducibles (Makefile o instrucciones en README)\n",
    "- GitHub Actions (opcional, para pruebas automáticas o linting)\n",
    "\n",
    "\n",
    "#### **Sugerencia adicional**\n",
    "\n",
    "Antes de la entrega final:\n",
    "- Se recomienda usar `issues` o `projects` de GitHub para organizar tus tareas \n",
    "- Usar ramas (`feature/`, `fix/`, `test/`) y _pull requests_ como parte de buenas prácticas\n",
    "- Añadir métricas reproducibles (BLEU, ROUGE, Perplexity, etc.) con graficación\n",
    "\n",
    "\n",
    "#### **Rúbrica de evaluación oral individual – Proyecto de NLP**\n",
    "\n",
    "| Criterio | Excelente (4 pts) | Bueno (3 pts) | Regular (2 pts) | Insuficiente (1 pt) | Puntaje |\n",
    "|---------|------------------|----------------|------------------|----------------------|--------|\n",
    "| **1. Dominio del contenido técnico** | Explica con precisión, seguridad y profundidad todos los componentes del proyecto, incluyendo algoritmos, métricas y decisiones clave | Explica adecuadamente los aspectos principales, aunque con algunos vacíos de detalle | Muestra comprensión limitada; omite o malinterpreta partes importantes del trabajo | Evidencia poco o nulo conocimiento del proyecto; depende de leer el código sin entenderlo |      /4 |\n",
    "| **2. Razonamiento y justificación técnica** | Justifica cada decisión con base en teoría o experimentación (hiperparámetros, modelos, técnicas de NLP, etc.) | Justifica la mayoría de las decisiones, aunque algunas sin evidencia concreta | Justifica parcialmente, con argumentos débiles o vagos | No justifica sus decisiones o no entiende por qué eligió un enfoque |      /4 |\n",
    "| **3. Presentación de resultados** | Presenta métricas con análisis crítico, señala ventajas, limitaciones y propone mejoras | Muestra resultados claros, aunque con análisis superficial o sin propuestas | Resultados vagos o sin explicación; dificultad para vincularlos con el objetivo | No puede explicar los resultados o sólo los menciona sin contexto |      /4 |\n",
    "| **4. Claridad y expresión oral** | Habla con fluidez, usa terminología técnica adecuada, mantiene coherencia y estructura lógica | Lenguaje claro en general, aunque con momentos de duda o tecnicismos mal empleados | Discurso poco claro, uso limitado del lenguaje técnico o desorganización al hablar | Lenguaje confuso o desordenado; no se logra comprender el enfoque del proyecto |      /4 |\n",
    "| **5. Recursos visuales y preparación** | Uso excelente de visualizaciones, notebooks, slides u otros medios que refuerzan la exposición; excelente preparación | Usa recursos adecuados para apoyar su exposición, aunque con limitaciones en claridad o estructura | Presenta recursos poco organizados o insuficientes para complementar la explicación | No presenta recursos de apoyo o usa código sin formato, sin estructura ni explicación |      /4 |\n",
    "\n",
    "#### **Total: /20 puntos**\n",
    "  \n",
    "> El plagio, falta de exposición o presentación de código no entendido anula la evaluación (**nota: 0**).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581624cd-ed94-4e63-a796-b914b47b4030",
   "metadata": {},
   "source": [
    "\n",
    "#### **Proyecto 1. Representaciones de texto y embeddings contextuales**\n",
    "\n",
    "Este proyecto se centra en comprender y experimentar con la evolución de las representaciones de texto, desde embeddings estáticos tradicionales hasta las representaciones contextuales basadas en Transformers.\n",
    "\n",
    "* **Objetivos de aprendizaje**\n",
    "\n",
    "  * Comparar embeddings estáticos (word2vec, GloVe) con embeddings contextualizados (ELMo, BERT).\n",
    "  * Visualizar y analizar la geometría del espacio latente.\n",
    "  * Evaluar métricas de similitud semántica y polisemia.\n",
    "\n",
    "* **Algoritmos y técnicas**\n",
    "\n",
    "  * **Skip-gram y CBOW** (Mikolov et al., 2013):\n",
    "\n",
    "    $$\n",
    "      \\max_{\\theta} \\sum_{t=1}^{T} \\sum_{-c \\le j \\le c,\\,j\\ne0}\\log p(w_{t+j}\\mid w_t;\\theta),\n",
    "    $$\n",
    "\n",
    "    donde\n",
    "\n",
    "    $$\n",
    "      p(w_{O}\\mid w_I) = \\frac{\\exp\\bigl(u_{w_O}^\\top v_{w_I}\\bigr)}\n",
    "                         {\\sum_{w=1}^{V}\\exp\\bigl(u_w^\\top v_{w_I}\\bigr)}.\n",
    "    $$\n",
    "  * **GloVe** (Pennington et al., 2014): optimiza la función de costo\n",
    "\n",
    "    $$\n",
    "      J = \\sum_{i,j=1}^V f(X_{ij})\\bigl(w_i^\\top \\tilde w_j + b_i + \\tilde b_j - \\log X_{ij}\\bigr)^2,\n",
    "    $$\n",
    "\n",
    "    con $X_{ij}$ la co-ocurrencia y una función de ponderación $f$.\n",
    "  * **Embeddings contextuales**:\n",
    "\n",
    "    * **ELMo**: utiliza un modelo bidireccional LSTM de múltiples capas. Las representaciones contextuales se obtienen como combinaciones lineales de los estados internos del LSTM.\n",
    "    * **BERT** (Devlin et al., 2019): pre-entrenamiento con **MLM** (Masked Language Modeling) y **NSP** (Next Sentence Prediction).\n",
    "\n",
    "* **Actividades prácticas**\n",
    "\n",
    "  1. **Entrenamiento de embeddings estáticos**\n",
    "\n",
    "     * Tomar un corpus reducido (e.g., Wikipedia en español, 10 M tokens).\n",
    "     * Entrenar modelos Skip-gram y CBOW con ventana $c=5$, dimensión $d=300$, usando negative sampling con $k=5$.\n",
    "     * Evaluar analogías: \"rey - hombre + mujer ≈ reina\".\n",
    "  2. **Extracción de embeddings de BERT**\n",
    "\n",
    "     * Usar una librería como Hugging Face Transformers para obtener vectores de una capa intermedia.\n",
    "     * Aplicar **t-SNE** o **UMAP** para reducir a 2D-3D.\n",
    "     * Proyectar pares de frases: sinónimas vs. antónimas, medir distancias euclídeas y coseno.\n",
    "  3. **Medición de polisemia**\n",
    "\n",
    "     * Seleccionar palabras con múltiples significados (\"banco\", \"caja\").\n",
    "     * Clustering de los embeddings contextuales para cada ocurrencia; calcular *silhouette score*.\n",
    "  4. **Análisis crítico**\n",
    "\n",
    "     * Comparar correlación de similitud con datasets de evaluación semántica (p.ej., SimLex-999).\n",
    "     * Discutir ventajas y limitaciones de cada enfoque.\n",
    "\n",
    "Este proyecto concluye con un informe detallado y un repositorio de código que documente los parámetros, los resultados y una breve discusión sobre la interpretabilidad del espacio latente.\n",
    "\n",
    "\n",
    "#### **Proyecto 2. Backpropagation Through Time y Modelos Seq2Seq**\n",
    "\n",
    "En este proyecto se profundiza en los algoritmos de entrenamiento de redes recurrentes y su aplicación en tareas de secuencia a secuencia (seq2seq), enfatizando los problemas de gradientes y las técnicas de regularización.\n",
    "\n",
    "* **Objetivos de aprendizaje**\n",
    "\n",
    "  * Comprender el algoritmo BPTT y sus variantes (truncamiento, grad-clipping).\n",
    "  * Implementar modelos seq2seq básicos con LSTM/GRU.\n",
    "  * Evaluar el impacto de *teacher forcing* y *exposure bias*.\n",
    "\n",
    "* **Algoritmos y ecuaciones clave**\n",
    "\n",
    "  * **RNN básico**:\n",
    "\n",
    "    $$\n",
    "      h_t = f\\bigl(U x_t + V h_{t-1}\\bigr),\\quad y_t = g\\bigl(W h_t\\bigr).\n",
    "    $$\n",
    "  * **Backpropagation Through Time**: derivadas de la pérdida $\\mathcal{L}$ respecto a pesos $V$:\n",
    "\n",
    "    $$\n",
    "      \\frac{\\partial \\mathcal{L}}{\\partial V}\n",
    "      = \\sum_{t=1}^T \\sum_{k=1}^t \\frac{\\partial \\mathcal{L}}{\\partial h_t}\n",
    "        \\Bigl(\\prod_{j=k+1}^t \\mathrm{diag}\\bigl(f'(r_j)\\bigr)V\\Bigr)\n",
    "        f'(r_k) h_{k-1}^\\top.\n",
    "    $$\n",
    "  * **Clipping de gradientes**:\n",
    "\n",
    "    $$\n",
    "      \\nabla \\leftarrow \\frac{\\phi}{\\|\\nabla\\|_2}\\nabla,\\quad\n",
    "      \\text{si }\\|\\nabla\\|_2 > \\phi.\n",
    "    $$\n",
    "  * **Seq2Seq con atención** (Bahdanau):\n",
    "\n",
    "    $$\n",
    "      e_{jt} = v_a^\\top \\tanh\\bigl(W_a s_{j-1} + U_a h_t\\bigr),\\quad\n",
    "      \\alpha_{jt} = \\frac{\\exp(e_{jt})}{\\sum_{k=1}^T \\exp(e_{jk})},\\quad\n",
    "      c_j = \\sum_{t=1}^T \\alpha_{jt} h_t.\n",
    "    $$\n",
    "\n",
    "* **Actividades prácticas**\n",
    "\n",
    "  1. **Implementación básica**\n",
    "\n",
    "     * En PyTorch, crear un encoder-decoder sin atención.\n",
    "     * Dataset toy: pares de secuencias numéricas (p.ej., inversión de listas).\n",
    "     * Entrenar con *teacher forcing* $p=1$ y sin él ($p=0$), comparar *loss* y calidad.\n",
    "  2. **Truncamiento de BPTT**\n",
    "\n",
    "     * Entrenar RNN con secuencias largas ($T=100$), truncamiento en ventanas de 20 pasos.\n",
    "     * Medir tiempos de entrenamiento y convergencia.\n",
    "  3. **Regularización**\n",
    "\n",
    "     * Aplicar *dropout* recurrente (Gal & Ghahramani, 2016).\n",
    "     * Comparar modelos con y sin dropout en términos de *perplexity*.\n",
    "  4. **Análisis de *exposure bias***\n",
    "\n",
    "     * Generar secuencias en inferencia con *greedy decoding*.\n",
    "     * Medir divergencia contra secuencias de referencia (BLEU, edit distance).\n",
    "\n",
    "Los resultados se documentan en un cuaderno Jupyter que incluya gráficas de pérdida, ejemplos de secuencias generadas y análisis crítico del comportamiento de gradientes.\n",
    "\n",
    "\n",
    "#### **Proyecto 3. Estrategias de decodificación avanzadas**\n",
    "\n",
    "Este proyecto explora a fondo las técnicas de generación de secuencia más allá del método más probable (greedy), enfocándose en beam search, muestreo probabilístico y penalizaciones para controlar longitud y repetición.\n",
    "\n",
    "* **Objetivos de aprendizaje**\n",
    "\n",
    "  * Dominar algoritmos de búsqueda y muestreo para decodificación.\n",
    "  * Implementar y comparar beam search vs. sampling con temperatura, top-k y nucleus sampling.\n",
    "  * Diseñar métricas para evaluar diversidad, coherencia y repetición.\n",
    "\n",
    "* **Algoritmos y fórmulas**\n",
    "\n",
    "  * **Beam Search**: mantenemos el conjunto $B_j$ de $k$ hipótesis con mayor puntuación hasta el paso $j$. Para cada hipótesis $h$, extendemos con todos los posibles siguientes tokens $w$ y computamos\n",
    "\n",
    "    $$\n",
    "      \\text{score}(h \\circ w) = \\frac{\\log p(w\\mid h)}{(j+1)^\\alpha},\n",
    "    $$\n",
    "\n",
    "    donde $\\alpha$ controla **penalización de longitud** (Wu et al., 2016).\n",
    "  * **Muestreo con temperatura**: dadas probabilidades $p_i$, ajustamos\n",
    "\n",
    "    $$\n",
    "      p_i' = \\frac{\\exp\\bigl(\\log p_i / T\\bigr)}{\\sum_j \\exp\\bigl(\\log p_j / T\\bigr)}.\n",
    "    $$\n",
    "\n",
    "    * $T<1$: distribución más aguda (cercana a greedy).\n",
    "    * $T>1$: más plana y diversa.\n",
    "  * **Top-k sampling**: se seleccionan los $k$ tokens con mayores $p_i$ y se normalizan.\n",
    "  * **Nucleus (top-p) sampling**: elegimos el conjunto mínimo $S$ tal que\n",
    "    $\\sum_{i\\in S} p_i \\ge p$, luego normalizamos sobre $S$.\n",
    "  * **Penalización de repetición**: rescala la probabilidad de tokens ya generados:\n",
    "\n",
    "    $$\n",
    "      p_i \\leftarrow \\frac{p_i}{(1 + \\beta \\cdot \\text{count}(i))},\n",
    "    $$\n",
    "\n",
    "    donde $\\beta>0$.\n",
    "\n",
    "* **Actividades prácticas**\n",
    "\n",
    "  1. **Implementación de beam search**\n",
    "\n",
    "     * Codificar algoritmo con $\\alpha=0.7$, beam width $k=\\{3,5,10\\}$.\n",
    "     * Evaluar en tarea de summarization sobre CNN/DailyMail (subconjunto).\n",
    "     * Medir ROUGE-1,2,L.\n",
    "  2. **Experimentos de sampling**\n",
    "\n",
    "     * Generar texto creativo con $T=\\{0.7,1.0,1.2\\}$, $k=\\{40,100\\}$, $p=\\{0.8,0.9\\}$.\n",
    "     * Calcular diversidad de n-gramas y repetición.\n",
    "  3. **Diseño de métricas**\n",
    "\n",
    "     * Índice de repetición $R = \\frac{\\#\\text{tokens repetidos}}{\\text{longitud total}}$.\n",
    "     * Coherencia semántica vía embeddings: promediar similitud coseno entre oraciones consecutivas.\n",
    "  4. **Análisis crítico**\n",
    "\n",
    "     * Comparar velocidad y calidad entre beam search y sampling.\n",
    "     * Discutir trade-offs en aplicaciones de diálogo vs. summarization.\n",
    "\n",
    "El informe final incluirá tablas comparativas, gráficas de diversidad y ejemplos de salidas para cada configuración.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e089bf5a-414c-4aff-9118-d457b6f5bc8c",
   "metadata": {},
   "source": [
    "#### **Proyecto 4. Familia de mecanismos de atención**\n",
    "\n",
    "En este módulo se detallan las variantes principales de mecanismos de atención, su formulación matemática y su implementación práctica en modelos seq2seq.\n",
    "\n",
    "* **Objetivos de aprendizaje**\n",
    "\n",
    "  * Comprender diferencias entre atención aditiva, basada en contenido y dot-product (normalizada vs. escalada).\n",
    "  * Implementar cada variante y medir su impacto en rendimiento y eficiencia.\n",
    "\n",
    "* **Variantes y fórmulas**\n",
    "\n",
    "  1. **Additive attention** (Bahdanau et al., 2015):\n",
    "\n",
    "     $$\n",
    "       e_{jt} = v_a^\\top \\tanh\\bigl(W_a s_{j-1} + U_a h_t\\bigr),\\quad\n",
    "       \\alpha_{jt} = \\frac{\\exp(e_{jt})}{\\sum_{k=1}^T \\exp(e_{jk})}.\n",
    "     $$\n",
    "  2. **Dot-product attention** (Luong et al., 2015):\n",
    "\n",
    "     $$\n",
    "       e_{jt} = s_{j-1}^\\top h_t,\\quad \\alpha_{jt} = \\frac{\\exp(e_{jt})}{\\sum_{k=1}^T \\exp(e_{jk})}.\n",
    "     $$\n",
    "  3. **Scaled dot-product** (Vaswani et al., 2017):\n",
    "\n",
    "     $$\n",
    "       e_{jt} = \\frac{s_{j-1}^\\top h_t}{\\sqrt{d_h}},\\quad \\alpha_{jt} = \\operatorname{softmax}(e_{j1},\\dots,e_{jT}).\n",
    "     $$\n",
    "  4. **Location-based attention**: emplea información de posición previa para guiar el score, p.ej., convoluciones unidimensionales sobre $\\alpha_{j-1}$.\n",
    "  5. **General** (Luong):\n",
    "\n",
    "     $$\n",
    "       e_{jt} = s_{j-1}^\\top W_a h_t.\n",
    "     $$\n",
    "  6. **Content-based**: se basa únicamente en similitud entre consultas y claves.\n",
    "\n",
    "* **Actividades prácticas**\n",
    "\n",
    "  1. **Implementación desde cero**\n",
    "\n",
    "     * En un toy-seq2seq (e.g., suma de dígitos), codificar las cuatro variantes.\n",
    "     * Mantener constantes el encoder/decoder y evaluar solo la atención.\n",
    "  2. **Visualización de mapas de atención**\n",
    "\n",
    "     * Para oraciones reales (traducir inglés->español), generar heatmaps de $\\alpha_{jt}$.\n",
    "     * Analizar alineaciones correctas vs. errores.\n",
    "  3. **Medición de rendimiento**\n",
    "\n",
    "     * Comparar tiempo de cómputo y memoria en cada variante.\n",
    "     * Evaluar calidad de traducción (BLEU) sobre IWSLT’14 small.\n",
    "  4. **Extensiones**\n",
    "\n",
    "     * Atenciones locales: restringir sumatoria de $k$ vectores alrededor de la posición central.\n",
    "     * Multi-head attention: combinar varias atenciones escaladas.\n",
    "\n",
    "El entregable es un notebook detallado con código modular, heatmaps interpretables y gráficas comparativas de BLEU y latencia.\n",
    "\n",
    "#### **Proyecto 5. Pre-entrenamiento autodidacta y fine-tuning**\n",
    "\n",
    "Este módulo aborda estrategias de pre-entrenamiento masivo y adaptación eficiente a tareas específicas, con técnicas de parameter-efficient fine-tuning.\n",
    "\n",
    "* **Objetivos de aprendizaje**\n",
    "\n",
    "  * Comprender objetivos de pre-entrenamiento: MLM, NSP, cross-entropy.\n",
    "  * Implementar fine-tuning full vs. adapters/LoRA.\n",
    "  * Medir trade-off entre número de parámetros actualizados y desempeño.\n",
    "\n",
    "* **Algoritmos y objetivos**\n",
    "\n",
    "  1. **MLM**:\n",
    "\n",
    "     $$\n",
    "       \\mathcal{L}_{\\mathrm{MLM}} = -\\sum_{i\\in M}\\log p(x_i \\mid \\widetilde{x}),\n",
    "     $$\n",
    "\n",
    "     donde $\\widetilde{x}$ es la secuencia con tokens enmascarados en $M$.\n",
    "  2. **NSP**:\n",
    "\n",
    "     $$\n",
    "       \\mathcal{L}_{\\mathrm{NSP}} = -\\bigl[y\\log p(y\\!\\mid x_1,x_2) + (1-y)\\log(1-p(y\\!\\mid x_1,x_2))\\bigr].\n",
    "     $$\n",
    "  3. **Adapters** (Houlsby et al., 2019): insertar capas pequeñas en cada bloque, entrenar solo esas:\n",
    "\n",
    "     $$\n",
    "       \\mathrm{Adapter}(x) = x + W_2(\\mathrm{ReLU}(W_1\\,x)).\n",
    "     $$\n",
    "  4. **LoRA** (Hu et al., 2021): reparametrizar la proyección $W$ como $W + \\Delta W$, con $\\Delta W = A B$, rangos bajos.\n",
    "\n",
    "* **Actividades prácticas**\n",
    "\n",
    "  1. **Pre-entrenamiento de BERT pequeño**\n",
    "\n",
    "     * Dataset: Wikipedia simplificado, 100 M tokens.\n",
    "     * Entrenar 100 k pasos, batch 64, lr=1e-4.\n",
    "  2. **Fine-tuning en clasificación**\n",
    "\n",
    "     * Tarea: SST-2 (sentimientos).\n",
    "     * Comparar full fine-tuning vs. adapters (2M parámetros) vs. LoRA (rank=4).\n",
    "     * Medir accuracy y tiempo de convergencia.\n",
    "  3. **Análisis de eficiencia**\n",
    "\n",
    "     * Parámetros entrenados vs. desempeño.\n",
    "     * Uso de GPU y memoria.\n",
    "\n",
    "El informe final incluye tablas de resultados, gráficas de accuracy vs. número de parámetros y discusiones sobre escalabilidad.\n",
    "\n",
    "#### **Proyecto 6. Redes neuronales con memoria externa (NTM, DNC)**\n",
    "\n",
    "En este proyecto se implementan y analizan modelos neuronales que integran una memoria diferenciable, extendiendo la capacidad de las RNN para tareas algorítmicas.\n",
    "\n",
    "* **Objetivos de aprendizaje**\n",
    "\n",
    "  * Entender lectura y escritura diferenciable en una matriz de memoria.\n",
    "  * Implementar un Neural Turing Machine básico y un Differentiable Neural Computer.\n",
    "  * Analizar la dinámica de los vectores de lectura/escritura.\n",
    "\n",
    "* **Estructura y ecuaciones**\n",
    "\n",
    "  1. **Memoria**: matriz $M\\in\\mathbb{R}^{N\\times W}$.\n",
    "  2. **Lectura por contenido**:\n",
    "\n",
    "     $$\n",
    "       w^c = \\operatorname{softmax}\\bigl(\\beta \\,\\mathrm{cosine}(k, M_i)\\bigr),\\quad\n",
    "       r = \\sum_{i=1}^N w_i^c M_i.\n",
    "     $$\n",
    "  3. **Interpolación con ubicación previa**:\n",
    "\n",
    "     $$\n",
    "       w^g = g\\,w^c + (1-g)\\,w^{t-1},\\quad\n",
    "       w = \\mathrm{conv}(w^g, s).\n",
    "     $$\n",
    "  4. **Escritura**:\n",
    "\n",
    "     * **Erase**: $M \\leftarrow M \\odot (1 - w\\,e^\\top)$.\n",
    "     * **Add**: $M \\leftarrow M + w\\,a^\\top$.\n",
    "\n",
    "* **Actividades prácticas**\n",
    "\n",
    "  1. **Implementación de NTM toy**\n",
    "\n",
    "     * Tarea: copiar y revertir secuencias binarias de longitud variable.\n",
    "     * Monitorizar curvas de *loss* y exactitud.\n",
    "  2. **Visualización**\n",
    "\n",
    "     * Graficar los pesos de lectura/escritura $w_t$.\n",
    "     * Comprender desplazamientos y mecanismos de atención.\n",
    "  3. **Comparación con RNN**\n",
    "\n",
    "     * Entrenar un LSTM puro en la misma tarea.\n",
    "     * Evaluar capacidad de generalización a secuencias más largas.\n",
    "\n",
    "Se entregará un notebook con animaciones de cómo evoluciona la atención en memoria y análisis cuantitativo de generalización.\n",
    "\n",
    "#### **Proyecto 7. Aprendizaje de pocos ejemplos e In-Context learning**\n",
    "\n",
    "Se exploran mecanismos para que grandes modelos generativos realicen tareas con pocos o ningún ajuste de parámetros, usando prompts.\n",
    "\n",
    "* **Objetivos de aprendizaje**\n",
    "\n",
    "  * Diseñar prompts efectivos para clasificación y extracción de información.\n",
    "  * Medir cómo varía desempeño al cambiar número de ejemplos en prompt.\n",
    "  * Explorar Instruction Tuning y calibración de polaridad.\n",
    "\n",
    "* **Técnicas**\n",
    "\n",
    "  1. **Prompting**:\n",
    "\n",
    "     $$\n",
    "       \\underbrace{\\langle\\text{Describe tarea}\\rangle}_{\\text{instrucción}}\n",
    "       \\,\\|\\,\\underbrace{\\langle\\text{Ejemplo}\\rangle}_{\\text{pocos–shots}}\n",
    "       \\,\\|\\,\\langle\\text{Entrada nueva}\\rangle.\n",
    "     $$\n",
    "  2. **PET** (Schick & Schütze): re-formulación de clasificación en problemas de lenguaje enmascarado.\n",
    "  3. **Instruction Tuning**: ajuste de modelo para seguir instrucciones naturales.\n",
    "\n",
    "* **Actividades prácticas**\n",
    "\n",
    "  1. **Diseño de prompts**\n",
    "\n",
    "     * Tarea de clasificación de sentimiento en oraciones cortas.\n",
    "     * Probar 0-shot, 1-shot, 5-shot.\n",
    "  2. **Cadena de prompts** (*chain-of-thought*)\n",
    "\n",
    "     * Formular razonamientos paso a paso.\n",
    "     * Evaluar precisión vs. prompts directos.\n",
    "  3. **Medición**\n",
    "\n",
    "     * Accuracy, F1, calibración (Brier score).\n",
    "\n",
    "Se documentará la influencia del número de *shots* y la calidad de prompts en el desempeño, concluyendo con recomendaciones prácticas.\n",
    "\n",
    "\n",
    "#### **Proyecto 8. Arquitecturas emergentes**\n",
    "\n",
    "Este módulo final revisa arquitecturas como Pointer Networks, Memory-Augmented RNNs y pipelines multimodales, proponiendo implementaciones toy para cada una.\n",
    "\n",
    "* **Objetivos de aprendizaje**\n",
    "\n",
    "  * Implementar Pointer Networks para resolver problemas de combinatoria.\n",
    "  * Diseñar un pipeline RAG (Retrieval-Augmented Generation).\n",
    "  * Exponer recientes avances en multimodalidad (Vision+Text).\n",
    "\n",
    "* **Algoritmos y técnicas**\n",
    "\n",
    "  1. **Pointer Network** (Vinyals et al., 2015): utiliza atención para seleccionar posiciones de entrada como salida.\n",
    "\n",
    "     $$\n",
    "       P(o_t = i \\mid o_{<t}, X) = \\alpha_{ti},\\quad\n",
    "       \\alpha_{ti} = \\mathrm{softmax}(u_i^\\top h_t).\n",
    "     $$\n",
    "  2. **RAG** (Lewis et al., 2020): combina recuperación (BM25 o DPR) con generación autoregresiva.\n",
    "\n",
    "     $$\n",
    "       p(y\\mid x) = \\sum_{d\\in D} p_\\theta(y\\mid x,d)\\,p_\\phi(d\\mid x).\n",
    "     $$\n",
    "  3. **Modelos multimodales**: CLIP (imagen+texto alineados), Vision Transformer + LLM para generación de captions.\n",
    "\n",
    "* **Actividades prácticas**\n",
    "\n",
    "  1. **Pointer Network toy**\n",
    "\n",
    "     * Tarea: sumar subconjuntos de una secuencia de enteros.\n",
    "     * Medir exactitud y eficiencia.\n",
    "  2. **Pipeline RAG simple**\n",
    "\n",
    "     * Índice de documentos en BM25.\n",
    "     * Usar GPT-2 para generar respuesta condicionada en top-k documentos recuperados.\n",
    "     * Evaluar fluidez y relevancia (BLEU manual).\n",
    "  3. **Seminario de papers (opcional)**\n",
    "\n",
    "     * Lectura crítica de 2-3 artículos recientes (último año).\n",
    "     * Presentación de hallazgos y open challenges.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d397d7bb-9061-4a7c-8e22-0c372c3789b1",
   "metadata": {},
   "source": [
    "\n",
    "#### **Proyecto 9 Modelos Seq2Seq No Autoregresivos (Non-Autoregressive Translation, NAT)**\n",
    "\n",
    "**Objetivo:** Reducir la latencia de generación eliminando la dependencia causal token-a-token, a costa de retos de multimodalidad en la distribución de salidas.\n",
    "\n",
    "**Descripción:**\n",
    "Los modelos seq2seq clásicos generan cada token condicionándose en todos los anteriores:\n",
    "\n",
    "$$\n",
    "P(y_{1:T}\\mid x_{1:S}) = \\prod_{t=1}^T P(y_t \\mid y_{<t},\\,x_{1:S})\\,.\n",
    "$$\n",
    "\n",
    "En cambio, los NAT introducen una factorización paralela:\n",
    "\n",
    "$$\n",
    "P(y_{1:T}\\mid x_{1:S}) \\approx \\prod_{t=1}^T P(y_t \\mid x_{1:S})\\,,\n",
    "$$\n",
    "\n",
    "generando toda la secuencia en un solo paso de atención.\n",
    "\n",
    "**Algoritmos clave a implementar:**\n",
    "\n",
    "1. **Mask-Predict** (Ghazvininejad et al., 2019):\n",
    "\n",
    "   * Se inicializa $y_{1:T}$ con tokens especiales $\\texttt{[MASK]}$.\n",
    "   * En cada iteración, se predicen todos los $\\hat y_t$ en paralelo y se \"desenmascaran\" los k tokens de mayor confianza.\n",
    "   * Repetir hasta que no queden máscaras.\n",
    "\n",
    "2. **Iterative Refinement with Fertility** (Goyal et al., 2020):\n",
    "\n",
    "   * Se predice primero la \"fertilidad\" $f_t$ de cada token fuente (cuántos tokens destino genera).\n",
    "\n",
    "   $$\n",
    "     P(f_{1:S}\\mid x_{1:S})\\,,\\quad \\sum_t f_t = T.\n",
    "   $$\n",
    "\n",
    "   * A partir de $f_t$, se expande cada embedding fuente y se aplica una capa de Transformer para afinar la salida.\n",
    "\n",
    "3. **Latent-variable NAT** (Tran et al., 2020):\n",
    "\n",
    "   * Introducen variables latentes $z$ que capturan la dependencia secuencial.\n",
    "\n",
    "   $$\n",
    "     P(y\\mid x) = \\int P(y\\mid x, z)\\,P(z\\mid x)\\,dz\\,,\n",
    "   $$\n",
    "\n",
    "   aproximado con VAE y refinamiento.\n",
    "\n",
    "**Retos y métricas:**\n",
    "\n",
    "* **Multimodalidad**: la distribución paralela no capta bien ambigüedades (p.ej., sintaxis válida múltiple).\n",
    "* **Re-ordering**: manejar reordenamientos no triviales entre idiomas.\n",
    "\n",
    "Evaluar en BLEU y latency (ms/ejemplo) para pares de idiomas con distinto orden sintáctico (inglés->alemán, japonés->inglés).\n",
    "\n",
    "\n",
    "#### **Proyecto 10. Arquitecturas Seq2Seq jerárquicas para documentos largos**\n",
    "\n",
    "**Objetivo:** Gestionar la generación y resumen de documentos extensos (miles de tokens) capturando coherencia global y dependencias entre secciones.\n",
    "\n",
    "**Descripción:**\n",
    "Los seq2seq estándar se quedan cortos en secuencias mayores a \\~1 000 tokens por limitaciones de memoria y degradación de contexto. Las arquitecturas jerárquicas dividen el problema en dos niveles:\n",
    "\n",
    "1. **Encoder jerárquico**:\n",
    "\n",
    "   * **Primer nivel**: procesa fragmentos (párrafos/chunks) $x^{(i)}_{1:L}$ y extrae representaciones locales $h^{(i)}$.\n",
    "\n",
    "     $$\n",
    "       h^{(i)} = \\mathrm{Encoder}_{\\mathrm{local}}(x^{(i)}_{1:L}).\n",
    "     $$\n",
    "   * **Segundo nivel**: un Transformer global que aplica atención sobre $\\{h^{(i)}\\}_{i=1}^M$, donde $M$ es el número de chunks.\n",
    "\n",
    "     $$\n",
    "       H = \\mathrm{Encoder}_{\\mathrm{global}}(\\{h^{(i)}\\}).\n",
    "     $$\n",
    "\n",
    "2. **Decoder condicionado jerárquico**:\n",
    "\n",
    "   * Genera primero un \"esquema\" de alto nivel (títulos de secciones) y luego expande cada sección.\n",
    "\n",
    "**Algoritmos a implementar:**\n",
    "\n",
    "* **Chunking dinámico**: adaptar tamaños de chunks por límite de GPU.\n",
    "* **Memory-augmented cross-chunk attention**: leer representaciones de secciones previas mediante un buffer de memoria diferenciable (Neural Turing Machine style).\n",
    "* **Coverage penalty** a nivel de sección para evitar repeticiones:\n",
    "\n",
    "  $$\n",
    "    \\mathrm{covPenalty} = \\sum_{i=1}^M \\min\\bigl(\\sum_{t}a_{t,i},1\\bigr),\n",
    "  $$\n",
    "\n",
    "  donde $a_{t,i}$ es atención de paso $t$ al chunk $i$.\n",
    "\n",
    "**Casos de uso y evaluación:**\n",
    "\n",
    "* **Resumen de documentos** (CNN/DailyMail extendido).\n",
    "* **Generación de informes técnicos**: coherencia a nivel de sección y transición de tópicos.\n",
    "\n",
    "**Métricas:**\n",
    "\n",
    "* ROUGE-L para resumen, cohesión textual (evaluada con métricas de coherencia de oraciones).\n",
    "* Perplexity sobre test set de documentos largos (>2 000 tokens).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b577bb8-8588-4765-b6ba-780bbb682a8c",
   "metadata": {},
   "source": [
    "#### Referencias\n",
    "\n",
    "1. **Word2Vec**\n",
    "   Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). *Efficient Estimation of Word Representations in Vector Space*. arXiv:1301.3781.\n",
    "\n",
    "2. **GloVe**\n",
    "   Pennington, J., Socher, R., & Manning, C. D. (2014). *GloVe: Global Vectors for Word Representation*. In *Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)* (pp. 1532–1543).\n",
    "\n",
    "3. **ELMo**\n",
    "   Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018). *Deep Contextualized Word Representations*. In *Proceedings of NAACL-HLT* (pp. 2227–2237).\n",
    "\n",
    "4. **BERT**\n",
    "   Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*. In *Proceedings of NAACL-HLT* (pp. 4171–4186).\n",
    "\n",
    "5. **Seq2Seq con Atención**\n",
    "   Bahdanau, D., Cho, K., & Bengio, Y. (2015). *Neural Machine Translation by Jointly Learning to Align and Translate*. In *International Conference on Learning Representations (ICLR)*.\n",
    "\n",
    "6. **Transformers**\n",
    "   Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … Polosukhin, I. (2017). *Attention Is All You Need*. In *Advances in Neural Information Processing Systems* (Vol. 30, pp. 5998–6008).\n",
    "\n",
    "7. **Pointer Networks**\n",
    "   Vinyals, O., Fortunato, M., & Jaitly, N. (2015). *Pointer Networks*. In *Advances in Neural Information Processing Systems* (Vol. 28, pp. 2692–2700).\n",
    "\n",
    "8. **SeqGAN**\n",
    "   Yu, L., Zhang, W., Wang, J., & Yu, Y. (2017). *SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient*. In *Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence* (pp. 2852–2858).\n",
    "\n",
    "9. **Neural Turing Machine**\n",
    "   Graves, A., Wayne, G., & Danihelka, I. (2014). *Neural Turing Machines*. arXiv:1410.5401.\n",
    "\n",
    "10. **Retrieval-Augmented Generation (RAG)**\n",
    "    Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., … Riedel, S. (2020). *Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks*. arXiv:2005.11401.\n"
    }
],                         
"metadata": {
  "kernelspec": {
    "display_name": "Python 3 (ipykernel)",
    "language": "python",
    "name": "python3"
  },
  "language_info": {
    "codemirror_mode": {
      "name": "ipython",
      "version": 3
    },
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": "3.8.13"
  }
},
"nbformat": 4,
"nbformat_minor": 5
}

