{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5d48f65",
   "metadata": {},
   "source": [
    "### Tutorial de transformers de Hugging Face \n",
    "\n",
    "Este cuaderno proporcionará una introducción a la librería Python Hugging Face Transformers y algunos patrones comunes que puedes usar para aprovecharla. Es más útil para usar o ajustar modelos transformadores preentrenados para tus proyectos.\n",
    "\n",
    "Hugging Face proporciona acceso a modelos (tanto el código que los implementa como sus pesos preentrenados, incluyendo los últimos LLMs como Llama3, DBRX, etc.), tokenizadores específicos de los modelos, así como pipelines para tareas comunes de NLP, y datasets y métricas en un paquete separado llamado `datasets`. Tiene implementaciones en PyTorch, Tensorflow y Flax (¡aunque usaremos las versiones de PyTorch aquí!)\n",
    "\n",
    "Vamos a repasar algunos casos de uso:\n",
    "* Descripción general de Tokenizers y Modelos\n",
    "* Ajuste fino - para tu propia tarea. Usaremos un ejemplo de clasificación de sentimientos.\n",
    "\n",
    "Se pueden aplicar a otros proyectos interesantes tambien:\n",
    "\n",
    "1. Aplicar un modelo preentrenado existente a una nueva aplicación o tarea y explorar cómo abordarlo/solucionarlo.\n",
    "2. Implementar una nueva o compleja arquitectura neural y demostrar su rendimiento en algunos datos.\n",
    "3. Analizar el comportamiento de un modelo: cómo representa el conocimiento lingüístico o qué tipo de fenómenos puede manejar o errores que comete.\n",
    "\n",
    "De estos, `transformers` será de mayor ayuda para (1) y para (3). (2) implica una curva de aprendizaje, pero si la dominas, encontrarás muy conveniente diseñar un modelo basado en los existentes proporcionados por Hugging Face. No lo cubriremos aquí y por favor refiérete a [este ejemplo](https://huggingface.co/docs/transformers/en/custom_models).\n",
    "\n",
    "Aquí hay recursos adicionales que introducen la librería que se utilizaron para hacer este cuaderno:\n",
    "\n",
    "* [Docs de Hugging Face](https://huggingface.co/docs/transformers/index)\n",
    "  * Documentación clara\n",
    "  * Tutoriales, recorridos y cuadernos de ejemplo\n",
    "  * Lista de modelos disponibles\n",
    "* [Curso de Hugging Face](https://huggingface.co/course/)\n",
    "* [Ejemplos de Hugging Face](https://github.com/huggingface/transformers/tree/main/examples/pytorch) Puedes encontrar estructuras de código muy similares en tareas/modelos descendentes muy diferentes usando Hugging Face.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffeeb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de las bibliotecas necesarias\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405187a2",
   "metadata": {},
   "source": [
    "Se escribe una función print_encoding diseñada para imprimir de manera legible el contenido de un diccionario,para mostrar las entradas del modelo después de la tokenización. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0626b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import json\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def print_encoding(model_inputs, indent=4):\n",
    "    indent_str = \" \" * indent\n",
    "    print(\"{\")\n",
    "    for k, v in model_inputs.items():\n",
    "        print(indent_str + k + \":\")\n",
    "        print(indent_str + indent_str + str(v))\n",
    "    print(\"}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7d4c78",
   "metadata": {},
   "source": [
    "### 1. Patrón común para usar Transformers de Hugging Face\n",
    "\n",
    "Vamos a empezar con un patrón de uso común para Transformadores de Hugging Face, usando el ejemplo de análisis de sentimientos.\n",
    "\n",
    "Primero, encuentra un modelo en el [hub](https://huggingface.co/models) de Hugging Face. Cualquiera puede subir su modelo para que otras personas lo usen. (Estoy usando un modelo de análisis de sentimientos de [este artículo](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3489963)).\n",
    "\n",
    "Luego, hay dos objetos que necesitan ser inicializados: un **tokenizador** y un **modelo**\n",
    "\n",
    "* El tokenizador convierte cadenas en listas de IDs de vocabulario que el modelo requiere.\n",
    "* El modelo toma los IDs de vocabulario y produce una predicción.\n",
    "\n",
    "![full_nlp_pipeline.png](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg)\n",
    "De [https://huggingface.co/course/chapter2/2?fw=pt](https://huggingface.co/course/chapter2/2?fw=pt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b7c163",
   "metadata": {},
   "source": [
    "#### RoBERTa\n",
    "\n",
    "RoBERTa (Robustly optimized BERT approach) es un modelo de lenguaje preentrenado desarrollado por Facebook AI. Es una variante del modelo BERT (Bidirectional Encoder Representations from Transformers) con algunas mejoras en el entrenamiento que lo hacen más robusto y eficaz en diversas tareas de procesamiento de lenguaje natural (NLP).\n",
    "\n",
    "\n",
    "- Preentrenamiento con más datos: RoBERTa se entrena con más datos que BERT, lo que mejora su capacidad para capturar patrones y relaciones en el lenguaje.\n",
    "- Más pasos de entrenamiento: Realiza más pasos de entrenamiento para mejorar el aprendizaje del modelo.\n",
    "- Batch sizes más grandes: Utiliza lotes de datos más grandes durante el entrenamiento, lo que ayuda a estabilizar y mejorar el aprendizaje.\n",
    "- Sin enmascaramiento de próxima oración: A diferencia de BERT, RoBERTa elimina la tarea de predicción de la próxima oración, lo que simplifica el entrenamiento y se enfoca más en la predicción de palabras enmascaradas.\n",
    "\n",
    "El código siguiente utiliza el modelo RoBERTa preentrenado para la clasificación de secuencias, específicamente para la clasificación de sentimientos en inglés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257cd0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Inicializar el tokenizador\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"siebert/sentiment-roberta-large-english\")\n",
    "# Inicializar el modelo\n",
    "modelo = AutoModelForSequenceClassification.from_pretrained(\"siebert/sentiment-roberta-large-english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e5d6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = \"I'm  happy to learn about Hugging Face Transformers!\"\n",
    "tokenized_inputs = tokenizer(inputs, return_tensors=\"pt\")\n",
    "outputs = modelo(**tokenized_inputs)\n",
    "\n",
    "labels = ['NEGATIVE', 'POSITIVE']\n",
    "prediction = torch.argmax(outputs.logits)\n",
    "\n",
    "\n",
    "print(\"Entradas:\")\n",
    "print(inputs)\n",
    "print()\n",
    "print(\"Tokenized Inputs:\")\n",
    "print_encoding(tokenized_inputs)\n",
    "print()\n",
    "print(\"Salida del modelo:\")\n",
    "print(outputs)\n",
    "print()\n",
    "print(f\"La prediccion es {labels[prediction]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a9fb68",
   "metadata": {},
   "source": [
    "### 1.2 Tokenizers (Tokenizadores)\n",
    "\n",
    "Los modelos preentrenados se implementan junto con **tokenizadores** que se usan para preprocesar sus entradas. Los tokenizadores toman cadenas de texto o listas de cadenas y producen lo que son efectivamente diccionarios que contienen las entradas del modelo.\n",
    "\n",
    "Puedes acceder a los tokenizadores ya sea con la clase Tokenizer específica del modelo que deseas usar (aquí DistilBERT), o con la clase AutoTokenizer.\n",
    "Los Fast Tokenizers están escritos en Rust, mientras que sus versiones lentas están escritas en Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1171f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertTokenizerFast, AutoTokenizer\n",
    "name = \"distilbert/distilbert-base-cased\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b77af5",
   "metadata": {},
   "source": [
    "#### DistilBERT\n",
    "\n",
    "DistilBERT es una versión comprimida y optimizada del modelo BERT (Bidirectional Encoder Representations from Transformers). Fue desarrollado por Hugging Face con el objetivo de hacer que los modelos de lenguaje grandes sean más ligeros, rápidos y eficientes sin una pérdida significativa de rendimiento. DistilBERT se entrena utilizando un proceso llamado distillation (destilación), en el que un modelo más pequeño (el estudiante) aprende a reproducir el comportamiento de un modelo más grande (el maestro).\n",
    "\n",
    "**Características principales de DistilBERT**\n",
    "\n",
    "- Tamaño reducido: DistilBERT tiene aproximadamente la mitad de los parámetros de BERT base, lo que lo hace más ligero y fácil de desplegar en producción.\n",
    "- Velocidad: Debido a su menor tamaño, DistilBERT es más rápido tanto en entrenamiento como en inferencia.\n",
    "- Rendimiento: A pesar de ser más pequeño y rápido, DistilBERT mantiene alrededor del 97% de la precisión de BERT en una variedad de tareas de procesamiento de lenguaje natural.\n",
    "\n",
    "DistilBERT se entrena utilizando un método llamado destilación de conocimiento, que implica entrenar un modelo más pequeño para imitar el comportamiento de un modelo más grande. El proceso incluye:\n",
    "\n",
    "- Entrenamiento con pérdidas combinadas: El modelo estudiante se entrena con una combinación de la pérdida estándar (por ejemplo, pérdida de entropía cruzada) y la pérdida de distilación, que mide qué tan bien las predicciones del modelo estudiante coinciden con las del modelo maestro.\n",
    "- Conservación del conocimiento: El modelo estudiante aprende a conservar y replicar el conocimiento adquirido por el modelo maestro, pero de una manera más compacta y eficiente.\n",
    "\n",
    "El siguiente código muestra cómo inicializar y utilizar DistilBERT para tareas de tokenización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fee86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(name)      # escrito en Python\n",
    "print(tokenizer)\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(name)  # escrito en Rust\n",
    "print(tokenizer)\n",
    "tokenizer = AutoTokenizer.from_pretrained(name) # por defecto es Fast\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b835152",
   "metadata": {},
   "source": [
    "Este resultado muestra la configuración y las propiedades de diferentes tokenizadores de DistilBERT que se han inicializado con el nombre de modelo distilbert/distilbert-base-cased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10474e96",
   "metadata": {},
   "source": [
    "```\n",
    "DistilBertTokenizer(\n",
    "    name_or_path='distilbert/distilbert-base-cased',\n",
    "    vocab_size=28996,\n",
    "    model_max_length=512,\n",
    "    is_fast=False,\n",
    "    padding_side='right',\n",
    "    truncation_side='right',\n",
    "    special_tokens={\n",
    "        'unk_token': '[UNK]',\n",
    "        'sep_token': '[SEP]',\n",
    "        'pad_token': '[PAD]',\n",
    "        'cls_token': '[CLS]',\n",
    "        'mask_token': '[MASK]'\n",
    "    },\n",
    "    clean_up_tokenization_spaces=True\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e9c9d9",
   "metadata": {},
   "source": [
    "DistilBertTokenizerFast (dos veces con la misma configuración)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e155bafb",
   "metadata": {},
   "source": [
    "```\n",
    "DistilBertTokenizerFast(\n",
    "    name_or_path='distilbert/distilbert-base-cased',\n",
    "    vocab_size=28996,\n",
    "    model_max_length=512,\n",
    "    is_fast=True,\n",
    "    padding_side='right',\n",
    "    truncation_side='right',\n",
    "    special_tokens={\n",
    "        'unk_token': '[UNK]',\n",
    "        'sep_token': '[SEP]',\n",
    "        'pad_token': '[PAD]',\n",
    "        'cls_token': '[CLS]',\n",
    "        'mask_token': '[MASK]'\n",
    "    },\n",
    "    clean_up_tokenization_spaces=True\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dafa73",
   "metadata": {},
   "source": [
    "El resultado muestra que has inicializado tres tokenizadores para el modelo distilbert-base-cased:\n",
    "\n",
    "- DistilBertTokenizer: Este es el tokenizador estándar, escrito en Python, que no es tan rápido como su contraparte rápida, pero aún es ampliamente utilizado para tareas de NLP.\n",
    "- DistilBertTokenizerFast (dos veces con la misma configuración): Estos son los tokenizadores rápidos, escritos en Rust, que son más eficientes en términos de tiempo de ejecución. Aunque se muestran dos veces, ambos representan la misma configuración y funcionalidad, indicando que has inicializado el tokenizador rápido más de una vez."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0c9732",
   "metadata": {},
   "source": [
    "El código siguiente proporciona una demostración de cómo tokenizar una cadena de texto utilizando un tokenizador preentrenado de Hugging Face.\n",
    "\n",
    "La salida incluye los identificadores de los tokens y la máscara de atención, que son esenciales para el procesamiento por parte del modelo. Además, se demuestra cómo acceder a los tokens de dos maneras diferentes, resaltando la flexibilidad de los objetos devueltos por la biblioteca transformers.\n",
    "\n",
    "**Máscara de atención**\n",
    "\n",
    "Una máscara de atención (attention mask) es una herramienta utilizada en modelos de procesamiento de lenguaje natural, especialmente en arquitecturas de transformers, para indicar qué tokens (palabras o subpalabras) deben ser atendidos por el modelo y cuáles deben ser ignorados durante el proceso de atención.\n",
    "\n",
    "En el contexto de los transformers y modelos como BERT o DistilBERT, las secuencias de entrada suelen tener diferentes longitudes. Sin embargo, para procesarlas de manera eficiente en lotes (batches), las secuencias deben ser de la misma longitud. Esto se logra mediante el relleno (padding), que añade tokens especiales ([PAD]) al final de las secuencias más cortas para que todas alcancen la misma longitud. La máscara de atención se utiliza para asegurarse de que estos tokens de relleno no influyan en las predicciones del modelo.\n",
    "\n",
    "Algunas función de la máscara de atención son:\n",
    "\n",
    "- Indicar tokens relevantes: La máscara de atención señala qué tokens en la secuencia son relevantes y deben ser considerados por el modelo.\n",
    "- Ignorar tokens de relleno: La máscara de atención asegura que los tokens de relleno ([PAD]) añadidos a las secuencias más cortas sean ignorados durante el cálculo de la atención.\n",
    "\n",
    "La máscara de atención es una lista o tensor de la misma longitud que la secuencia de entrada tokenizada. Contiene valores binarios:\n",
    "\n",
    "- 1: Indica que el token correspondiente es relevante y debe ser atendido.\n",
    "- 0: Indica que el token correspondiente es un token de relleno y debe ser ignorado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4877a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Así es como llamas al tokenizador\n",
    "input_str = \"Hugging Face Transformers is great!\"\n",
    "tokenized_inputs = tokenizer(input_str) # https://huggingface.co/learn/nlp-course/en/chapter6/6\n",
    "\n",
    "print(\"Tokenización Básica\")\n",
    "print_encoding(tokenized_inputs)\n",
    "print()\n",
    "\n",
    "# Dos formas de acceder:\n",
    "print(tokenized_inputs.input_ids)\n",
    "print(tokenized_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4981ae38",
   "metadata": {},
   "source": [
    "El código siguiente realiza una serie de pasos para tokenizar una cadena de texto, agregar tokens especiales (como los tokens de clasificación `[CLS]` y separación `[SEP])`, y luego decodificar los tokens de vuelta a texto.\n",
    "\n",
    "Estos pasos no crean la máscara de atención ni añaden los caracteres especiales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a05b7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = [tokenizer.cls_token_id]\n",
    "sep = [tokenizer.sep_token_id]\n",
    "\n",
    "# La tokenización ocurre en unos pocos pasos:\n",
    "input_tokens = tokenizer.tokenize(input_str)\n",
    "input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n",
    "input_ids_special_tokens = cls + input_ids + sep\n",
    "\n",
    "decoded_str = tokenizer.decode(input_ids_special_tokens)\n",
    "\n",
    "print(\"inicio:                \", input_str)\n",
    "print(\"tokenizar:             \", input_tokens)\n",
    "print(\"convert_tokens_to_ids:\", input_ids)\n",
    "print(\"agregar tokens especiales:   \", input_ids_special_tokens)\n",
    "print(\"--------\")\n",
    "print(\"decodificar:               \", decoded_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a151de27",
   "metadata": {},
   "source": [
    "El siguiente fragmento de código utiliza el FastTokenizer de la biblioteca transformers para tokenizar una cadena de texto y luego analiza los tokens resultantes en detalle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae106cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para Fast Tokenizer, hay otra opción también:\n",
    "inputs = tokenizer._tokenizer.encode(input_str)\n",
    "\n",
    "print(input_str)\n",
    "print(\"-\"*5)\n",
    "print(f\"Número de tokens: {len(inputs)}\")\n",
    "print(f\"Ids: {inputs.ids}\")\n",
    "print(f\"Tokens: {inputs.tokens}\")\n",
    "print(f\"Máscara de tokens especiales: {inputs.special_tokens_mask}\")\n",
    "print()\n",
    "print(\"char_to_word da el wordpiece de un carácter en la entrada\")\n",
    "char_idx = 8\n",
    "print(f\"Por ejemplo, el {char_idx + 1}º carácter de la cadena es '{input_str[char_idx]}',\"+\\\n",
    "      f\" y es parte del wordpiece {inputs.char_to_token(char_idx)}, '{inputs.tokens[inputs.char_to_token(char_idx)]}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd1de05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Otros trucos interesantes:\n",
    "# El tokenizador puede devolver tensores de pytorch\n",
    "model_inputs = tokenizer(\"¡Los Transformadores de Hugging Face son geniales!\", return_tensors=\"pt\")\n",
    "print(\"Tensores PyTorch:\")\n",
    "print_encoding(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f755801",
   "metadata": {},
   "source": [
    "El código siguiente demuestra cómo tokenizar y rellenar múltiples secuencias de texto para que tengan la misma longitud, lo cual es necesario para el procesamiento por lotes en modelos de transformers. También se muestra cómo se utilizan los tokens de relleno (padding) y las máscaras de atención para indicar qué partes de las secuencias son relevantes para el modelo. Esta técnica asegura que los modelos de lenguaje puedan procesar secuencias de longitud variable de manera eficiente y precisa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90500169",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer([\"Hugging Face Transformers is great!\",\n",
    "                         \"The quick brown fox jumps over the lazy dog.\" +\\\n",
    "                         \"Then the dog got up and ran away because she didn't like foxes.\",\n",
    "                         ],\n",
    "                         return_tensors=\"pt\",\n",
    "                         padding=True,\n",
    "                         truncation=True)\n",
    "print(f\"Token de relleno: {tokenizer.pad_token} | ID del token de relleno: {tokenizer.pad_token_id}\")\n",
    "print(\"Relleno (padding):\")\n",
    "print_encoding(model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19be9632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# También puedes decodificar un lote completo a la vez:\n",
    "print(\"Decodificación por Lote:\")\n",
    "print(tokenizer.batch_decode(model_inputs.input_ids))\n",
    "print()\n",
    "print(\"Decodificación por Lote: (sin caracteres especiales)\")\n",
    "print(tokenizer.batch_decode(model_inputs.input_ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099fa981",
   "metadata": {},
   "source": [
    "Para obtener más información sobre los tokenizadores, puedes consultar:\n",
    "[Hugging Face Transformers Docs](https://huggingface.co/docs/transformers/main_classes/tokenizer) y la [Hugging Face Tokenizers Library](https://huggingface.co/docs/tokenizers/python/latest/quicktour.html).¡La librería de Tokenizers incluso te permite entrenar tus propios tokenizadores!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f21f5ee",
   "metadata": {},
   "source": [
    "#### 1.3 Modelos\n",
    "\n",
    "Inicializar modelos es muy similar a inicializar tokenizadores. Puedes usar la clase del modelo específica para tu modelo o puedes usar una clase AutoModel. Prefiero AutoModel, especialmente cuando quiero comparar modelos, porque es fácil especificar los modelos como cadenas.\n",
    "\n",
    "Aunque la mayoría de los transformers preentrenados tienen una arquitectura similar, hay pesos adicionales, llamados \"heads\" (cabecera) que debes entrenar si estás haciendo clasificación de secuencias, preguntas y respuestas, u otra tarea. Hugging Face configura automáticamente la arquitectura que necesitas cuando especificas la clase del modelo. Por ejemplo, estamos haciendo análisis de sentimientos, así que vamos a usar `DistilBertForSequenceClassification`. Si fuéramos a continuar entrenando DistilBERT en su objetivo de entrenamiento de modelado de lenguaje enmascarado, usaríamos `DistilBertForMaskedLM`, y si solo quisiéramos las representaciones del modelo, tal vez para nuestra propia tarea descendente, podríamos usar `DistilBertModel`.\n",
    "\n",
    "Aquí tienes una imagen estilizada de un modelo recreada a partir de una encontrada aquí: [https://huggingface.co/course/chapter2/2?fw=pt](https://huggingface.co/course/chapter2/2?fw=pt).\n",
    "![model_illustration.png](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head.svg)\n",
    "\n",
    "Aquí tienes algunos ejemplos.\n",
    "\n",
    "`* Model`\n",
    "\n",
    "`* ForMaskedLM`\n",
    "\n",
    "`* ForSequenceClassification`\n",
    "\n",
    "`* ForTokenClassification`\n",
    "\n",
    "`* ForQuestionAnswering`\n",
    "\n",
    "`* ForMultipleChoice`\n",
    "\n",
    "donde `*` puede ser `AutoModel` o un modelo preentrenado específico (por ejemplo, `DistilBert`).\n",
    "\n",
    "Hay tres tipos de modelos:\n",
    "\n",
    "* Encoders (por ejemplo, BERT)\n",
    "* Decoders (por ejemplo, GPT2)\n",
    "* Modelos Encoder-Decoder (por ejemplo, BART o T5)\n",
    "\n",
    "Las clases específicas de tareas disponibles dependen del tipo de modelo con el que estés tratando.\n",
    "\n",
    "Una lista completa de opciones está disponible en los [docs](https://huggingface.co/docs/transformers/model_doc/auto). Ten en cuenta que no todos los modelos son compatibles con todas las arquitecturas de modelos, por ejemplo DistilBERT no es compatible con los modelos Seq2Seq porque solo consiste en un encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2c2ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, DistilBertForSequenceClassification, DistilBertModel\n",
    "print('Cargando modelo base')\n",
    "modelo_base = DistilBertModel.from_pretrained('distilbert-base-cased')\n",
    "print(\"Cargando modelo de clasificación desde el checkpoint del modelo base\")\n",
    "modelo = DistilBertForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=2)\n",
    "modelo = AutoModelForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ee7b7f",
   "metadata": {},
   "source": [
    "También puedes inicializar con pesos aleatorios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b3f890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertConfig, DistilBertModel\n",
    "\n",
    "# Inicializando una configuración de DistilBERT\n",
    "configuracion = DistilBertConfig()\n",
    "configuracion.num_labels=2\n",
    "# Inicializando un modelo (con pesos aleatorios) desde la configuración\n",
    "modelo = DistilBertForSequenceClassification(configuracion)\n",
    "\n",
    "# Accediendo a la configuración del modelo\n",
    "configuracion = modelo.config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf73a585",
   "metadata": {},
   "source": [
    "Pasar entradas al modelo es súper fácil. Este código realiza la inferencia utilizando un modelo de clasificación de secuencias entrenado con un texto tokenizado. Aquí:\n",
    "\n",
    "- Se convierte la cadena de entrada en tokens y los representa como tensores de PyTorch.\n",
    "- Los tokens se acompañan de una máscara de atención que indica qué tokens son relevantes.\n",
    "- Se realiza la inferencia usando los input_ids y attention_mask.\n",
    "- El modelo produce logits que representan las salidas antes de aplicar la función softmax.\n",
    "- Se aplica la función softmax a los logits para obtener probabilidades de clase.\n",
    "- Se interpreta la clase más probable basada en estas probabilidades.\n",
    "\n",
    "Este flujo de trabajo muestra cómo se utiliza un modelo de clasificación de secuencias preentrenado para hacer predicciones sobre una cadena de texto tokenizada. La salida incluye tanto los tokens de entrada como las predicciones del modelo en forma de logits y distribuciones de probabilidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92515745",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
    "# Opción 1\n",
    "model_outputs = modelo(input_ids=model_inputs.input_ids, attention_mask=model_inputs.attention_mask)\n",
    "\n",
    "# Opción 2 - las claves del diccionario que devuelve el tokenizador son las mismas que los argumentos de palabra clave\n",
    "#            que espera el modelo\n",
    "\n",
    "# f({k1: v1, k2: v2}) = f(k1=v1, k2=v2)\n",
    "\n",
    "model_outputs = modelo(**model_inputs)\n",
    "\n",
    "print(model_inputs)\n",
    "print()\n",
    "print(model_outputs)\n",
    "print()\n",
    "print(f\"Distribución sobre etiquetas: {torch.softmax(model_outputs.logits, dim=1)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b294bcf1",
   "metadata": {},
   "source": [
    "Si te das cuenta, es un poco extraño que tengamos dos clases para una tarea de clasificación binaria - podrías fácilmente tener una sola clase y simplemente elegir un umbral. Es así por cómo los modelos de huggingface calculan la pérdida. Esto aumentará el número de parámetros que tenemos, pero no debería afectar el rendimiento.\n",
    "\n",
    "¡Estos modelos son solo módulos de Pytorch! Puedes calcular la pérdida con tu `loss_func` y llamar a `loss.backward`. Puedes usar cualquiera de los optimizadores o planificadores de tasas de aprendizaje que usas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2722b1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puedes calcular la pérdida como de costumbre\n",
    "label = torch.tensor([1])\n",
    "loss = torch.nn.functional.cross_entropy(model_outputs.logits, label)\n",
    "print(loss)\n",
    "loss.backward()\n",
    "# Puedes obtener los parámetros\n",
    "list(modelo.named_parameters())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fef2da",
   "metadata": {},
   "source": [
    "Hugging Face proporciona una forma adicional fácil de calcular la pérdida también:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881b4bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para calcular la pérdida, necesitamos pasar una etiqueta:\n",
    "model_inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
    "\n",
    "labels = ['NEGATIVE', 'POSITIVE']\n",
    "model_inputs['labels'] = torch.tensor([1])\n",
    "\n",
    "model_outputs = modelo(**model_inputs)\n",
    "\n",
    "print(model_outputs)\n",
    "print()\n",
    "print(f\"Predicciones del modelo: {labels[model_outputs.logits.argmax()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58290886",
   "metadata": {},
   "source": [
    "Puedes obtener los estados ocultos y los pesos de atención de los modelos muy fácilmente. Esto es particularmente útil si estás trabajando en un proyecto de análisis. (Por ejemplo, ver [What does BERT look at?](https://arxiv.org/abs/1906.04341)).\n",
    "\n",
    "\n",
    "El código siguiente carga un modelo preentrenado de distilbert-base-cased utilizando la biblioteca transformers de Hugging Face. Luego, el modelo es utilizado para generar salidas ocultas y atenciones para un texto de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664de3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "modelo = AutoModel.from_pretrained(\"distilbert-base-cased\", output_attentions=True, output_hidden_states=True)\n",
    "modelo.eval()\n",
    "\n",
    "model_inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    model_output = modelo(**model_inputs)\n",
    "\n",
    "print(\"Tamaño del estado oculto (por capa):  \", model_output.hidden_states[0].shape)\n",
    "print(\"Tamaño del head de atención (por capa):\", model_output.attentions[0].shape)     # (capa, lote, índice_palabra_consulta, índices_palabra_clave)\n",
    "                                                                                       # eje y es consulta, eje x es clave\n",
    "# print(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e046ff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(model_inputs.input_ids[0])\n",
    "print(tokens)\n",
    "\n",
    "n_layers = len(model_output.attentions)\n",
    "n_heads = len(model_output.attentions[0][0])\n",
    "fig, axes = plt.subplots(6, 12)\n",
    "fig.set_size_inches(18.5*2, 10.5*2)\n",
    "for layer in range(n_layers):\n",
    "    for i in range(n_heads):\n",
    "        axes[layer, i].imshow(model_output.attentions[layer][0, i])\n",
    "        axes[layer][i].set_xticks(list(range(9)))\n",
    "        axes[layer][i].set_xticklabels(labels=tokens, rotation=\"vertical\")\n",
    "        axes[layer][i].set_yticks(list(range(9)))\n",
    "        axes[layer][i].set_yticklabels(labels=tokens)\n",
    "\n",
    "        if layer == 5:\n",
    "            axes[layer, i].set(xlabel=f\"head={i}\")\n",
    "        if i == 0:\n",
    "            axes[layer, i].set(ylabel=f\"layer={layer}\")\n",
    "\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983b4a94",
   "metadata": {},
   "source": [
    "La salida del código es una figura  con múltiples subplots organizados en una cuadrícula de 6 filas y 12 columnas. Cada subplot representa la matriz de atención de una cabecera de atención específica en una capa específica del modelo. En cada matriz de atención:\n",
    "\n",
    "- Eje X: Representa los tokens de la secuencia de entrada que actúan como claves.\n",
    "- Eje Y: Representa los tokens de la secuencia de entrada que actúan como consultas.\n",
    "- Valores: Los valores en la matriz indican cuánta atención está poniendo un token de consulta en cada token de clave. Un valor más alto indica más atención.\n",
    "\n",
    "Este tipo de visualización es útil para entender cómo el modelo está distribuyendo su atención en diferentes partes de la secuencia de entrada a través de sus múltiples capas y cabeceras de atención."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e18101",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "1 . Practica la tokenización y decodificación de texto utilizando AutoTokenizer.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Utiliza el tokenizador distilbert-base-cased para tokenizar una frase de tu elección.\n",
    "- Imprime los tokens generados.\n",
    "- Convierte los tokens de nuevo a texto utilizando el método decode.\n",
    "- Compara el texto original con el texto decodificado y observa cualquier diferencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9eb52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4618e77",
   "metadata": {},
   "source": [
    "2 . Utiliza el modelo gpt2 para generar texto basado en una frase inicial dada. Implementa los pasos necesarios para inicializar el modelo y el tokenizer, generar texto y mostrar la salida.\n",
    "\n",
    "Código de inicio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1facbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Inicializar el tokenizer y el modelo\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "modelo = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Generar texto\n",
    "input_text = \"Once upon a time\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generar salida\n",
    "output = modelo.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "\n",
    "# Decodificar y mostrar la salida\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bdd879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a3622c",
   "metadata": {},
   "source": [
    "3 . Utiliza el pipeline de reconocimiento de entidades nombradas (NER) de Hugging Face para extraer entidades de un texto. Implementa los pasos necesarios para inicializar el pipeline, procesar el texto y mostrar las entidades extraídas.\n",
    "\n",
    "Código de inicio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab6fc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Inicializar el pipeline de NER\n",
    "ner_pipeline = pipeline(\"ner\", grouped_entities=True)\n",
    "\n",
    "# Texto de ejemplo\n",
    "text = \"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very close to the Manhattan Bridge.\"\n",
    "\n",
    "# Extraer entidades nombradas\n",
    "entities = ner_pipeline(text)\n",
    "\n",
    "# Mostrar las entidades\n",
    "for entity in entities:\n",
    "    print(f\"Entity: {entity['word']}, Label: {entity['entity_group']}, Score: {entity['score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea34b08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dbaffd",
   "metadata": {},
   "source": [
    "4 .  Fine-Tuning de un modelo para una nueva tarea\n",
    "\n",
    "Ajusta un modelo preentrenado para una nueva tarea de clasificación de sentimientos usando un dataset personalizado.\n",
    "\n",
    "Instrucciones:\n",
    "- Carga el dataset yelp_polarity de Hugging Face.\n",
    "- Ajusta un modelo distilbert-base-uncased usando este dataset.\n",
    "- Evalúa el modelo ajustado en un conjunto de datos de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4edf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import DistilBertForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "dataset = load_dataset(\"yelp_polarity\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(200))\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "results = trainer.evaluate()\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61c5296",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01b12bb",
   "metadata": {},
   "source": [
    "5 . Visualiza los pesos de atención de un modelo preentrenado.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Carga el modelo bert-base-uncased con la opción output_attentions=True.\n",
    "- Usa una frase de tu elección y pasa por el modelo para obtener los pesos de atención.\n",
    "- Visualiza los pesos de atención usando matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dba09bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n",
    "model.eval()\n",
    "\n",
    "input_str = \"El procesamiento de lenguaje natural es fascinante.\"\n",
    "model_inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**model_inputs)\n",
    "\n",
    "attentions = outputs.attentions\n",
    "\n",
    "# Visualizar la primera capa de atención\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "cax = ax.matshow(attentions[0][0][0].cpu().numpy(), cmap='viridis')\n",
    "fig.colorbar(cax)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000699cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33dd2c0",
   "metadata": {},
   "source": [
    "6 . Genera texto usando un modelo de lenguaje causal.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Usa el modelo gpt2 para generar texto a partir de una frase inicial.\n",
    "- Ajusta los parámetros de generación (por ejemplo, max_length, top_p) para observar diferentes resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427ad8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "prompt = \"Era un día soleado cuando\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "output = model.generate(**inputs, max_length=50, do_sample=True, top_p=0.9)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0135b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
