{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c3fbffa-bfc7-4133-a358-2464054258b6",
   "metadata": {},
   "source": [
    "## **Práctica calificada 1 CC0C2**\n",
    "\n",
    "#### **Indicaciones generales**\n",
    "\n",
    "Cada estudiante debe:\n",
    "1. Elegir **uno de los proyectos propuestos (no se admite proyectos repetidos)** y registrar su elección. **Fecha de entrega 3 de mayo hasta las 14:00 PM** \n",
    "2. Crear un **repositorio público en GitHub** con el nombre `nlp-proyectoXX`, donde `XX` es el número del proyecto.\n",
    "3. Actualizar dicho repositorio diariamente (mínimo un `commit` con mensaje descriptivo) con:\n",
    "   - Código fuente\n",
    "   - Notebooks de prueba\n",
    "   - Métricas intermedias\n",
    "   - Registro de decisiones técnicas\n",
    "4. Entregar el **enlace del repositorio** vía formulario o correo antes del dia jueves 24 de este mes. \n",
    "5. Presentar el trabajo en la **sesión final obligatoria de exposición**, en la que se evaluará:\n",
    "   - Entendimiento técnico\n",
    "   - Justificación de decisiones\n",
    "   - Análisis de resultados\n",
    "   - Calidad de visualización y comunicación\n",
    "\n",
    "\n",
    "#### **Cronograma sugerido de avance**\n",
    "\n",
    "| Día | Actividades esperadas | Verificable en GitHub |\n",
    "|-----|------------------------|------------------------|\n",
    "| 1 | Registro del proyecto elegido y estructura base del repositorio | `README.md`, carpeta `src/` y `notebooks/`, primer commit |\n",
    "| 2 | Primer prototipo o pruebas preliminares con datos | código de carga y procesado inicial |\n",
    "| 3 | Implementación parcial de componentes clave | archivo funcional + evidencia de ejecución |\n",
    "| 4 | Primeros resultados / métricas básicas | gráficos, tablas de salida, resultados intermedios |\n",
    "| 5 | Feedback intermedio (revisión cruzada) entre equipos | issues/comentarios cruzados en GitHub |\n",
    "| 6 | Mejoras / refactorización / experimentos adicionales | scripts refactorizados y limpieza de logs |\n",
    "| 7 | Documentación de decisiones, generación de reportes | notebooks explicativos o `doc/` con comparativas |\n",
    "| 8 | Implementación final y validación de resultados | ejecución reproducible + versión final `main.ipynb` |\n",
    "| 9 | Preparación de presentación (slides, notebook narrado) | `presentation.ipynb` o `slides.pdf` |\n",
    "| 10 | **Presentación en clase obligatoria** | defensa oral, sesión de preguntas, revisión del repo |\n",
    "\n",
    "\n",
    "#### **Criterios de evaluación**\n",
    "\n",
    "| Criterio | Peso (%) |\n",
    "|---------|----------|\n",
    "| **Presentación oral** (defensa técnica, comprensión profunda, dominio del contenido) | **60%** |\n",
    "| **Repositorio de GitHub** (progreso continuo, calidad del código, documentación) | 30% |\n",
    "| **Presentación del trabajo final** (visualización de resultados, storytelling técnico) | 10% |\n",
    "\n",
    "> **Importante:** La presentación del trabajo **no se puede reemplazar**. Si el estudiante no se presenta o presenta un trabajo no comprendido, la calificación será de  **0**. Si se presenta un trabajo al final sin mostrar seguimiento continuo, el puntaje por el repositorio de trabajo sera de 0%. \n",
    "\n",
    "#### **Requisitos del repositorio**\n",
    "\n",
    "Debe incluir al menos:\n",
    "- `README.md` con descripción clara del proyecto, dataset y enfoque\n",
    "- Carpeta `src/` con los módulos desarrollados\n",
    "- Carpeta `notebooks/` con pruebas y visualizaciones\n",
    "- Archivo `requirements.txt` o `environment.yml`\n",
    "- Comandos reproducibles (Makefile o instrucciones en README)\n",
    "- GitHub Actions (opcional, para pruebas automáticas o linting)\n",
    "\n",
    "\n",
    "#### **Sugerencia adicional**\n",
    "\n",
    "Antes de la entrega final:\n",
    "- Se recomienda usar `issues` o `projects` de GitHub para organizar tus tareas \n",
    "- Usar ramas (`feature/`, `fix/`, `test/`) y _pull requests_ como parte de buenas prácticas\n",
    "- Añadir métricas reproducibles (BLEU, ROUGE, Perplexity, etc.) con graficación\n",
    "\n",
    "\n",
    "#### **Rúbrica de evaluación oral individual – Proyecto de NLP**\n",
    "\n",
    "| Criterio | Excelente (4 pts) | Bueno (3 pts) | Regular (2 pts) | Insuficiente (1 pt) | Puntaje |\n",
    "|---------|------------------|----------------|------------------|----------------------|--------|\n",
    "| **1. Dominio del contenido técnico** | Explica con precisión, seguridad y profundidad todos los componentes del proyecto, incluyendo algoritmos, métricas y decisiones clave | Explica adecuadamente los aspectos principales, aunque con algunos vacíos de detalle | Muestra comprensión limitada; omite o malinterpreta partes importantes del trabajo | Evidencia poco o nulo conocimiento del proyecto; depende de leer el código sin entenderlo |      /4 |\n",
    "| **2. Razonamiento y justificación técnica** | Justifica cada decisión con base en teoría o experimentación (hiperparámetros, modelos, técnicas de NLP, etc.) | Justifica la mayoría de las decisiones, aunque algunas sin evidencia concreta | Justifica parcialmente, con argumentos débiles o vagos | No justifica sus decisiones o no entiende por qué eligió un enfoque |      /4 |\n",
    "| **3. Presentación de resultados** | Presenta métricas con análisis crítico, señala ventajas, limitaciones y propone mejoras | Muestra resultados claros, aunque con análisis superficial o sin propuestas | Resultados vagos o sin explicación; dificultad para vincularlos con el objetivo | No puede explicar los resultados o sólo los menciona sin contexto |      /4 |\n",
    "| **4. Claridad y expresión oral** | Habla con fluidez, usa terminología técnica adecuada, mantiene coherencia y estructura lógica | Lenguaje claro en general, aunque con momentos de duda o tecnicismos mal empleados | Discurso poco claro, uso limitado del lenguaje técnico o desorganización al hablar | Lenguaje confuso o desordenado; no se logra comprender el enfoque del proyecto |      /4 |\n",
    "| **5. Recursos visuales y preparación** | Uso excelente de visualizaciones, notebooks, slides u otros medios que refuerzan la exposición; excelente preparación | Usa recursos adecuados para apoyar su exposición, aunque con limitaciones en claridad o estructura | Presenta recursos poco organizados o insuficientes para complementar la explicación | No presenta recursos de apoyo o usa código sin formato, sin estructura ni explicación |      /4 |\n",
    "\n",
    "#### **Total: /20 puntos**\n",
    "  \n",
    "> El plagio, falta de exposición o presentación de código no entendido anula la evaluación (**nota: 0**).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581624cd-ed94-4e63-a796-b914b47b4030",
   "metadata": {},
   "source": [
    "### **Proyecto 1: Implementación de un Data Loader escalable y optimizado para IA generativa**\n",
    "\n",
    "Este proyecto consiste en diseñar y desarrollar desde cero un **Data Loader** capaz de alimentar modelos de IA generativa con flujos de datos de texto de gran tamaño, manteniendo latencias bajas y throughput alto. Sus componentes principales son:\n",
    "\n",
    "1. **Arquitectura de streaming y batching**  \n",
    "   - **Prefetching**: implementar un pool de hilos o procesos que lean chunks de texto desde disco o S3, descomprimiendo y normalizando en paralelo.  \n",
    "   - **Buffers circulares**: usar un buffer de tamaño $B$ para almacenar batches de tamaño $b$, de modo que mientras el modelo consume batch $i$, el loader prepara el $i+1$.  \n",
    "   - **Control de backpressure**: medir el queue length y ajustar dinámicamente la tasa de lectura para evitar saturar la memoria.\n",
    "\n",
    "2. **Preprocesamiento en la misma tubería**  \n",
    "   - **Normalización Unicode**: aplicar NFKC/ NFKD y pasar a minúsculas.  \n",
    "   - **Lematización y filtrado**: usar un lematizador (por ejemplo SpaCy o NLTK) y filtrar tokens muy raros con una distancia de Levenshtein mínima  \n",
    "     $$\n",
    "       d(i,j) = \n",
    "       \\begin{cases}\n",
    "         0 & \\text{si }i=j,\\\\\n",
    "         \\min\\{d(i-1,j)+1,\\;d(i,j-1)+1,\\;d(i-1,j-1)+\\mathbf{1}_{i\\neq j}\\} & \\text{en otro caso.}\n",
    "       \\end{cases}\n",
    "     $$\n",
    "   - **Segmentación de oraciones**: dividir en unidades semánticas usando expresiones regulares o modelos basados en CRF para detectar puntos de corte.\n",
    "\n",
    "3. **Manejo de vocabulario incremental**  \n",
    "   - **Construcción de índices**: asignar IDs de token de forma lazy, reservando un token `<UNK>` inicial.  \n",
    "   - **Reservoir sampling para OOV**: si aparece un token que no está en vocab y el vocab está lleno ($V_{\\max}$), reemplazar aleatoriamente un token de baja frecuencia.\n",
    "\n",
    "4. **Métricas de rendimiento y monitoreo**  \n",
    "   - **Throughput $\\tau$**: medir tokens procesados por segundo,\n",
    "     $$\n",
    "       \\tau = \\frac{\\sum_{i=1}^n |\\,\\text{batch}_i|}{T_\\text{total}}\\quad[\\tfrac{\\text{tokens}}{\\text{s}}].\n",
    "     $$\n",
    "   - **Latencia de batch**: $L_b = t_\\text{fin} - t_\\text{inicio}$. Graficar percentiles $p_{50}, p_{90}, p_{99}$.  \n",
    "   - **Uso de memoria**: trackear RSS y ajustarse para que nunca exceda un umbral $M_{\\max}$.\n",
    "\n",
    "5. **Iteraciones y mejoras**  \n",
    "   - Introducir **async I/O** (por ejemplo con `aiofiles`) y comparar contra hilos tradicionales.  \n",
    "   - Implementar **Compresión en vuelo** de batches usando LZ4 o Zstandard para reducir I/O.  \n",
    "   - Explorar **DataLoader distribuido**: sharding de corpus y uso de Redis como cola.\n",
    "\n",
    "**Entregables y entregas parciales**:  \n",
    "- Diseño de la arquitectura y pruebas de I/O secuencial vs paralelo.  \n",
    "- Integración de normalización y lematización.  \n",
    "- Métricas de rendimiento y optimizaciones de compresión.  \n",
    "- Reporte final con comparativa de throughput y latencia, código en GitHub y notebook reproducible.\n",
    "\n",
    "\n",
    "### **Proyecto 2: Tokenización avanzada y compresión de vocabulario con BPE**\n",
    "\n",
    "El objetivo es implementar un sistema de tokenización basado en **Byte-Pair Encoding (BPE)** que ajuste dinámicamente su vocabulario para maximizar cobertura y minimizar OOV, midiendo su impacto en modelos de lenguaje n‑grama y neuronales.\n",
    "\n",
    "1. **Implementación de BPE desde cero**  \n",
    "   - **Inicialización**: partir de un vocabulario de caracteres.  \n",
    "   - **Conteo de pares**: en cada iteración, calcular la frecuencia de cada par de sub‑tokens $(x,y)$.  \n",
    "   - **Merge greedily**: seleccionar el par con mayor frecuencia  \n",
    "     $$\n",
    "       (x^*,y^*) = \\arg\\max_{(x,y)} \\mathrm{count}(x\\,y)\n",
    "     $$\n",
    "     y reemplazar todas sus apariciones por un nuevo token `xy`.  \n",
    "   - **Complejidad**: usar estructuras `heap` para actualizar en $O(\\log V)$ por merge, total de $O(M \\log V)$ donde $M$ es número de merges.\n",
    "\n",
    "2. **Configuración y métricas**  \n",
    "   - **Tamaño de vocabulario** $V$: experimentar con $V\\in\\{10^4,2\\!\\times10^4,5\\!\\times10^4\\}$.  \n",
    "   - **Subword regularization**: implementar técnica de sampling para merges estocásticos, como en SentencePiece.  \n",
    "   - **Cobertura de OOV**:  \n",
    "     $$\n",
    "       \\text{OOV rate} = 1 - \\frac{\\#\\text{tokens cubiertos}}{\\#\\text{tokens totales}}.\n",
    "     $$\n",
    "\n",
    "3. **Impacto en modelos de lenguaje**  \n",
    "   - **n‑grama**: entrenar modelos de orden 3–5 con distintos BPE y medir perplejidad.  \n",
    "   - **Neuronal**: usar un RNN sencillo (sin gates) con embedding size fijo y comparar convergencia.\n",
    "\n",
    "4. **Dimensión semántica de los sub‑tokens**  \n",
    "   - Visualizar embeddings de sub‑tokens con PCA o t-SNE.  \n",
    "   - Analizar merges lingüísticos: caen en morfemas o generan 'juntas' arbitrarias.\n",
    "\n",
    "5. **Entregas y pruebas**  \n",
    "   - Implementación básica de BPE.  \n",
    "   - Añadir subword regularization y medir OOV.  \n",
    "   - Entrenar n‑grama y RNN breve, evaluar perplejidad y curva de aprendizaje.  \n",
    "   - Análisis cualitativo de sub‑tokens, muestras de merges y reporte completo.\n",
    "\n",
    "\n",
    "\n",
    "### **Proyecto 3: Análisis comparativo de técnicas de suavizado en modelos n‑grama**\n",
    "\n",
    "En este proyecto, se implementan y comparan exhaustivamente distintas estrategias de suavizado para modelos n‑grama, evaluando su impacto en métricas de calidad y distribución de probabilidad.\n",
    "\n",
    "1. **Modelos n‑grama y MLE**  \n",
    "   - Definir probabilidad MLE de un n‑grama:\n",
    "     $$\n",
    "       P_{\\mathrm{MLE}}(w_n\\mid w_{n-N+1}^{n-1}) = \\frac{c(w_{n-N+1}^n)}{c(w_{n-N+1}^{n-1})}.\n",
    "     $$\n",
    "\n",
    "2. **Suavizado aditivo (Laplace y Add‑$k$)**  \n",
    "   - **Laplace** ($k=1$):\n",
    "     $$\n",
    "       P_{\\mathrm{Lap}}(w) = \\frac{c(w) + 1}{C + V}.\n",
    "     $$\n",
    "   - **Add‑$k$**:\n",
    "     $$\n",
    "       P_{\\mathrm{Add}\\,k}(w) = \\frac{c(w) + k}{C + k\\,V}.\n",
    "     $$\n",
    "\n",
    "3. **Descuento absoluto y backoff de Katz**  \n",
    "   - **Absolute discounting**:\n",
    "     $$\n",
    "       P_{\\mathrm{AD}}(w_n\\mid h) = \\frac{\\max(c(h\\,w_n) - D,0)}{c(h)} + \\alpha(h)\\,P_{\\mathrm{lower}}(w_n\\mid h').\n",
    "     $$\n",
    "   - **Backoff de Katz**: aplicar descuento sólo si $c>D$; en otro caso, retroceder a $n-1$-grama.\n",
    "\n",
    "4. **Interpolación lineal**  \n",
    "   - Combinar K órdenes:\n",
    "     $$\n",
    "       P_{\\mathrm{INT}}(w_n\\mid h) = \\sum_{i=1}^N \\lambda_i\\,P_{\\mathrm{MLE}}(w_n\\mid h_i),\\quad \\sum\\lambda_i=1.\n",
    "     $$\n",
    "\n",
    "5. **Suavizado de Kneser–Ney**  \n",
    "   - **Kneser–Ney** modificado:\n",
    "     $$\n",
    "       P_{\\mathrm{KN}}(w_n\\mid h) = \\frac{\\max(c(h\\,w_n)-D,0)}{c(h)} + \\gamma(h)\\,P_{\\mathrm{cont}}(w_n),\n",
    "     $$\n",
    "     donde\n",
    "     $$\n",
    "       P_{\\mathrm{cont}}(w) = \\frac{\\#\\{\\text{historia }h':h'\\,w\\}}{\\sum_{w'}\\#\\{\\!h'\\!:\\!h'\\,w'\\}}.\n",
    "     $$\n",
    "   - Calcular $\\gamma(h)=\\frac{D}{c(h)}\\times|\\{w:c(h\\,w)>0\\}|$.\n",
    "\n",
    "6. **Evaluación y métricas**  \n",
    "   - **Perplejidad** en conjunto de validación:\n",
    "     $$\n",
    "       \\text{PP} = \\exp\\Bigl(-\\tfrac{1}{N}\\sum_{i=1}^N\\log P(w_i\\mid w_{1}^{i-1})\\Bigr).\n",
    "     $$\n",
    "   - **Held‑out likelihood**: producto de probabilidades en datos no vistos.  \n",
    "   - Experimentar con ruido sintético (inserción de errores tipográficos) y medir degradación.\n",
    "\n",
    "7. **Cronograma**  \n",
    "   - MLE, Laplace y Add‑$k$.  \n",
    "   - Absolute discounting y Katz backoff.  \n",
    "   - Interpolación lineal y ajuste de $\\lambda$.  \n",
    "   - Implementación completa de Kneser–Ney, evaluación comparativa con gráficos de PP y análisis de trade‑offs.\n",
    "\n",
    "\n",
    "### **Proyecto 4: Descuento y backoff \"online\" para n‑gramas en flujo de datos**\n",
    "\n",
    "Este reto propone diseñar un sistema de **suavizado dinámico** que ajuste parámetros de descuento y backoff en tiempo real sobre flujos de texto—fundamental para servicios de lenguaje en producción.\n",
    "\n",
    "1. **Manejo de flujo de datos**  \n",
    "   - Emplear un generador infinito de texto (por ejemplo, logs de chat).  \n",
    "   - Utilizar un **Sliding Window** de tamaño $W$ para mantener en memoria solo los últimos $W$ tokens.\n",
    "\n",
    "2. **Conteo aproximado de n‑gramas**  \n",
    "   - Para escalabilidad, usar un **Count–Min Sketch** (CMS) con parámetros $(\\epsilon,\\delta)$, de modo que  \n",
    "     $$\n",
    "       \\hat{c}(w) \\le c(w) + \\epsilon \\|\\mathbf{c}\\|_1 \\quad\\text{con probabilidad }1-\\delta.\n",
    "     $$\n",
    "   - Mantener CMS por órdenes de n‑grama (1 a $N$).\n",
    "\n",
    "3. **Actualización de parámetros de descuento**  \n",
    "   - Estimar dinámicamente el descuento $D$ según frecuencia de co‑ocurrencias en ventana:  \n",
    "     $$\n",
    "       D = \\frac{N_1}{N_1 + 2N_2},\n",
    "     $$\n",
    "     donde $N_k$ es el número de n‑gramas vistos exactamente $k$ veces en la ventana.\n",
    "\n",
    "4. **Backoff \"online\"**  \n",
    "   - Calcular la normalización $\\alpha(h)$ en cada paso usando conteos parciales de CMS.  \n",
    "   - Implementar versiones streaming de  \n",
    "     $$\n",
    "       P(w\\mid h) = \n",
    "       \\begin{cases}\n",
    "         \\frac{\\max(\\hat{c}(h\\,w)-D,0)}{\\hat{c}(h)} & \\hat{c}(h)>0,\\\\\n",
    "         \\alpha(h)\\,P(w\\mid h')                & \\text{en otro caso.}\n",
    "       \\end{cases}\n",
    "     $$\n",
    "\n",
    "5. **Algoritmo**  \n",
    "   ```pseudo\n",
    "   init CMSs for orders 1..N\n",
    "   for each token t in stream:\n",
    "     update_CMSs(window.append(t))\n",
    "     if window.size>W: pop_left(window)\n",
    "     current_ngram = last N tokens\n",
    "     estimate c, D, α\n",
    "     output P(t | history) via discounted-backoff\n",
    "   ```\n",
    "\n",
    "6. **Métricas de calidad**  \n",
    "   - **Degradación de perplejidad** con ventanas de distintos tamaños $W$.  \n",
    "   - **Latencia de cálculo** por token (máximo < 1 ms).  \n",
    "   - **Memoria usada** por CMS ($O(\\frac{1}{\\epsilon}\\log\\frac{1}{\\delta})$).\n",
    "\n",
    "7. **Cronograma**  \n",
    "   - Prototipo secuencial con ventanas fijas.  \n",
    "   - Integración de CMS y cálculo incremental de $D$.  \n",
    "   - Implementación de backoff completo \"online\".  \n",
    "   - Pruebas de latencia y memoria; ajuste de $\\epsilon,\\delta$.  \n",
    "   - Reporte con comparativas y recomendaciones de configuración.\n",
    "\n",
    "\n",
    "### **Proyecto 5: Retropropagación a través del tiempo y ajuste de hiperparámetros en RNN con puertas**\n",
    "\n",
    "En este trabajo se construye y ajusta un RNN con puertas (LSTM o GRU) aplicado a modelado de lenguaje, prestando especial atención a la **Retropropagación a través del tiempo (BPTT)** y la estabilidad del entrenamiento.\n",
    "\n",
    "1. **Definición de la arquitectura**  \n",
    "   - **LSTM**:  \n",
    "     $$\n",
    "       f_t = \\sigma(W_f[x_t,h_{t-1}]+b_f),\\quad\n",
    "       i_t = \\sigma(W_i[x_t,h_{t-1}]+b_i),\n",
    "     $$\n",
    "     $$\n",
    "       \\tilde{c}_t = \\tanh(W_c[x_t,h_{t-1}]+b_c),\\quad\n",
    "       c_t = f_t\\odot c_{t-1} + i_t\\odot \\tilde{c}_t,\n",
    "     $$\n",
    "     $$\n",
    "       o_t = \\sigma(W_o[x_t,h_{t-1}]+b_o),\\quad\n",
    "       h_t = o_t\\odot\\tanh(c_t).\n",
    "     $$\n",
    "   - **GRU** (opcional): similar pero con menos puertas.\n",
    "\n",
    "2. **BPTT y cálculo de gradientes**  \n",
    "   - Para un loss $L$ sobre secuencia de largo $T$:\n",
    "     $$\n",
    "       \\frac{\\partial L}{\\partial W_h}\n",
    "       = \\sum_{t=1}^T \\frac{\\partial L}{\\partial h_t}\\frac{\\partial h_t}{\\partial W_h}\n",
    "       = \\sum_{t=1}^T \\delta_t\\;h_{t-1}^\\top.\n",
    "     $$\n",
    "   - **Truncamiento** de BPTT en ventanas de longitud $k$ para limitar costo computacional.\n",
    "\n",
    "3. **Problemas de vanishing/exploding gradients**  \n",
    "   - **Clip de gradiente**:  \n",
    "     $$\n",
    "       \\hat{g} = \n",
    "       \\begin{cases}\n",
    "         g & \\|g\\|\\le C,\\\\\n",
    "         \\frac{C}{\\|g\\|}\\,g & \\text{si }\\|g\\|>C.\n",
    "       \\end{cases}\n",
    "     $$\n",
    "   - **Inicialización de pesos**: usar Xavier/He.\n",
    "\n",
    "4. **Selección de hiperparámetros**  \n",
    "   - **Learning rate** $\\alpha$ (grid search o bayesiano).  \n",
    "   - **Batch size** $B$.  \n",
    "   - **Hidden size** $H$.  \n",
    "   - **Dropout** entre capas.  \n",
    "   - **Clipping norm** $C$.\n",
    "\n",
    "5. **Evaluación**  \n",
    "   - **Curvas de entrenamiento** (loss vs epoch).  \n",
    "   - Medir **perplejidad** al final de cada epoch:\n",
    "     $$\n",
    "       \\mathrm{PP} = \\exp\\bigl(\\tfrac{L}{N}\\bigr).\n",
    "     $$\n",
    "   - Graficar gradientes medios y distribuciones para confirmar estabilidad.\n",
    "\n",
    "6. **Plan de entrega**  \n",
    "   - Implementación básica de LSTM/GRU y BPTT truncado.  \n",
    "   - Explorar efectos de $\\alpha,B,H$.  \n",
    "   - Integrar clipping y dropout, analizar gradientes.  \n",
    "   - Entrenamiento completo, recolectar métricas.  \n",
    "   - Fine‑tuning automático de hiperparámetros.  \n",
    "   - Comparar arquitecturas (LSTM vs GRU), documentar resultados.\n",
    "\n",
    "\n",
    "### **Proyecto 6: Evaluación de modelos de lenguaje recurrentes: perplejidad y palabras fuera de vocabulario**\n",
    "\n",
    "Este proyecto gira en torno a la **evaluación rigurosa** de modelos RNN para lenguaje, con foco en métricas clave y manejo de OOV.\n",
    "\n",
    "1. **Definición de métricas**  \n",
    "   - **Perplejidad (PP)**:\n",
    "     $$\n",
    "       \\mathrm{PP}(W) = \\exp\\Bigl(-\\tfrac{1}{N}\\sum_{i=1}^N\\log P(w_i\\mid w_{1}^{i-1})\\Bigr).\n",
    "     $$\n",
    "   - **Held‑out Likelihood**:  \n",
    "     $$\n",
    "       L_{\\text{held}} = \\prod_{i=1}^{N_{\\text{held}}}P(w_i\\mid w_{1}^{i-1}).\n",
    "     $$\n",
    "   - **Coverage de vocabulario**: porcentaje de tokens en test presentes en vocabulario de entrenamiento.\n",
    "\n",
    "2. **Gestión de OOV**  \n",
    "   - **Token `<UNK>`**: reemplazar todo token raro por un token UNK.  \n",
    "   - **Backoff a modelo de caracteres**: usar un RNN char‑level para generar UNK a nivel subword.  \n",
    "   - **Distancia de Levenshtein** para asignar candidatos cercanos:  \n",
    "     $$\n",
    "       \\hat{w} =\n",
    "       \\arg\\min_{v\\in V}\\,d_{\\text{Lev}}(w,v).\n",
    "     $$\n",
    "\n",
    "3. **Diseño experimental**  \n",
    "   - Entrenar un LSTM de 2 capas con embedding de dimensión 300 y hidden size 512 sobre un corpus estándar (WikiText-2).  \n",
    "   - Generar tres variantes de vocabulario ($10^4, 3\\times10^4, 5\\times10^4$).\n",
    "\n",
    "4. **Análisis de resultados**  \n",
    "   - Reportar PP en validación y test.  \n",
    "   - Medir **rate de OOV**:  \n",
    "     $$\n",
    "       \\mathrm{OOV\\%} = 100\\cdot\\frac{|\\{w_i:w_i\\notin V\\}|}{N}.\n",
    "     $$\n",
    "   - Comparar estrategias: UNK puro vs. backoff char‑level vs. Levenshtein.\n",
    "\n",
    "5. **Cronograma**  \n",
    "   - Preparar datasets y pipeline de entrenamiento.  \n",
    "   - Entrenar y evaluar vocab $10^4$.  \n",
    "   - Entrenar vocab $3\\times10^4$, medir OOV.  \n",
    "   - Implementar backoff char‑level.  \n",
    "   - Medir Levenshtein reassignment.  \n",
    "   - Compilar métricas y gráficos.  \n",
    "   - Escribir reporte con tablas de PP vs OOV y recomendaciones.\n",
    "\n",
    "### **Proyecto 7: Algoritmo de Viterbi y notación de semianillos en HMM para etiquetado de secuencias**\n",
    "\n",
    "El objetivo es implementar y generalizar el **algoritmo de Viterbi** para HMM clásicos y su formulación con **semianillos**, explorando distintos semiring y sus aplicaciones en etiquetado de secuencias.\n",
    "\n",
    "1. **Definición de HMM**  \n",
    "   - Estados $S=\\{s_1,\\dots,s_K\\}$, observaciones $O$, matriz de transición $A=[a_{ij}]$ y de emisión $B=[b_i(o)]$.  \n",
    "   - Objetivo: encontrar la secuencia $s_{1:T}^*$ que maximiza $P(s_{1:T},o_{1:T})$.\n",
    "\n",
    "2. **Viterbi clásico**  \n",
    "   - Recurrencia:\n",
    "     $$\n",
    "       \\delta_t(i) = \\max_{1\\le j\\le K}\\bigl[\\delta_{t-1}(j)\\,a_{j,i}\\bigr]\\;b_i(o_t),\n",
    "     $$\n",
    "     con\n",
    "     $\\delta_1(i)=\\pi_i b_i(o_1)$.  \n",
    "   - **Backpointer**:\n",
    "     $$\n",
    "       \\psi_t(i) = \\arg\\max_j[\\delta_{t-1}(j)\\,a_{j,i}].\n",
    "     $$\n",
    "   - Reconstrucción:\n",
    "     $\\hat{s}_T = \\arg\\max_i \\delta_T(i)$, luego retroceder con $\\psi$.\n",
    "\n",
    "3. **Semianillos y formulación general**  \n",
    "   - Un **semiring** es $(\\mathcal{K},\\oplus,\\otimes,0,1)$.  \n",
    "   - Viterbi ↔ $\\oplus=\\max,\\;\\otimes=\\times$.  \n",
    "   - Forward ↔ $\\oplus=+\\,,\\;\\otimes=\\times$.  \n",
    "   - Recurrencia general:\n",
    "     $$\n",
    "       \\delta_t(i)=\\bigoplus_j\\bigl[\\delta_{t-1}(j)\\otimes u_{j,i}(o_t)\\bigr].\n",
    "     $$\n",
    "\n",
    "4. **Aplicación y extensiones**  \n",
    "   - Implementar Viterbi con diferentes semiring (por ejemplo, log‑semiring para evitar underflow).  \n",
    "   - Extender a **Viterbi generalizado** con semianillos de conteo para enumerar las $k$ mejores trayectorias.\n",
    "\n",
    "5. **Plan de actividades**  \n",
    "   - Codificar HMM básico y Viterbi clásico.  \n",
    "   - Backpointer y reconstrucción de secuencia.  \n",
    "   - Definir abstracción semiring y reescribir Viterbi genérico.  \n",
    "   - Implementar semiring de sum‑product y log‑semiring.  \n",
    "   - Extender para top‑$k$ paths (k‑best Viterbi).  \n",
    "   - Pruebas en corpus de POS-tagging y análisis de precisión.\n",
    "\n",
    "\n",
    "### **Proyecto 8: Etiquetado de secuencias discriminativo: perceptrón estructurado, SVM estructurada y CRF**\n",
    "\n",
    "Comparar tres enfoques discriminativos para **sequence labeling**, desde algoritmos online sencillos hasta modelos probabilísticos complejos.\n",
    "\n",
    "1. **Perceptrón estructurado**  \n",
    "   - **Función de puntuación**: $F(x,y;\\theta)=\\theta^\\top f(x,y)$.  \n",
    "   - **Actualización online**:\n",
    "     $$\n",
    "       \\theta \\leftarrow \\theta + f(x,y^{(t)}) - f(x,\\hat y^{(t)}),\n",
    "     $$\n",
    "     donde $\\hat y^{(t)}=\\arg\\max_y F(x,y;\\theta)$.  \n",
    "   - **Features**:  \n",
    "     - Unigramas y bigramas de etiquetas ($y_{t-1},y_t$).  \n",
    "     - Prefijos y sufijos de la palabra $x_t$.  \n",
    "     - Indicadores de mayúscula, dígitos y morfología.\n",
    "\n",
    "2. **SVM estructurada (Structured SVM)**  \n",
    "   - **Objetivo primal**:\n",
    "     $$\n",
    "       \\min_{\\theta,\\xi} \\;\\tfrac{1}{2}\\|\\theta\\|^2 + C\\sum_i \\xi_i,\n",
    "     $$\n",
    "     sujeto a  \n",
    "     $\\forall i,y\\neq y_i:\\;\\theta^\\top[f(x_i,y_i)-f(x_i,y)]\\ge \\Delta(y_i,y)-\\xi_i.$  \n",
    "   - Usar **cutting-plane** (algoritmo de Joachims) o **SGD**.\n",
    "\n",
    "3. **Conditional Random Fields (CRF)**  \n",
    "   - **Log‑likelihood**:\n",
    "     $$\n",
    "       L(\\theta) = \\sum_{i=1}^M \\Bigl[\\theta^\\top f(x^{(i)},y^{(i)}) - \\log Z_\\theta(x^{(i)})\\Bigr],\n",
    "     $$\n",
    "     donde  \n",
    "     $$\n",
    "       Z_\\theta(x)=\\sum_{y'}\\exp\\bigl(\\theta^\\top f(x,y')\\bigr).\n",
    "     $$\n",
    "   - Gradiente:\n",
    "     $\\nabla L=\\sum_i [f(x,y^{(i)}) - \\mathbb{E}_{y'\\sim p_\\theta}[f(x,y')]]$.  \n",
    "   - Optimizar con L-BFGS o SGD estocástico.\n",
    "\n",
    "4. **Evaluación**  \n",
    "   - Métricas: **precision**, **recall**, **F1** a nivel de entidad.  \n",
    "   - Conjunto de datos: CoNLL‑2003 NER.  \n",
    "   - Comparar velocidad de entrenamiento y calidad.\n",
    "\n",
    "5. **Cronograma**  \n",
    "   - Implementar perceptrón estructurado.  \n",
    "   - Entrenar y evaluar perceptrón.  \n",
    "   - Codificar Structured SVM (usar lib SVM-light estructural).  \n",
    "   - Entrenar SVM y recoger métricas.  \n",
    "   - Implementar CRF (usar CRFsuite o desde cero).  \n",
    "   - Comparar resultados, ablation de features y reporte técnico.\n",
    "\n",
    "\n",
    "### **Proyecto 9: Etiquetado de secuencias con redes neuronales: RNNs y CNNs a nivel de carácter**\n",
    "\n",
    "Se crea un modelo híbrido de **CNN char‑level + RNN word‑level** para resolver tasks de NER o POS-tagging, aprovechando información morfológica.\n",
    "\n",
    "1. **Módulo char‑CNN**  \n",
    "   - **Embedding de caracteres**: vector de dimensión $d_c$.  \n",
    "   - **Convoluciones 1D** con filtros de anchuras $\\{3,5,7\\}$ y $F$ mapas cada uno.  \n",
    "   - **Max‑pooling** sobre longitud variable para obtener vector fijo por palabra:\n",
    "     $$\n",
    "       v_{\\text{char}} = \\max_{i=1\\dots L}\\bigl(\\mathrm{CNN}(c_1,\\dots,c_L)\\bigr).\n",
    "     $$\n",
    "\n",
    "2. **Combinación con embedding de palabra**  \n",
    "   - Concatenar $v_{\\text{char}}$ con embedding preentrenado (o aleatorio) $v_{\\text{word}}$.  \n",
    "   - Resultado $x_t = [v_{\\text{char}};v_{\\text{word}}]$.\n",
    "\n",
    "3. **RNN bidireccional**  \n",
    "   - Usar **BiLSTM**:\n",
    "     $\\overrightarrow{h}_t = \\mathrm{LSTM}(x_t,\\overrightarrow{h}_{t-1})$,  \n",
    "     $\\overleftarrow{h}_t = \\mathrm{LSTM}(x_t,\\overleftarrow{h}_{t+1})$.  \n",
    "   - Concatenar $[\\,\\overrightarrow{h}_t;\\overleftarrow{h}_t]$ y pasar a capa densa.\n",
    "\n",
    "4. **Capa de etiquetado y pérdida**  \n",
    "   - **Softmax** sobre etiquetas:\n",
    "     $$\n",
    "       \\hat y_t = \\mathrm{softmax}(W[h_t] + b).\n",
    "     $$\n",
    "   - **Loss**: cross‑entropy sumado sobre $t$:\n",
    "     $\\mathcal{L} = -\\sum_{t=1}^T \\log \\hat y_t[y_t]$.\n",
    "\n",
    "5. **Optimización y regularización**  \n",
    "   - **Adam** con $\\alpha=0.001$.  \n",
    "   - **Dropout** en embedders y salida de RNN (p=0.5).  \n",
    "   - **Early stopping** con paciencia de 5 epochs.\n",
    "\n",
    "6. **Plan de trabajo**  \n",
    "   - Codificar char‑CNN y probar extracción de features morfológicos.  \n",
    "   - Integrar embedding de palabra y RNN unidireccional.  \n",
    "   - Cambiar a BiLSTM y ajustar hiperparámetros.  \n",
    "   - Añadir dropout y early stopping.  \n",
    "   - Entrenar en CoNLL‑2003 y medir precision/recall.  \n",
    "   - Análisis de errores por palabra (OOV vs IV) y reporte final.\n",
    "\n",
    "### **Proyecto 10: Semántica vectorial y evaluación de representaciones: TF‑IDF, PMI, Word2vec (CBOW vs Skip‑gram)**\n",
    "\n",
    "Este proyecto explora métodos clásicos y neuronales para obtener **representaciones de palabras**, evaluando su calidad intrínseca y extrínseca.\n",
    "\n",
    "1. **TF‑IDF y PMI**  \n",
    "   - **TF‑IDF**:\n",
    "     $$\n",
    "       \\text{tfidf}_{t,d} = \\text{tf}_{t,d}\\times \\log\\frac{N}{\\text{df}_t}.\n",
    "     $$\n",
    "   - **PMI** para par $(w,c)$:\n",
    "     $$\n",
    "       \\mathrm{PMI}(w,c) = \\log\\frac{P(w,c)}{P(w)P(c)}.\n",
    "     $$\n",
    "\n",
    "2. **Word2vec: CBOW y Skip‑gram**  \n",
    "   - **CBOW** objetivo:\n",
    "     $$\n",
    "       \\max_\\theta \\sum_{t=1}^T \\log P(w_t\\mid w_{t-k}^{t-1},w_{t+1}^{t+k}),\n",
    "     $$\n",
    "     con\n",
    "     $\\;P(w_t\\mid \\cdot) = \\mathrm{softmax}(v_{w_t}^\\top \\bar{v})$.  \n",
    "   - **Skip‑gram** objetivo:\n",
    "     $$\n",
    "       \\max_\\theta \\sum_{t=1}^T \\sum_{-k\\le j\\le k,\\,j\\neq0}\\log P(w_{t+j}\\mid w_t).\n",
    "     $$\n",
    "   - **Negative sampling**:\n",
    "     $$\n",
    "       \\log\\sigma(v_c^\\top v_w) + \\sum_{i=1}^K\\log\\sigma(-v_{c_i}^\\top v_w).\n",
    "     $$\n",
    "\n",
    "3. **Evaluación intrínseca**  \n",
    "   - **Similitud de coseno**:\n",
    "     $$\n",
    "       \\cos(\\mathbf{u},\\mathbf{v}) = \\frac{\\mathbf{u}\\cdot \\mathbf{v}}{\\|\\mathbf{u}\\|\\|\\mathbf{v}\\|}.\n",
    "     $$\n",
    "   - **Analogías**: resolver $v_{king}-v_{man}+v_{woman}\\approx v_{queen}$.  \n",
    "   - Correlación Spearman con benchmarks (WordSim-353).\n",
    "\n",
    "4. **Evaluación extrínseca**  \n",
    "   - **Clasificación de documentos**: entrenar SVM con features TF‑IDF vs embeddings promedio.  \n",
    "   - Métricas: **accuracy**, **F1**, **ROC‑AUC**.\n",
    "\n",
    "5. **Análisis de sesgo y equidad**  \n",
    "   - **WEAT** (Word Embedding Association Test): calcular diferenciales $d$.  \n",
    "   - Probar reducción de sesgo con **hard‑debiasing** (Bolukbasi et al.).\n",
    "\n",
    "6. **Cronograma**  \n",
    "   - Calcular TF‑IDF y PMI; entrenar modelos Word2vec.  \n",
    "   - Evaluación intrínseca.  \n",
    "   - Experimento extrínseco en clasificación.  \n",
    "   - Medir y mitigar sesgos.  \n",
    "   - Consolidar resultados, graficar comparativas de coseno, correlaciones y métricas de clasificación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3231a69a-a8a3-489b-b735-24a818e6f7fe",
   "metadata": {},
   "source": [
    "### **Proyecto 11: Explorando librerías de IA generativa y creación de un pipeline modular**\n",
    "\n",
    "**Objetivo.** Evaluar y comparar las principales librerías de IA generativa (por ejemplo, Hugging Face Transformers, OpenAI API, Diffusers) y construir un _pipeline_ modular que permita experimentar con distintos modelos, tokenizadores y estrategias de _prompting_.\n",
    "\n",
    "1. **Selección y evaluación de librerías**  \n",
    "   - **Transformers** (Hugging Face): carga de modelos preentrenados, _tokenizers_ rápidos en Rust, y generación con _top-k_ y _nucleus sampling_.  \n",
    "   - **OpenAI API**: abstracción de llamadas REST, límites de tasa y _streaming_.  \n",
    "   - **Diffusers**: para tareas multimodales (texto→imagen), uso de pipelines como `StableDiffusionPipeline`.  \n",
    "   - **Benchmark**: medir tiempo de inicialización, uso de GPU vs CPU, latencia por _token_ y calidad (mediante métricas cualitativas).\n",
    "\n",
    "2. **Diseño de un DataLoader/genérico**  \n",
    "   - Integra con el Proyecto 1: convierte corpus de texto e imágenes en batches mixtos.  \n",
    "   - Usa una interfaz común (\"`next_batch()`\"): cada batch contiene `input_ids`, `attention_mask`, y metadatos (longitudes, idiomas).  \n",
    "   - Permite combinar distintos _tokenizers_ (BPE, WordPiece, SentencePiece) en el mismo flujo.\n",
    "\n",
    "3. **Prompt engineering y estrategias de decodificación**  \n",
    "   - Implementar _sampling_ con hyperparámetros:  \n",
    "     $$\n",
    "       P_\\text{next}(w) \\propto  \n",
    "       \\begin{cases}\n",
    "         p(w) & \\text{si }p(w)\\ge p_{\\min},\\\\\n",
    "         0 & \\text{si }p(w)< p_{\\min},\n",
    "       \\end{cases}\n",
    "     $$  \n",
    "     donde $p_{\\min}$ surge de _top-p_ (nucleus) o _top-k_.  \n",
    "   - Medir diversidad con **self-BLEU**:  \n",
    "     $$\n",
    "       \\text{self-BLEU} = \\frac{1}{M}\\sum_{i=1}^M \\text{BLEU}(\\text{gen}_i,\\{\\text{gen}_j\\}_{j\\neq i}).\n",
    "     $$\n",
    "\n",
    "4. **Evaluación de calidad**  \n",
    "   - **Métricas automáticas**: BLEU, ROUGE-L, METEOR.  \n",
    "   - **Evaluación humana**: cuadrantes de coherencia, relevancia y fluidez en una escala Likert de 1–5.  \n",
    "   - Crear un pequeño panel de 5 evaluadores que ratingeen 50 muestras.  \n",
    "\n",
    "5. **Cronograma**  \n",
    "   - Instalar y fijar entornos para cada librería; pruebas de carga y generación de ejemplos mínimos.  \n",
    "   - Implementar DataLoader genérico y pipelines de entrada.  \n",
    "   - Integrar estrategias de _prompting_ y decodificación; primer benchmark de latencia.  \n",
    "   - Aplicar métricas automáticas; recoger resultados en tablas comparativas.  \n",
    "   - Organizar evaluación humana y procesar scores.  \n",
    "   - Comparación final, análisis de trade‑offs (velocidad vs. calidad), recomendaciones y código en repositorio público.\n",
    "\n",
    "\n",
    "### **Proyecto 12: Normalización, lematización y su impacto en modelos n‑grama y RNN**\n",
    "\n",
    "**Objetivo.** Diseñar distintos pipelines de preprocesamiento de texto centrados en normalización y lematización, y medir su efecto sobre modelos n‑grama y RNN.\n",
    "\n",
    "1. **Técnicas de preprocesamiento**  \n",
    "   - **Unicode normalization**: NFKC vs. NFD.  \n",
    "   - **Lowercasing** y manejo de acentos (`á→a`).  \n",
    "   - **Lematización**: SpaCy vs. NLTK vs. Porter stemmer.  \n",
    "   - **Distancia de Levenshtein** para agrupar variantes:  \n",
    "     $$\n",
    "       d_{\\text{Lev}}(s,t)=\n",
    "       \\begin{cases}\n",
    "         0 & s=t,\\\\\n",
    "         \\min\\{d(s_{1..|s|-1},t)+1,\\;d(s,t_{1..|t|-1})+1,\\;d(s_{1..|s|-1},t_{1..|t|-1})+\\mathbf{1}_{s_{|s|}\\neq t_{|t|}}\\}\n",
    "       \\end{cases}\n",
    "     $$\n",
    "\n",
    "2. **Pipelines a comparar**  \n",
    "   - **Pipeline A**: NFKC → minúsculas → tokenización whitespace → no lematización.  \n",
    "   - **Pipeline B**: Unicode NFC → lower → lematización SpaCy → filtro tokens < 3 caracteres.  \n",
    "   - **Pipeline C**: NFD → lower → stemmer Porter → agrupación Levenshtein ≤ 1.  \n",
    "\n",
    "3. **Evaluación en n‑grama y RNN**  \n",
    "   - Reentrenar modelo n‑grama de orden 4 con suavizado Kneser–Ney (proyecto 3) en cada pipeline.  \n",
    "   - Entrenar un LSTM de 2 capas (proyecto 5) con embeddings de 300 dimensiones.  \n",
    "   - Medir **perplejidad** ($\\mathrm{PP}$) en test:  \n",
    "     $$\n",
    "       \\mathrm{PP}=\\exp\\Bigl(-\\tfrac{1}{N}\\sum_{i=1}^N\\log P(w_i\\mid w_{<i})\\Bigr).\n",
    "     $$\n",
    "\n",
    "4. **Análisis de resultados**  \n",
    "   - Tabla comparativa de $\\mathrm{PP}$ y _held‑out likelihood_ para cada pipeline.  \n",
    "   - Tiempo de preprocesamiento por millón de tokens.  \n",
    "   - Impacto en _vocab size_ y **OOV rate**:\n",
    "     $$\n",
    "       \\text{OOV rate} = 1-\\frac{|\\{w_i:w_i\\in V\\}|}{N}.\n",
    "     $$\n",
    "\n",
    "5. **Cronograma**  \n",
    "   - Implementar y probar pipelines A, B y C.  \n",
    "   - Reentrenar n‑grama y recopilar métricas de PP.  \n",
    "   - Entrenar LSTM en cada pipeline; monitorear convergencia y PP.  \n",
    "   - Análisis estadístico (ANOVA) para verificar diferencias significativas.  \n",
    "   - Reporte con gráficos de PP vs. pipeline y recomendaciones de preprocesamiento.\n",
    "\n",
    "\n",
    "\n",
    "### **Proyecto 13: Modelos secuencia a secuencia con atención y métricas de evaluación de calidad**\n",
    "\n",
    "**Objetivo.** Construir un modelo Seq2Seq con atención para una tarea de traducción o resumen, y evaluar su desempeño con métricas como BLEU, ROUGE, precisión, recall y F1.\n",
    "\n",
    "1. **Arquitectura Seq2Seq con atención**  \n",
    "   - **Encoder**: BiLSTM de encoder de dimensión $H$.  \n",
    "   - **Decoder**: LSTM unidireccional, paso a paso, con atención global de Bahdanau:  \n",
    "     $$\n",
    "       e_{t,i} = v_a^\\top \\tanh(W_h h_i + W_s s_{t-1}),\\quad\n",
    "       \\alpha_{t,i} = \\frac{\\exp(e_{t,i})}{\\sum_j\\exp(e_{t,j})},\n",
    "     $$\n",
    "     $$\n",
    "       c_t = \\sum_i \\alpha_{t,i} h_i,\\quad\n",
    "       s_t = \\mathrm{LSTM}(y_{t-1},[s_{t-1};c_t]).\n",
    "     $$\n",
    "   - **Output**: cálculo de probabilidad con _softmax_:\n",
    "     $$\n",
    "       P(y_t\\mid y_{<t},x) = \\mathrm{softmax}(W_o[s_t;c_t]+b_o).\n",
    "     $$\n",
    "\n",
    "2. **Métricas de evaluación**  \n",
    "   - **BLEU** (n=1…4):\n",
    "     $$\n",
    "       \\mathrm{BLEU} = \\mathrm{BP}\\exp\\Bigl(\\sum_{n=1}^4 w_n\\log p_n\\Bigr),\\quad\n",
    "       \\mathrm{BP} = \\exp\\bigl(1-\\tfrac{r}{c}\\bigr)^+.\n",
    "     $$\n",
    "   - **ROUGE‑L** (Longest Common Subsequence).  \n",
    "   - **Precisión, Recall, F1** a nivel de _n_-grama:\n",
    "     $$\n",
    "       \\text{Precision} = \\frac{\\#\\text{ngramas predichos correctos}}{\\#\\text{ngramas predichos}},\n",
    "     \\quad\n",
    "       \\text{Recall} = \\frac{\\#\\text{ngramas predichos correctos}}{\\#\\text{ngramas de referencia}}.\n",
    "     $$\n",
    "\n",
    "3. **Tarea experimental**  \n",
    "   - Elegir dataset: IWSLT’14 (fr->en) o CNN/DailyMail (resumen).  \n",
    "   - Entrenar modelo con embedding de dimensión 256 y hidden 512.  \n",
    "   - Optimizar con Adam ($\\alpha=0.0005$), _dropout_ 0.3 y _early stopping_.\n",
    "\n",
    "4. **Plan de entregas**  \n",
    "   - Preparar datos y pipeline de lectura, tokenización BPE (proyecto 2).  \n",
    "   - Implementar encoder/decoder y atención básica.  \n",
    "   - Entrenar y ajustar hiperparámetros; graficar _loss_ vs _epoch_.  \n",
    "   - Implementar cálculo de BLEU/ROUGE y generar ejemplos de salida.  \n",
    "   - Análisis de errores: atención mal enfocada y long sequence.  \n",
    "   - Reporte final con métricas, ejemplos y recomendaciones para producción.\n",
    "\n",
    "\n",
    "### **Proyecto 14: Etiquetado de secuencias no supervisado y sistemas dinámicos lineales**\n",
    "\n",
    "**Objetivo.** Explorar métodos de **etiquetado de secuencias sin supervisión**, combinando HMM entrenados por EM, clúster jerárquico y **sistemas dinámicos lineales (LDS)** para descubrir etiquetas latentes.\n",
    "\n",
    "1. **HMM no supervisado con EM**  \n",
    "   - Parámetros: $\\pi_i$, $A=[a_{ij}]$, emisiones $B=[b_i(x)]$.  \n",
    "   - **E‑step**: calcular $\\gamma_t(i)=P(s_t=i\\mid x_{1:T})$ y $\\xi_t(i,j)=P(s_{t-1}=i,s_t=j\\mid x_{1:T})$ con forward-backward.  \n",
    "   - **M‑step**: reestimar  \n",
    "     $$\n",
    "       \\pi_i' = \\gamma_1(i),\\quad\n",
    "       a_{ij}' = \\frac{\\sum_{t=2}^T\\xi_t(i,j)}{\\sum_{t=2}^T\\gamma_{t-1}(i)},\\quad\n",
    "       b_i'(x_k) = \\frac{\\sum_{t:x_t=x_k}\\gamma_t(i)}{\\sum_{t=1}^T\\gamma_t(i)}.\n",
    "     $$\n",
    "\n",
    "2. **Clúster jerárquico de secuencias**  \n",
    "   - Representar cada secuencia como vector de frecuencias de n‑gramas.  \n",
    "   - Aplicar **agglomerative clustering** con distancia de Coseno.  \n",
    "   - Asignar etiquetas según cluster.\n",
    "\n",
    "3. **Sistemas dinámicos lineales (LDS)**  \n",
    "   - Formulación de Kalman Filter:  \n",
    "     $$\n",
    "       x_{t+1} = F x_t + w_t,\\quad\n",
    "       y_t = H x_t + v_t,\\quad\n",
    "       w_t\\sim\\mathcal{N}(0,Q),\\;v_t\\sim\\mathcal{N}(0,R).\n",
    "     $$\n",
    "   - Estimar $F,H,Q,R$ con EM de LDS.  \n",
    "   - Uso de estados latentes $x_t$ como etiquetas suavizadas.\n",
    "\n",
    "4. **Comparativa y evaluation**  \n",
    "   - Medir **coherencia interna** de clusters (Silhouette).  \n",
    "   - Para HMM y LDS, evaluar **perplejidad** como proxy de ajuste.  \n",
    "   - Visualizar trayectorias latentes en 2D con PCA.\n",
    "\n",
    "5. **Cronograma**  \n",
    "   - Implementar HMM+EM y probar convergencia.  \n",
    "   - clustering jerárquico y análisis de Silhouette.  \n",
    "   - Formular y entrenar LDS con EM; filtrar secuencias.  \n",
    "   - Comparar etiquetas y evaluar perplejidad/clustering.  \n",
    "   - Reporte final, visualizaciones de trayectorias latentes y discusión de ventajas y limitaciones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8499c449-a8ae-4100-b773-a287eb47c7a1",
   "metadata": {},
   "source": [
    "### **Proyecto 15: Implementación de HMM y algoritmo de Viterbi con ejemplo de POS‑Tagging**\n",
    "\n",
    "**Objetivo.** Construir desde cero un **Hidden Markov Model (HMM)** de primer orden para etiquetado de partes de habla (POS‑tagging) y aplicar el **algoritmo de Viterbi** con backpointers, mostrando un ejemplo completo paso a paso.\n",
    "\n",
    "1. **Definición del HMM**  \n",
    "   - Conjunto de estados $S=\\{\\text{NOUN, VERB, ADJ,…}\\}$, observaciones $O$ (palabras).  \n",
    "   - Parámetros: $\\pi_i=P(s_1=i)$, $A=[a_{ij}=P(s_t=j\\mid s_{t-1}=i)]$ y $B=[b_j(o)=P(o\\mid s=j)]$.\n",
    "\n",
    "2. **Algoritmo de Viterbi**  \n",
    "   - **Inicialización** ($t=1$):\n",
    "     $$\n",
    "       \\delta_1(i)=\\pi_i\\,b_i(o_1),\\quad \\psi_1(i)=0.\n",
    "     $$\n",
    "   - **Recurrencia** ($t=2\\ldots T$):\n",
    "     $$\n",
    "       \\delta_t(j)=\\max_{i}\\bigl[\\delta_{t-1}(i)\\,a_{ij}\\bigr]\\;b_j(o_t),\\quad\n",
    "       \\psi_t(j)=\\arg\\max_{i}[\\delta_{t-1}(i)\\,a_{ij}].\n",
    "     $$\n",
    "   - **Terminación**:\n",
    "     $\\hat s_T=\\arg\\max_i\\delta_T(i)$ y retroceso usando $\\psi$.  \n",
    "   - **Backpointer**: reconstrucción $\\hat s_t=\\psi_{t+1}(\\hat s_{t+1})$.\n",
    "\n",
    "3. **Ejemplo detallado**  \n",
    "   - Corpus pequeño: \"Time flies like an arrow\".  \n",
    "   - Tablas de $\\pi$, $A$ y $B$ con valores sintéticos.  \n",
    "   - Cálculo manual de $\\delta_t$ y $\\psi_t$ para cada paso, mostrando matrices y vectores.\n",
    "\n",
    "4. **Características de orden superior**  \n",
    "   - Extender a trigramas de estados ($P(s_t\\mid s_{t-2},s_{t-1})$).  \n",
    "   - Modificar recurrencia:\n",
    "     $$\n",
    "       \\delta_t(k)\\!=\\!\\max_{i,j}[\\delta_{t-2}(i)\\,a_{ij,k}]\\;b_k(o_t),\n",
    "     $$\n",
    "     donde $a_{ij,k}=P(s_t=k\\mid s_{t-2}=i,s_{t-1}=j)$.\n",
    "\n",
    "5. **Entregables y cronograma**  \n",
    "   - Definición de estados, estimación puntual de $\\pi,A,B$.  \n",
    "   - Implementación de Viterbi de primer orden y ejemplo manual.  \n",
    "   - Pruebas y validación con corpus real (e.g., Brown Corpus).  \n",
    "   - Extensión a trigrama‑HMM y adaptación de código.  \n",
    "   - Informe comparativo de precisión y tiempo de ejecución.  \n",
    "   - Optimización de estructuras de datos (arrays vs. diccionarios) y preprocesamiento de etiquetas.  \n",
    "\n",
    "### **Proyecto 16: Estimación de parámetros en HMM con Baum–Welch y análisis de convergencia**\n",
    "\n",
    "**Objetivo.** Implementar el **algoritmo Baum–Welch** (EM para HMM) para estimar parámetros $\\pi,A,B$ a partir de datos sin etiquetas y analizar su convergencia y sensibilidad a la inicialización.\n",
    "\n",
    "1. **Formulación EM**  \n",
    "   - **E‑step**: calcular esperanzas usando forward-backward:  \n",
    "     $$\n",
    "       \\gamma_t(i)=P(s_t=i\\mid O,\\theta),\\quad\n",
    "       \\xi_t(i,j)=P(s_t=i,s_{t+1}=j\\mid O,\\theta).\n",
    "     $$\n",
    "   - **Forward** $\\alpha$ y **Backward** $\\beta$:  \n",
    "     $\\alpha_1(i)=\\pi_i b_i(o_1)$,  \n",
    "     $\\alpha_{t+1}(j)=\\sum_i\\alpha_t(i)a_{ij}b_j(o_{t+1})$.  \n",
    "     Similar para $\\beta$.\n",
    "\n",
    "2. **M‑step**: reestimar  \n",
    "   $$\n",
    "     \\pi_i'=\\gamma_1(i),\\quad\n",
    "     a_{ij}'=\\frac{\\sum_{t=1}^{T-1}\\xi_t(i,j)}{\\sum_{t=1}^{T-1}\\gamma_t(i)},\\quad\n",
    "     b_j'(k)=\\frac{\\sum_{t:o_t=k}\\gamma_t(j)}{\\sum_{t=1}^T\\gamma_t(j)}.\n",
    "   $$\n",
    "\n",
    "3. **Análisis de convergencia**  \n",
    "   - Ejecutar múltiples inicializaciones aleatorias.  \n",
    "   - Medir log‑likelihood en cada iteración:\n",
    "     $$\n",
    "       L(\\theta) = \\sum_{t=1}^T \\log\\Bigl(\\sum_i\\alpha_t(i)\\beta_t(i)\\Bigr).\n",
    "     $$\n",
    "   - Visualizar curvas de $L(\\theta)$ vs. iteraciones y comparar tiempo a convergencia.\n",
    "\n",
    "4. **Ejemplo**  \n",
    "   - Dataset de etiquetas ocultas: secuencias de POS parcialmente observadas.  \n",
    "   - Comparar parámetros estimados contra valores “verdaderos” en HMM sintético.\n",
    "\n",
    "5. **Entregables y cronograma**  \n",
    "   - Implementación de forward–backward y validación numérica.  \n",
    "   - E‑step y M‑step completos en toy example.  \n",
    "   - Pruebas con corpus real y captación de log‑likelihood.  \n",
    "   - Análisis de sensibilidad a $\\theta^{(0)}$.  \n",
    "   - Ajustes de regularización (pseudocounts).  \n",
    "   - Informe con tablas de convergencia y recomendaciones de inicialización.  \n",
    "\n",
    "### **Proyecto 17: Inferencia en HMM: Algoritmo forward–backward, entropía y probabilidad marginal**\n",
    "\n",
    "**Objetivo.** Profundizar en la **inferencia** de HMM calculando probabilidades marginales, entropía de secuencias y probabilidad de observaciones, usando el **algoritmo forward–backward** y variantes en el dominio log.\n",
    "\n",
    "1. **Forward–Backward**  \n",
    "   - **Forward** ($\\alpha$) como en proyecto 16.  \n",
    "   - **Backward** ($\\beta$):  \n",
    "     $\\beta_T(i)=1$,  \n",
    "     $\\beta_t(i)=\\sum_j a_{ij}\\,b_j(o_{t+1})\\,\\beta_{t+1}(j).$\n",
    "\n",
    "2. **Cálculo de probabilidades marginales**  \n",
    "   - $P(s_t=i\\mid O) = \\frac{\\alpha_t(i)\\beta_t(i)}{\\sum_k\\alpha_t(k)\\beta_t(k)}.$  \n",
    "   - Visualizar distribuciones marginales en cada posición $t$.\n",
    "\n",
    "3. **Entropía de la cadena**  \n",
    "   - Definir entropía condicional:\n",
    "     $$\n",
    "       H(S\\mid O) = -\\sum_{t=1}^T\\sum_i P(s_t=i\\mid O)\\log P(s_t=i\\mid O).\n",
    "     $$\n",
    "   - Interpretar incertidumbre del modelo ante diferente longitud de secuencia.\n",
    "\n",
    "4. **Probabilidad de la secuencia de observaciones**  \n",
    "   $$\n",
    "     P(O) = \\sum_i\\alpha_T(i)\\quad\\text{o en log‑dominio:}\\quad\n",
    "     \\log P(O) = \\log\\sum_i\\exp(\\log\\alpha_T(i)).\n",
    "   $$\n",
    "   - Implementar **log-sum-exp** para evitar underflow:\n",
    "     $$\n",
    "       \\log\\sum_i e^{x_i} = m + \\log\\sum_i e^{x_i - m},\\quad m=\\max_i x_i.\n",
    "     $$\n",
    "\n",
    "5. **Entregables y cronograma**  \n",
    "   - Forward–Backward en toy example; cálculo de $\\alpha,\\beta$.  \n",
    "   - Inferencia marginal y mapas de calor de $P(s_t)$.  \n",
    "   - Entropía y análisis de incertidumbre por longitud $T$.  \n",
    "   - Implementación log‑sum‑exp y pruebas numéricas.  \n",
    "   - Aplicar a corpus real y comparar entropías entre oraciones cortas y largas.  \n",
    "   - Reporte con gráficos de entropía vs. longitud y código modular.  \n",
    "\n",
    "\n",
    "### **Proyecto 18: HMM de orden superior y Viterbi extendido para modelos trigramáticos**\n",
    "\n",
    "**Objetivo.** Ampliar el HMM clásico a un **HMM de segundo orden** (trigramas de estados) e implementar la versión extendida de Viterbi para secuencias más ricas en dependencias de contexto.\n",
    "\n",
    "1. **Definición de HMM de orden 2**  \n",
    "   - Transiciones $a_{i,j,k}=P(s_t=k\\mid s_{t-2}=i, s_{t-1}=j)$.  \n",
    "   - Emisiones $b_k(o_t)$ como antes.\n",
    "\n",
    "2. **Viterbi extendido**  \n",
    "   - **Inicialización** (t=1,2):  \n",
    "     $\\delta_2(i,j)=\\pi_i\\,a_{i,j} \\,b_j(o_2)$.  \n",
    "   - **Recurrencia** ($t\\ge3$):\n",
    "     $$\n",
    "       \\delta_t(j,k)=\\max_i\\bigl[\\delta_{t-1}(i,j)\\,a_{i,j,k}\\bigr]\\,b_k(o_t),\n",
    "     $$\n",
    "     con backpointer $\\psi_t(j,k)=\\arg\\max_i[\\delta_{t-1}(i,j)\\,a_{i,j,k}]$.\n",
    "\n",
    "3. **Complejidad y optimizaciones**  \n",
    "   - Complejidad $O(T\\times|S|^3)$.  \n",
    "   - Proponer poda de estados de baja probabilidad ($\\delta<\\epsilon$).  \n",
    "   - Uso de estructuras dispersas si $A$ es esparso.\n",
    "\n",
    "4. **Ejemplo práctico**  \n",
    "   - Corpus limitado: dispositivos médicos con jerga.  \n",
    "   - Comparar Viterbi 1ro vs 2do orden en precisión de etiquetas.\n",
    "\n",
    "5. **Entregables y cronograma**  \n",
    "   - Implementar Viterbi de 2do orden en un ejemplo.  \n",
    "   - Crear corpus sintético para medir precisión.  \n",
    "   - Optimizar poda y probar efectos en velocidad.  \n",
    "   - Análisis comparativo de precisión vs costo computacional.  \n",
    "   - Extender a dato real (e.g., WSJ) y medir mejoras.  \n",
    "   - Informe final con gráficas de trade‑off y recomendaciones.  \n",
    "\n",
    "\n",
    "### **Proyecto 19: Etiquetado de secuencias discriminativo con perceptrón estructurado y características de orden superior**\n",
    "\n",
    "**Objetivo.** Implementar un modelo de **sequence labeling discriminativo** basado en **Perceptrón Estructurado** usando features de orden superior (trigramas de etiquetas, prefijos, sufijos, patrones morfológicos).\n",
    "\n",
    "1. **Función de puntuación**  \n",
    "   $$\n",
    "     F(x,y;\\theta)=\\sum_{t=1}^T \\theta\\cdot f_t(x,y_{t-2},y_{t-1},y_t),\n",
    "   $$\n",
    "   donde $f_t$ incluye:  \n",
    "   - Indicadores de trigramas $(y_{t-2},y_{t-1},y_t)$.  \n",
    "   - Prefijos/sufijos de longitud 3–5 de $x_t$.  \n",
    "   - Características ortográficas (mayúscula, dígitos).\n",
    "\n",
    "2. **Algoritmo online**  \n",
    "   - Para cada secuencia de entrenamiento $(x^{(i)},y^{(i)})$:  \n",
    "     - Inferir $\\hat y^{(i)}=\\arg\\max_y F(x^{(i)},y;\\theta)$ (usar Viterbi en perceptrón).  \n",
    "     - Actualizar:\n",
    "       $\\theta\\leftarrow\\theta + f(x^{(i)},y^{(i)}) - f(x^{(i)},\\hat y^{(i)})$.\n",
    "\n",
    "3. **Decodificación**  \n",
    "   - Adaptar Viterbi para usar $\\oplus=\\max,\\otimes=+$ con features de orden superior.\n",
    "\n",
    "4. **Evaluación**  \n",
    "   - Métricas: precision, recall, F1 a nivel de etiqueta completa.  \n",
    "   - Dataset: CoNLL‑2000 Chunking o CoNLL‑2003 NER.\n",
    "\n",
    "5. **Entregables y cronograma**  \n",
    "   - Definición de features y estructura de $\\theta$.  \n",
    "   - Implementación de decodificador (Viterbi modificada).  \n",
    "   - Bucle de entrenamiento online y evaluación inicial.  \n",
    "   - Inclusión de características de orden superior adicionales.  \n",
    "   - Tuning de tasa de aprendizaje y regularización.  \n",
    "   - Comparativa vs perceptrón unigramas/bigramas y reporte de resultados.  \n",
    "\n",
    "\n",
    "### **Proyecto 20: Máquinas de vectores de soporte estructuradas para etiquetado de secuencias**\n",
    "\n",
    "**Objetivo.** Desarrollar un **Structured SVM** para secuencia de etiquetas, usando cutting‑plane o subgradient descent, y compararlo con el perceptrón estructurado.\n",
    "\n",
    "1. **Formulación del Structured SVM**  \n",
    "   - Optimizar:\n",
    "     $$\n",
    "       \\min_{\\theta,\\xi}\\tfrac{1}{2}\\|\\theta\\|^2 + C\\sum_i\\xi_i,\n",
    "     $$\n",
    "     sujeto a\n",
    "     $\\forall i,\\forall y\\neq y_i:\\;\\theta^\\top[f(x_i,y_i)-f(x_i,y)]\\ge \\Delta(y_i,y)-\\xi_i.$\n",
    "\n",
    "2. **Función de pérdida estructural**  \n",
    "   - Usar **Hamming Loss** normalizada:\n",
    "     $\\Delta(y,y') = \\frac{1}{T}\\sum_{t=1}^T\\mathbf{1}_{y_t\\neq y'_t}.$\n",
    "\n",
    "3. **Algoritmo de optimización**  \n",
    "   - **Cutting‑plane** (SVM‑struct de Joachims): iterar generando constraints más violadas $\\arg\\max_y[\\Delta(y_i,y)+\\theta^\\top f(x_i,y)]$.  \n",
    "   - Alternativamente, **SDCA** o SGD con loss-augmented decoding (usar Viterbi modificado).\n",
    "\n",
    "4. **Decodificación y loss-augmented inference**  \n",
    "   - Resolver $\\max_y[\\Delta(y_i,y)+\\theta^\\top f(x_i,y)]$ con Viterbi con penalización de etiquetas incorrectas.\n",
    "\n",
    "5. **Evaluación**  \n",
    "   - Precision, recall, F1 en CoNLL‑2003.  \n",
    "   - Medir tiempo de entrenamiento vs perceptrón estructurado.\n",
    "\n",
    "6. **Entregables y cronograma**  \n",
    "   - Configuración de features y data.  \n",
    "   - Implementar loss-augmented Viterbi.  \n",
    "   - Cutting‑plane loop base.  \n",
    "   - Optimizaciones de performance y caché de constraints.  \n",
    "   - Entrenamiento completo, ajuste de $C$.  \n",
    "   - Reporte comparativo y recomendaciones de uso.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e336a2d3-5e59-4882-9ea8-3bf103292756",
   "metadata": {},
   "source": [
    "#### **Referencias**  \n",
    "\n",
    "1. **Proyecto 1 – Data Loader escalable**  \n",
    "   - Paszke, A. et al. (2019). *PyTorch: An Imperative Style, High-Performance Deep Learning Library*. NeurIPS.  \n",
    "   - Abadi, M. et al. (2016). *TensorFlow: A System for Large-Scale Machine Learning*. OSDI.  \n",
    "\n",
    "2. **Proyecto 2 – Tokenización y BPE**  \n",
    "   - Sennrich, R., Haddow, B. & Birch, A. (2016). *Neural Machine Translation of Rare Words with Subword Units*. ACL.  \n",
    "   - Gage, P. (1994). *A New Algorithm for Data Compression*. C Users Journal.  \n",
    "\n",
    "3. **Proyecto 3 – Suavizado en modelos n‑grama**  \n",
    "   - Chen, S. F. & Goodman, J. (1999). *An Empirical Study of Smoothing Techniques for Language Modeling*. Computer Speech & Language.  \n",
    "   - Kneser, R. & Ney, H. (1995). *Improved Backing‐off for M‐gram Language Modeling*. ICASSP.  \n",
    "\n",
    "4. **Proyecto 4 – Descuento y backoff online**  \n",
    "   - Good, I. J. (1953). *The Population Frequencies of Species and the Estimation of Population Parameters*. Biometrika.  \n",
    "   - Cormode, G. & Muthukrishnan, S. (2005). *An Improved Data Stream Summary: The Count-Min Sketch and Its Applications*. LATIN.  \n",
    "\n",
    "5. **Proyecto 5 – BPTT y RNN con puertas**  \n",
    "   - Rumelhart, D. E., Hinton, G. E. & Williams, R. J. (1986). *Learning Representations by Back-Propagating Errors*. Nature.  \n",
    "   - Hochreiter, S. & Schmidhuber, J. (1997). *Long Short-Term Memory*. Neural Computation.  \n",
    "\n",
    "6. **Proyecto 6 – Evaluación de RNN: perplejidad y OOV**  \n",
    "   - Mikolov, T. et al. (2010). *Recurrent Neural Network Based Language Model*. Interspeech.  \n",
    "   - Jurafsky, D. & Martin, J. H. (2009). *Speech and Language Processing* (Cap. 3: Perplexity). Prentice Hall.  \n",
    "\n",
    "7. **Proyecto 7 – HMM y Viterbi (tutorial)**  \n",
    "   - Rabiner, L. R. (1989). *A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition*. Proceedings of the IEEE.  \n",
    "\n",
    "8. **Proyecto 8 – Perceptrón estructurado y características de orden superior**  \n",
    "   - Collins, M. (2002). *Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms*. EMNLP.  \n",
    "\n",
    "9. **Proyecto 9 – Etiquetado con CNN + RNN a nivel de carácter**  \n",
    "   - Ma, X. & Hovy, E. (2016). *End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF*. ACL.  \n",
    "\n",
    "10. **Proyecto 10 – Embeddings: TF‑IDF, PMI y Word2vec**  \n",
    "    - Mikolov, T. et al. (2013). *Efficient Estimation of Word Representations in Vector Space*. arXiv.  \n",
    "    - Turney, P. D. & Pantel, P. (2010). *From Frequency to Meaning: Vector Space Models of Semantics*. Journal of Artificial Intelligence Research.  \n",
    "\n",
    "11. **Proyecto 11 – Semántica distribucional y distribuida**  \n",
    "    - Deerwester, S. et al. (1990). *Indexing by Latent Semantic Analysis*. Journal of the American Society for Information Science.  \n",
    "    - Brown, P. F. et al. (1992). *Class-based n-gram Models of Natural Language*. Computational Linguistics.  \n",
    "\n",
    "12. **Proyecto 12 – Librerías de IA generativa y pipeline**  \n",
    "    - Wolf, T. et al. (2020). *Transformers: State-of-the-Art Natural Language Processing*. EMNLP.  \n",
    "    - Ramesh, A. et al. (2022). *Hierarchical Text-Conditional Image Generation with CLIP Latents*. arXiv.  \n",
    "\n",
    "13. **Proyecto 13 – Normalización y lematización**  \n",
    "    - Bird, S., Klein, E. & Loper, E. (2009). *Natural Language Processing with Python*. O’Reilly Media.  \n",
    "    - Honnibal, M. & Montani, I. (2020). *spaCy 2: Natural Language Understanding with Bloom Embeddings, Convolutional Neural Networks and Incremental Parsing*. ACL System Demonstrations.  \n",
    "\n",
    "14. **Proyecto 14 – Seq2Seq con atención**  \n",
    "    - Sutskever, I., Vinyals, O. & Le, Q. V. (2014). *Sequence to Sequence Learning with Neural Networks*. NeurIPS.  \n",
    "    - Bahdanau, D., Cho, K. & Bengio, Y. (2015). *Neural Machine Translation by Jointly Learning to Align and Translate*. ICLR.  \n",
    "\n",
    "15. **Proyecto 15 – Etiquetado no supervisado y LDS**  \n",
    "    - Rabiner, L. R. (1989). *A Tutorial on Hidden Markov Models…* (EM para HMM).  \n",
    "    - Ghahramani, Z. & Hinton, G. E. (1996). *Parameter Estimation for Linear Dynamical Systems*. Technical Report.  \n",
    "\n",
    "16. **Proyecto 16 – HMM y Viterbi con ejemplo paso a paso**  \n",
    "    - Rabiner, L. R. (1989). *A Tutorial on Hidden Markov Models…*.  \n",
    "\n",
    "17. **Proyecto 17 – Estimación en HMM con Baum–Welch**  \n",
    "    - Baum, L. E., Petrie, T., Soules, G. & Weiss, N. (1970). *A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains*. The Annals of Mathematical Statistics.  \n",
    "    - Rabiner, L. R. (1989). *A Tutorial on Hidden Markov Models…*.  \n",
    "\n",
    "18. **Proyecto 18 – Inferencia en HMM: Forward–Backward**  \n",
    "    - Rabiner, L. R. (1989). *A Tutorial on Hidden Markov Models…*.  \n",
    "\n",
    "19. **Proyecto 19 – HMM de orden superior y Viterbi extendido**  \n",
    "    - Jurafsky, D. & Martin, J. H. (2009). *Speech and Language Processing* (Secc. 4.4: Modelos n‑grama de orden superior).  \n",
    "\n",
    "20. **Proyecto 20 – Perceptrón estructurado con características complejas**  \n",
    "    - Collins, M. (2002). *Discriminative Training Methods for Hidden Markov Models…*.  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
