{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0a1d661",
   "metadata": {},
   "source": [
    "### Seq2seq\n",
    "\n",
    "Un modelo secuencia a secuencia (Seq2Seq) es una arquitectura de red neuronal diseñada para transformar una secuencia de entrada en una secuencia de salida, y es especialmente útil para tareas donde la longitud de las secuencias de entrada y salida puede diferir. Los modelos Seq2Seq son comúnmente utilizados en aplicaciones como traducción automática, resumen de texto, y generación de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c74e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# S: Símbolo que muestra el inicio de la entrada de decodificación\n",
    "# E: Símbolo que muestra el inicio de la salida de decodificación\n",
    "# P: Símbolo que llenará la secuencia en blanco si el tamaño de los datos del lote actual es menor que los pasos de tiempo\n",
    "\n",
    "def make_batch():\n",
    "    input_batch, output_batch, target_batch = [], [], []\n",
    "\n",
    "    for seq in seq_data:\n",
    "        for i in range(2):\n",
    "            seq[i] = seq[i] + 'P' * (n_step - len(seq[i]))\n",
    "\n",
    "        input_seq = [num_dic[n] for n in seq[0]]\n",
    "        output_seq = [num_dic[n] for n in ('S' + seq[1])]\n",
    "        target = [num_dic[n] for n in (seq[1] + 'E')]\n",
    "\n",
    "        input_batch.append(np.eye(n_class)[input_seq])\n",
    "        output_batch.append(np.eye(n_class)[output_seq])\n",
    "        target_batch.append(target) # no es one-hot\n",
    "\n",
    "    # Convertir listas a numpy arrays antes de convertir a tensores\n",
    "    input_batch = np.array(input_batch)\n",
    "    output_batch = np.array(output_batch)\n",
    "    target_batch = np.array(target_batch)\n",
    "\n",
    "    # crear tensor\n",
    "    return torch.FloatTensor(input_batch), torch.FloatTensor(output_batch), torch.LongTensor(target_batch)\n",
    "\n",
    "# crear lote de prueba\n",
    "def make_testbatch(input_word):\n",
    "    input_batch, output_batch = [], []\n",
    "\n",
    "    input_w = input_word + 'P' * (n_step - len(input_word))\n",
    "    input_seq = [num_dic[n] for n in input_w]\n",
    "    output_seq = [num_dic[n] for n in 'S' + 'P' * n_step]\n",
    "\n",
    "    input_batch = np.eye(n_class)[input_seq]\n",
    "    output_batch = np.eye(n_class)[output_seq]\n",
    "\n",
    "    # Convertir a numpy arrays antes de convertir a tensores\n",
    "    input_batch = np.array(input_batch)\n",
    "    output_batch = np.array(output_batch)\n",
    "\n",
    "    return torch.FloatTensor(input_batch).unsqueeze(0), torch.FloatTensor(output_batch).unsqueeze(0)\n",
    "\n",
    "# Modelo\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, num_layers=2, dropout=0.5)\n",
    "        self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, num_layers=2, dropout=0.5)\n",
    "        self.fc = nn.Linear(n_hidden, n_class)\n",
    "\n",
    "    def forward(self, enc_input, enc_hidden, dec_input):\n",
    "        enc_input = enc_input.transpose(0, 1) # enc_input: [max_len(=n_step, pasos de tiempo), tamaño_lote, n_class]\n",
    "        dec_input = dec_input.transpose(0, 1) # dec_input: [max_len(=n_step, pasos de tiempo), tamaño_lote, n_class]\n",
    "\n",
    "        # enc_states : [num_layers * num_directions(=1), tamaño_lote, n_hidden]\n",
    "        _, enc_states = self.enc_cell(enc_input, enc_hidden)\n",
    "        # outputs : [max_len+1(=6), tamaño_lote, num_directions(=1) * n_hidden(=128)]\n",
    "        outputs, _ = self.dec_cell(dec_input, enc_states)\n",
    "\n",
    "        model = self.fc(outputs) # model : [max_len+1(=6), tamaño_lote, n_class]\n",
    "        return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    n_step = 5\n",
    "    n_hidden = 128\n",
    "\n",
    "    char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz']\n",
    "    num_dic = {n: i for i, n in enumerate(char_arr)}\n",
    "    seq_data = [['man', 'women'], ['black', 'white'], ['king', 'queen'], ['girl', 'boy'], ['up', 'down'], ['high', 'low']]\n",
    "\n",
    "    n_class = len(num_dic)\n",
    "    tamaño_lote = len(seq_data)\n",
    "\n",
    "    model = Seq2Seq()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    input_batch, output_batch, target_batch = make_batch()\n",
    "\n",
    "    for epoch in range(5000):\n",
    "        # crear forma oculta [num_layers * num_directions, tamaño_lote, n_hidden]\n",
    "        hidden = torch.zeros(2, tamaño_lote, n_hidden) # cambiar a 2 capas\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # input_batch : [tamaño_lote, max_len(=n_step, pasos de tiempo), n_class]\n",
    "        # output_batch : [tamaño_lote, max_len+1(=n_step, pasos de tiempo) (por 'S' o 'E'), n_class]\n",
    "        # target_batch : [tamaño_lote, max_len+1(=n_step, pasos de tiempo)], no es one-hot\n",
    "        output = model(input_batch, hidden, output_batch)\n",
    "        # output : [max_len+1, tamaño_lote, n_class]\n",
    "        output = output.transpose(0, 1) # [tamaño_lote, max_len+1(=6), n_class]\n",
    "        loss = 0\n",
    "        for i in range(0, len(target_batch)):\n",
    "            # output[i] : [max_len+1, n_class, target_batch[i] : max_len+1]\n",
    "            loss += criterion(output[i], target_batch[i])\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print('Epoca:', '%04d' % (epoch + 1), 'costo =', '{:.6f}'.format(loss))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Prueba\n",
    "    def translate(word):\n",
    "        input_batch, output_batch = make_testbatch(word)\n",
    "\n",
    "        # crear forma oculta [num_layers * num_directions, tamaño_lote, n_hidden]\n",
    "        hidden = torch.zeros(2, 1, n_hidden) # cambiar a 2 capas\n",
    "        output = model(input_batch, hidden, output_batch)\n",
    "        # output : [max_len+1(=6), tamaño_lote(=1), n_class]\n",
    "\n",
    "        predict = output.data.max(2, keepdim=True)[1] # seleccionar dimensión n_class\n",
    "        decoded = [char_arr[i] for i in predict]\n",
    "        end = decoded.index('E')\n",
    "        translated = ''.join(decoded[:end])\n",
    "\n",
    "        return translated.replace('P', '')\n",
    "\n",
    "    print('test')\n",
    "    print('man ->', translate('man'))\n",
    "    print('mans ->', translate('mans'))\n",
    "    print('king ->', translate('king'))\n",
    "    print('black ->', translate('black'))\n",
    "    print('upp ->', translate('upp'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5ae88d",
   "metadata": {},
   "source": [
    "### Greedy, beam search y selección aleatoria con temperatura\n",
    "\n",
    "Podemos implementar las estrategias de decodificación greedy, beam search y selección aleatoria con temperatura, podemos modificar el código dado agregando funciones específicas para cada método de decodificación.\n",
    "\n",
    "**Estrategia greedy**\n",
    "\n",
    "La estrategia Greedy selecciona el token con la mayor probabilidad en cada paso de decodificación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6b79a4-579b-4ed9-9573-b8bb2cb8caf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# S: Símbolo que muestra el inicio de la entrada de decodificación\n",
    "# E: Símbolo que muestra el inicio de la salida de decodificación\n",
    "# P: Símbolo que llenará la secuencia en blanco si el tamaño de los datos del lote actual es menor que los pasos de tiempo\n",
    "\n",
    "def make_batch():\n",
    "    input_batch, output_batch, target_batch = [], [], []\n",
    "\n",
    "    for seq in seq_data:\n",
    "        for i in range(2):\n",
    "            seq[i] = seq[i] + 'P' * (n_step - len(seq[i]))\n",
    "\n",
    "        input_seq = [num_dic[n] for n in seq[0]]\n",
    "        output_seq = [num_dic[n] for n in ('S' + seq[1])]\n",
    "        target = [num_dic[n] for n in (seq[1] + 'E')]\n",
    "\n",
    "        input_batch.append(np.eye(n_class)[input_seq])\n",
    "        output_batch.append(np.eye(n_class)[output_seq])\n",
    "        target_batch.append(target)  # no es one-hot\n",
    "\n",
    "    # Convertir listas a numpy arrays antes de convertir a tensores\n",
    "    input_batch = np.array(input_batch)\n",
    "    output_batch = np.array(output_batch)\n",
    "    target_batch = np.array(target_batch)\n",
    "\n",
    "    # Crear tensor\n",
    "    return torch.FloatTensor(input_batch), torch.FloatTensor(output_batch), torch.LongTensor(target_batch)\n",
    "\n",
    "# Crear lote de prueba\n",
    "def make_testbatch(input_word):\n",
    "    input_batch, output_batch = [], []\n",
    "\n",
    "    input_w = input_word + 'P' * (n_step - len(input_word))\n",
    "    input_seq = [num_dic[n] for n in input_w]\n",
    "    output_seq = [num_dic[n] for n in 'S' + 'P' * n_step]\n",
    "\n",
    "    input_batch = np.eye(n_class)[input_seq]\n",
    "    output_batch = np.eye(n_class)[output_seq]\n",
    "\n",
    "    # Convertir a numpy arrays antes de convertir a tensores\n",
    "    input_batch = np.array(input_batch)\n",
    "    output_batch = np.array(output_batch)\n",
    "\n",
    "    return torch.FloatTensor(input_batch).unsqueeze(0), torch.FloatTensor(output_batch).unsqueeze(0)\n",
    "\n",
    "# Modelo\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, num_layers=2, dropout=0.5)\n",
    "        self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, num_layers=2, dropout=0.5)\n",
    "        self.fc = nn.Linear(n_hidden, n_class)\n",
    "\n",
    "    def forward(self, enc_input, enc_hidden, dec_input):\n",
    "        enc_input = enc_input.transpose(0, 1)  # [max_len, tamaño_lote, n_class]\n",
    "        dec_input = dec_input.transpose(0, 1)  # [max_len, tamaño_lote, n_class]\n",
    "\n",
    "        # enc_states : [num_layers, tamaño_lote, n_hidden]\n",
    "        _, enc_states = self.enc_cell(enc_input, enc_hidden)\n",
    "        # outputs : [max_len, tamaño_lote, n_hidden]\n",
    "        outputs, _ = self.dec_cell(dec_input, enc_states)\n",
    "\n",
    "        model = self.fc(outputs)  # [max_len, tamaño_lote, n_class]\n",
    "        return model, enc_states\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    n_step = 5\n",
    "    n_hidden = 128\n",
    "\n",
    "    char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz']\n",
    "    num_dic = {n: i for i, n in enumerate(char_arr)}\n",
    "    seq_data = [['man', 'women'], ['black', 'white'], ['king', 'queen'], ['girl', 'boy'], ['up', 'down'], ['high', 'low']]\n",
    "\n",
    "    n_class = len(num_dic)\n",
    "    tamaño_lote = len(seq_data)\n",
    "\n",
    "    model = Seq2Seq()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    input_batch, output_batch, target_batch = make_batch()\n",
    "\n",
    "    for epoch in range(5000):\n",
    "        model.train()\n",
    "        # Crear forma oculta [num_layers, tamaño_lote, n_hidden]\n",
    "        hidden = torch.zeros(2, tamaño_lote, n_hidden)  # 2 capas\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        output, _ = model(input_batch, hidden, output_batch)\n",
    "        # output : [max_len, tamaño_lote, n_class]\n",
    "        output = output.transpose(0, 1)  # [tamaño_lote, max_len, n_class]\n",
    "        loss = 0\n",
    "        for i in range(tamaño_lote):\n",
    "            loss += criterion(output[i], target_batch[i])\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print('Época:', '%04d' % (epoch + 1), 'costo =', '{:.6f}'.format(loss.item()))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Prueba con decodificación greedy\n",
    "    def translate(word):\n",
    "        model.eval()  # Modo evaluación\n",
    "        with torch.no_grad():  # Desactivar cálculo de gradientes\n",
    "            input_w = word + 'P' * (n_step - len(word))\n",
    "            input_seq = [num_dic[n] for n in input_w]\n",
    "            input_batch = np.eye(n_class)[input_seq]\n",
    "            input_batch = torch.FloatTensor(input_batch).unsqueeze(0)  # [1, max_len, n_class]\n",
    "\n",
    "            # Inicializar el estado oculto del codificador\n",
    "            hidden = torch.zeros(2, 1, n_hidden)  # 2 capas\n",
    "\n",
    "            # Codificar la entrada\n",
    "            enc_input = input_batch.transpose(0, 1)  # [max_len, 1, n_class]\n",
    "            _, enc_states = model.enc_cell(enc_input, hidden)\n",
    "\n",
    "            # Inicializar la entrada del decodificador con el símbolo de inicio 'S'\n",
    "            decoder_input = torch.zeros(1, 1, n_class)  # [1, 1, n_class]\n",
    "            decoder_input[0, 0, num_dic['S']] = 1.0\n",
    "\n",
    "            decoded = []\n",
    "            for _ in range(n_step + 1):  # +1 para incluir 'E'\n",
    "                # Decodificar paso a paso\n",
    "                output, enc_states = model.dec_cell(decoder_input, enc_states)\n",
    "                output = model.fc(output.squeeze(0))  # [1, n_class]\n",
    "\n",
    "                # Seleccionar el token con mayor probabilidad\n",
    "                _, topi = output.topk(1)  # [1]\n",
    "                next_token = topi.item()\n",
    "\n",
    "                # Obtener el carácter correspondiente\n",
    "                char = char_arr[next_token]\n",
    "                if char == 'E':\n",
    "                    break\n",
    "                if char != 'P':  # Ignorar los rellenos\n",
    "                    decoded.append(char)\n",
    "\n",
    "                # Preparar la siguiente entrada del decodificador\n",
    "                decoder_input = torch.zeros(1, 1, n_class)\n",
    "                decoder_input[0, 0, next_token] = 1.0\n",
    "\n",
    "            translated = ''.join(decoded)\n",
    "            return translated\n",
    "\n",
    "    print('Prueba de decodificación greedy:')\n",
    "    print('man ->', translate('man'))\n",
    "    print('mans ->', translate('mans'))\n",
    "    print('king ->', translate('king'))\n",
    "    print('black ->', translate('black'))\n",
    "    print('upp ->', translate('upp'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae18962-1071-4a87-9f6b-63d27d47cced",
   "metadata": {},
   "source": [
    "**Estrategia beam search**\n",
    "\n",
    "Beam Search mantiene las mejores k secuencias en cada paso, lo que permite explorar múltiples caminos en la decodificación y seleccionar la secuencia más probable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f34097-8d29-4b32-9301-fca30e49c2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# S: Símbolo que muestra el inicio de la entrada de decodificación\n",
    "# E: Símbolo que muestra el fin de la salida de decodificación\n",
    "# P: Símbolo que llenará la secuencia en blanco si el tamaño de los datos del lote actual es menor que los pasos de tiempo\n",
    "\n",
    "def make_batch():\n",
    "    input_batch, output_batch, target_batch = [], [], []\n",
    "\n",
    "    for seq in seq_data:\n",
    "        for i in range(2):\n",
    "            # Rellenar las secuencias con 'P' para alcanzar n_step\n",
    "            seq[i] = seq[i] + 'P' * (n_step - len(seq[i]))\n",
    "\n",
    "        input_seq = [num_dic[n] for n in seq[0]]\n",
    "        output_seq = [num_dic[n] for n in ('S' + seq[1])]\n",
    "        target = [num_dic[n] for n in (seq[1] + 'E')]\n",
    "\n",
    "        input_batch.append(np.eye(n_class)[input_seq])\n",
    "        output_batch.append(np.eye(n_class)[output_seq])\n",
    "        target_batch.append(target)  # No es one-hot\n",
    "\n",
    "    # Convertir listas a numpy arrays antes de convertir a tensores\n",
    "    input_batch = np.array(input_batch)\n",
    "    output_batch = np.array(output_batch)\n",
    "    target_batch = np.array(target_batch)\n",
    "\n",
    "    # Crear tensor\n",
    "    return torch.FloatTensor(input_batch), torch.FloatTensor(output_batch), torch.LongTensor(target_batch)\n",
    "\n",
    "# Crear lote de prueba\n",
    "def make_testbatch(input_word):\n",
    "    input_batch, output_batch = [], []\n",
    "\n",
    "    input_w = input_word + 'P' * (n_step - len(input_word))\n",
    "    input_seq = [num_dic[n] for n in input_w]\n",
    "    output_seq = [num_dic[n] for n in 'S' + 'P' * n_step]\n",
    "\n",
    "    input_batch = np.eye(n_class)[input_seq]\n",
    "    output_batch = np.eye(n_class)[output_seq]\n",
    "\n",
    "    # Convertir a numpy arrays antes de convertir a tensores\n",
    "    input_batch = np.array(input_batch)\n",
    "    output_batch = np.array(output_batch)\n",
    "\n",
    "    return torch.FloatTensor(input_batch).unsqueeze(0), torch.FloatTensor(output_batch).unsqueeze(0)\n",
    "\n",
    "# Modelo\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, num_layers=2, dropout=0.5)\n",
    "        self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, num_layers=2, dropout=0.5)\n",
    "        self.fc = nn.Linear(n_hidden, n_class)\n",
    "\n",
    "    def forward(self, enc_input, enc_hidden, dec_input):\n",
    "        enc_input = enc_input.transpose(0, 1)  # [max_len, tamaño_lote, n_class]\n",
    "        dec_input = dec_input.transpose(0, 1)  # [max_len, tamaño_lote, n_class]\n",
    "\n",
    "        # enc_states : [num_layers, tamaño_lote, n_hidden]\n",
    "        _, enc_states = self.enc_cell(enc_input, enc_hidden)\n",
    "        # outputs : [max_len, tamaño_lote, n_hidden]\n",
    "        outputs, _ = self.dec_cell(dec_input, enc_states)\n",
    "\n",
    "        model = self.fc(outputs)  # [max_len, tamaño_lote, n_class]\n",
    "        return model, enc_states\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    n_step = 5\n",
    "    n_hidden = 128\n",
    "\n",
    "    char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz']\n",
    "    num_dic = {n: i for i, n in enumerate(char_arr)}\n",
    "    seq_data = [['man', 'women'], ['black', 'white'], ['king', 'queen'], ['girl', 'boy'], ['up', 'down'], ['high', 'low']]\n",
    "\n",
    "    n_class = len(num_dic)\n",
    "    tamaño_lote = len(seq_data)\n",
    "\n",
    "    model = Seq2Seq()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    input_batch, output_batch, target_batch = make_batch()\n",
    "\n",
    "    for epoch in range(5000):\n",
    "        model.train()\n",
    "        # Crear forma oculta [num_layers, tamaño_lote, n_hidden]\n",
    "        hidden = torch.zeros(2, tamaño_lote, n_hidden)  # 2 capas\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        output, _ = model(input_batch, hidden, output_batch)\n",
    "        # output : [max_len, tamaño_lote, n_class]\n",
    "        output = output.transpose(0, 1)  # [tamaño_lote, max_len, n_class]\n",
    "        loss = 0\n",
    "        for i in range(tamaño_lote):\n",
    "            loss += criterion(output[i], target_batch[i])\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print('Época:', '%04d' % (epoch + 1), 'costo =', '{:.6f}'.format(loss.item()))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Prueba con decodificación beam search\n",
    "    def translate_beam_search(word, beam_size=3, max_dec_steps=10):\n",
    "        model.eval()  # Modo evaluación\n",
    "        with torch.no_grad():  # Desactivar cálculo de gradientes\n",
    "            input_w = word + 'P' * (n_step - len(word))\n",
    "            input_seq = [num_dic.get(n, num_dic['P']) for n in input_w]\n",
    "            input_batch = np.eye(n_class)[input_seq]\n",
    "            input_batch = torch.FloatTensor(input_batch).unsqueeze(0)  # [1, max_len, n_class]\n",
    "\n",
    "            # Inicializar el estado oculto del codificador\n",
    "            hidden = torch.zeros(2, 1, n_hidden)  # 2 capas\n",
    "\n",
    "            # Codificar la entrada\n",
    "            enc_input = input_batch.transpose(0, 1)  # [max_len, 1, n_class]\n",
    "            _, enc_states = model.enc_cell(enc_input, hidden)\n",
    "\n",
    "            # Inicializar el haz con la secuencia inicial [S]\n",
    "            beam = [{\n",
    "                'sequence': [num_dic['S']],\n",
    "                'hidden': enc_states.clone(),\n",
    "                'score': 0.0\n",
    "            }]\n",
    "\n",
    "            completed_sequences = []\n",
    "\n",
    "            for _ in range(max_dec_steps):\n",
    "                new_beam = []\n",
    "                for seq in beam:\n",
    "                    last_token = seq['sequence'][-1]\n",
    "                    if last_token == num_dic['E']:\n",
    "                        completed_sequences.append(seq)\n",
    "                        continue\n",
    "\n",
    "                    # Preparar la entrada del decodificador (one-hot)\n",
    "                    decoder_input = torch.zeros(1, 1, n_class)\n",
    "                    decoder_input[0, 0, last_token] = 1.0\n",
    "\n",
    "                    # Obtener la salida del decodificador\n",
    "                    dec_output, dec_hidden = model.dec_cell(decoder_input, seq['hidden'])\n",
    "                    logits = model.fc(dec_output.squeeze(0))  # [1, n_class]\n",
    "                    log_probs = nn.functional.log_softmax(logits, dim=1)  # [1, n_class]\n",
    "\n",
    "                    topk_log_probs, topk_indices = torch.topk(log_probs, beam_size, dim=1)  # [1, beam_size]\n",
    "\n",
    "                    for i in range(beam_size):\n",
    "                        token = topk_indices[0, i].item()\n",
    "                        score = seq['score'] + topk_log_probs[0, i].item()\n",
    "                        new_seq = seq['sequence'] + [token]\n",
    "                        new_hidden = dec_hidden.clone()\n",
    "                        new_beam.append({\n",
    "                            'sequence': new_seq,\n",
    "                            'hidden': new_hidden,\n",
    "                            'score': score\n",
    "                        })\n",
    "\n",
    "                # Ordenar todas las nuevas secuencias por su puntuación y seleccionar las top `beam_size`\n",
    "                new_beam = sorted(new_beam, key=lambda x: x['score'], reverse=True)\n",
    "                beam = new_beam[:beam_size]\n",
    "\n",
    "                # Si todas las secuencias en el haz han finalizado, detener la búsqueda\n",
    "                if len(completed_sequences) >= beam_size:\n",
    "                    break\n",
    "\n",
    "            # Si no hay secuencias completadas, usar las actuales\n",
    "            if not completed_sequences:\n",
    "                completed_sequences = beam\n",
    "\n",
    "            # Ordenar las secuencias completadas por su puntuación y seleccionar la mejor\n",
    "            completed_sequences = sorted(completed_sequences, key=lambda x: x['score'], reverse=True)\n",
    "            best_sequence = completed_sequences[0]['sequence']\n",
    "\n",
    "            # Convertir la secuencia de índices a caracteres\n",
    "            decoded = [char_arr[i] for i in best_sequence]\n",
    "\n",
    "            # Encontrar el índice del símbolo de fin 'E'\n",
    "            if 'E' in decoded:\n",
    "                end = decoded.index('E')\n",
    "                decoded = decoded[:end]\n",
    "\n",
    "            translated = ''.join([char for char in decoded if char not in ['S', 'P']])\n",
    "\n",
    "            return translated\n",
    "\n",
    "    print('\\nPrueba de decodificación beam search:')\n",
    "    print('man ->', translate_beam_search('man'))\n",
    "    print('mans ->', translate_beam_search('mans'))\n",
    "    print('king ->', translate_beam_search('king'))\n",
    "    print('black ->', translate_beam_search('black'))\n",
    "    print('upp ->', translate_beam_search('upp'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e705b50c-920d-4c30-89fe-0a2bcf1f7d36",
   "metadata": {},
   "source": [
    "**Selección aleatoria con temperatura**\n",
    "\n",
    "La selección aleatoria con temperatura ajusta las probabilidades de los tokens antes de muestrear de la distribución, permitiendo más exploración en la generación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeff0976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# S: Símbolo que muestra el inicio de la entrada de decodificación\n",
    "# E: Símbolo que muestra el inicio de la salida de decodificación\n",
    "# P: Símbolo que llenará la secuencia en blanco si el tamaño de los datos del lote actual es menor que los pasos de tiempo\n",
    "\n",
    "def make_batch():\n",
    "    input_batch, output_batch, target_batch = [], [], []\n",
    "\n",
    "    for seq in seq_data:\n",
    "        for i in range(2):\n",
    "            seq[i] = seq[i] + 'P' * (n_step - len(seq[i]))\n",
    "\n",
    "        input_seq = [num_dic[n] for n in seq[0]]\n",
    "        output_seq = [num_dic[n] for n in ('S' + seq[1])]\n",
    "        target = [num_dic[n] for n in (seq[1] + 'E')]\n",
    "\n",
    "        input_batch.append(np.eye(n_class)[input_seq])\n",
    "        output_batch.append(np.eye(n_class)[output_seq])\n",
    "        target_batch.append(target) # no es one-hot\n",
    "\n",
    "    # Convertir listas a numpy arrays antes de convertir a tensores\n",
    "    input_batch = np.array(input_batch)\n",
    "    output_batch = np.array(output_batch)\n",
    "    target_batch = np.array(target_batch)\n",
    "\n",
    "    # crear tensor\n",
    "    return torch.FloatTensor(input_batch), torch.FloatTensor(output_batch), torch.LongTensor(target_batch)\n",
    "\n",
    "# crear lote de prueba\n",
    "def make_testbatch(input_word):\n",
    "    input_batch, output_batch = [], []\n",
    "\n",
    "    input_w = input_word + 'P' * (n_step - len(input_word))\n",
    "    input_seq = [num_dic[n] for n in input_w]\n",
    "    output_seq = [num_dic[n] for n in 'S' + 'P' * n_step]\n",
    "\n",
    "    input_batch = np.eye(n_class)[input_seq]\n",
    "    output_batch = np.eye(n_class)[output_seq]\n",
    "\n",
    "    # Convertir a numpy arrays antes de convertir a tensores\n",
    "    input_batch = np.array(input_batch)\n",
    "    output_batch = np.array(output_batch)\n",
    "\n",
    "    return torch.FloatTensor(input_batch).unsqueeze(0), torch.FloatTensor(output_batch).unsqueeze(0)\n",
    "\n",
    "# Modelo\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, num_layers=2, dropout=0.5)\n",
    "        self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, num_layers=2, dropout=0.5)\n",
    "        self.fc = nn.Linear(n_hidden, n_class)\n",
    "\n",
    "    def forward(self, enc_input, enc_hidden, dec_input):\n",
    "        enc_input = enc_input.transpose(0, 1) # enc_input: [max_len(=n_step, pasos de tiempo), tamaño_lote, n_class]\n",
    "        dec_input = dec_input.transpose(0, 1) # dec_input: [max_len(=n_step, pasos de tiempo), tamaño_lote, n_class]\n",
    "\n",
    "        # enc_states : [num_layers * num_directions(=1), tamaño_lote, n_hidden]\n",
    "        _, enc_states = self.enc_cell(enc_input, enc_hidden)\n",
    "        # outputs : [max_len+1(=6), tamaño_lote, num_directions(=1) * n_hidden(=128)]\n",
    "        outputs, _ = self.dec_cell(dec_input, enc_states)\n",
    "\n",
    "        model = self.fc(outputs) # model : [max_len+1(=6), tamaño_lote, n_class]\n",
    "        return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    n_step = 5\n",
    "    n_hidden = 128\n",
    "\n",
    "    char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz']\n",
    "    num_dic = {n: i for i, n in enumerate(char_arr)}\n",
    "    seq_data = [['man', 'women'], ['black', 'white'], ['king', 'queen'], ['girl', 'boy'], ['up', 'down'], ['high', 'low']]\n",
    "\n",
    "    n_class = len(num_dic)\n",
    "    tamaño_lote = len(seq_data)\n",
    "\n",
    "    model = Seq2Seq()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    input_batch, output_batch, target_batch = make_batch()\n",
    "\n",
    "    for epoch in range(5000):\n",
    "        # crear forma oculta [num_layers * num_directions, tamaño_lote, n_hidden]\n",
    "        hidden = torch.zeros(2, tamaño_lote, n_hidden) # cambiar a 2 capas\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # input_batch : [tamaño_lote, max_len(=n_step, pasos de tiempo), n_class]\n",
    "        # output_batch : [tamaño_lote, max_len+1(=n_step, pasos de tiempo) (por 'S' o 'E'), n_class]\n",
    "        # target_batch : [tamaño_lote, max_len+1(=n_step, pasos de tiempo)], no es one-hot\n",
    "        output = model(input_batch, hidden, output_batch)\n",
    "        # output : [max_len+1, tamaño_lote, n_class]\n",
    "        output = output.transpose(0, 1) # [tamaño_lote, max_len+1(=6), n_class]\n",
    "        loss = 0\n",
    "        for i in range(0, len(target_batch)):\n",
    "            # output[i] : [max_len+1, n_class, target_batch[i] : max_len+1]\n",
    "            loss += criterion(output[i], target_batch[i])\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print('Epoca:', '%04d' % (epoch + 1), 'costo =', '{:.6f}'.format(loss))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Estrategias de Decodificación\n",
    "    def greedy_decode(input_batch, hidden, output_batch):\n",
    "        output = model(input_batch, hidden, output_batch)\n",
    "        predict = output.data.max(2, keepdim=True)[1] # seleccionar dimensión n_class\n",
    "        return predict\n",
    "\n",
    "    def beam_search_decode(input_batch, hidden, output_batch, beam_width=3):\n",
    "        output = model(input_batch, hidden, output_batch)\n",
    "        output = F.softmax(output, dim=2)  # Aplicar softmax para obtener probabilidades\n",
    "        sequences = [[list(), 1.0]]  # (sequence, score)\n",
    "\n",
    "        for row in output.squeeze(1):  # Asegurar que se procesa la salida correcta\n",
    "            all_candidates = list()\n",
    "            for seq, score in sequences:\n",
    "                for j in range(len(row)):\n",
    "                    candidate = [seq + [j], score * row[j].item()]  # Multiplicar las probabilidades, no sumar logaritmos negativos\n",
    "                    all_candidates.append(candidate)\n",
    "            # Ordenar todos los candidatos por puntuación de manera descendente (probabilidad más alta primero)\n",
    "            ordered = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)\n",
    "            sequences = ordered[:beam_width]\n",
    "\n",
    "        best_sequence = sequences[0][0]\n",
    "        return torch.tensor(best_sequence, dtype=torch.long).view(1, -1, 1)\n",
    "\n",
    "\n",
    "    def random_sample_with_temperature(output, temperature=1.0):\n",
    "        output = output.div(temperature).exp()\n",
    "        probs = F.softmax(output, dim=-1)\n",
    "        return torch.multinomial(probs.view(-1, probs.size(-1)), 1).view(output.size(0), output.size(1), -1)\n",
    "\n",
    "    def decode_with_temperature(input_batch, hidden, output_batch, temperature=1.0):\n",
    "        output = model(input_batch, hidden, output_batch)\n",
    "        sampled_output = random_sample_with_temperature(output, temperature)\n",
    "        return sampled_output\n",
    "\n",
    "    def translate(word, strategy='greedy', beam_width=3, temperature=1.0):\n",
    "        input_batch, output_batch = make_testbatch(word)\n",
    "        hidden = torch.zeros(2, 1, n_hidden) # cambiar a 2 capas\n",
    "    \n",
    "        if strategy == 'greedy':\n",
    "            predict = greedy_decode(input_batch, hidden, output_batch)\n",
    "        elif strategy == 'beam_search':\n",
    "            predict = beam_search_decode(input_batch, hidden, output_batch, beam_width)\n",
    "        elif strategy == 'temperature':\n",
    "            predict = decode_with_temperature(input_batch, hidden, output_batch, temperature)\n",
    "    \n",
    "        decoded = [char_arr[i] for i in predict.squeeze()]\n",
    "        if 'E' in decoded:\n",
    "            end = decoded.index('E')\n",
    "            translated = ''.join(decoded[:end])\n",
    "        else:\n",
    "            translated = ''.join(decoded)\n",
    "\n",
    "        return translated.replace('P', '')\n",
    "\n",
    "    words = ['man', 'mans', 'king', 'black', 'upp']\n",
    "    \n",
    "    print('Comparativa de decodificación:\\n')\n",
    "    \n",
    "    for word in words:\n",
    "        print(f'Palabra: {word}')\n",
    "        print(f'Greedy: {translate(word, strategy=\"greedy\")}')\n",
    "        print(f'Beam Search: {translate(word, strategy=\"beam_search\", beam_width=3)}')\n",
    "        print(f'Temperatura (T=1.0): {translate(word, strategy=\"temperature\", temperature=1.0)}')\n",
    "        print('---------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9d8228",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "1 .Implementa un mecanismo de atención en el modelo Seq2Seq para mejorar la traducción.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Añade una capa de atención al modelo Seq2Seq.\n",
    "- Modifica el método forward para incorporar la atención.\n",
    "- Entrena el modelo y compara el rendimiento con el modelo original.\n",
    "\n",
    "Pistas:\n",
    "\n",
    "- Utiliza la clase nn.Linear para calcular los pesos de atención.\n",
    "- Multiplica los pesos de atención con los estados ocultos del codificador para obtener el contexto.\n",
    "- Concatena el contexto con la entrada del decodificador en cada paso de tiempo.\n",
    "\n",
    "2 . Compara el rendimiento de las estrategias de decodificación Greedy, Beam Search y Temperatura en diferentes configuraciones de entrenamiento.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Entrena el modelo con diferentes tamaños de conjunto de datos y configuraciones de hiperparámetros (por ejemplo, diferentes tamaños de hidden_size y num_layers).\n",
    "- Aplica las tres estrategias de decodificación a cada modelo entrenado.\n",
    "- Evalúa y compara la calidad de las traducciones utilizando métricas como la precisión y el BLEU score.\n",
    "\n",
    "Pistas:\n",
    "\n",
    "- Usa conjuntos de datos grandes y pequeños para ver cómo cambia el rendimiento.\n",
    "- Experimenta con diferentes beam_width y temperatures.\n",
    "\n",
    "3 . Implementa una variante de Beam Search que penalice secuencias más largas para evitar repeticiones innecesarias.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Modifica la función beam_search_decode para incluir una penalización de longitud.\n",
    "- Ajusta el cálculo de las puntuaciones de las secuencias para penalizar las secuencias más largas.\n",
    "- Compara los resultados con el Beam Search estándar.\n",
    "\n",
    "Pistas:\n",
    "\n",
    "- Multiplica la puntuación de cada secuencia por una función de penalización basada en su longitud.\n",
    "- Puedes usar una función de penalización lineal o exponencial.\n",
    "\n",
    "4 . Implementa una estrategia de decodificación por temperatura que ajuste dinámicamente la temperatura en cada paso de tiempo.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Modifica la función decode_with_temperature para ajustar la temperatura en cada paso de tiempo.\n",
    "- Implementa una función que disminuya la temperatura a medida que avanza la decodificación, incentivando exploración al principio y explotación al final.\n",
    "- Evalúa el impacto de la temperatura dinámica en la calidad de las traducciones.\n",
    "\n",
    "Pistas:\n",
    "\n",
    "- Usa una función de decremento lineal o exponencial para la temperatura.\n",
    "- Compara los resultados con una temperatura fija.\n",
    "\n",
    "5 . Analiza la complejidad computacional y el rendimiento de las diferentes estrategias de decodificación.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Mide el tiempo de ejecución de las estrategias Greedy, Beam Search y Temperatura para diferentes tamaños de vocabulario y longitudes de secuencia.\n",
    "- Analiza cómo cambia la complejidad computacional con respecto a beam_width y temperature.\n",
    "- Discute los trade-offs entre la calidad de la traducción y el tiempo de ejecución.\n",
    "\n",
    "Pistas:\n",
    "\n",
    "- Usa la biblioteca time para medir el tiempo de ejecución.\n",
    "- Realiza pruebas con diferentes configuraciones y grafica los resultados.\n",
    "\n",
    "\n",
    "6 .Mejora la generalización del modelo incorporando técnicas de regularización y dropout.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Añade capas de dropout adicionales al modelo Seq2Seq.\n",
    "- Implementa técnicas de regularización como L2.\n",
    "- Entrena el modelo con estas técnicas y compara el rendimiento con el modelo original.\n",
    "\n",
    "Pistas:\n",
    "\n",
    "- Usa nn.Dropout en las capas RNN y totalmente conectadas.\n",
    "- Ajusta los hiperparámetros de regularización y dropout para encontrar la configuración óptima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efe5f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
