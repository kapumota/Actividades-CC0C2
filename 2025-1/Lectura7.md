### Sobre los transformers

Desde su aparición, la arquitectura Transformer ha supuesto un cambio radical en el campo del procesamiento de lenguaje natural, al permitir que los modelos, en lugar de procesar las secuencias de forma estrictamente secuencial, como en las redes recurrentes, puedan atender a todas 
las posiciones de la entrada en paralelo. En esencia, un Transformer recibe una secuencia de embeddings de palabras a las que se les suma una codificación posicional que incorpora información sobre la posición de cada elemento dentro de la secuencia. 

De este modo, aunque el modelo carezca de bucles internos, no pierde la noción del orden original de los tokens. A partir de estas representaciones enriquecidas, el Transformer construye su famoso mecanismo de atención, que ha demostrado ser capaz de capturar relaciones contextuales de largo alcance entre palabras de un modo más efectivo y escalable que las arquitecturas previas.

La **codificación posicional** es esencial para dotar al Transformer de sensibilidad al orden. Una estrategia clásica consiste en generar, para cada posición $i$ y cada dimensión $k$ del espacio de embeddings, valores senoidales y cosenoidales de la forma

$$
\text{PE}(i,2k) = \sin\bigl(i / 10000^{2k/d}\bigr),\quad
\text{PE}(i,2k+1) = \cos\bigl(i / 10000^{2k/d}\bigr),
$$

donde $d$ es la dimensión total del embedding. Alternativamente, se pueden aprender vectores de posición entrenándolos junto al resto de parámetros. En ambos casos, las codificaciones posicionales se suman a los embeddings de palabras, haciendo que cada vector posea información tanto semántica como posicional.

El **mecanismo de atención** se basa en tres matrices,queries ($Q$), keys ($K$) y values ($V$), que se obtienen mediante proyecciones lineales de las representaciones de entrada. Con ellas, se calcula una matriz de atención que determina la importancia de cada posición en la secuencia para cada otra posición. De forma intuitiva, podemos aplicar este mecanismo a los embeddings de las palabras para "preguntar" a cada token qué otras partes del texto son más relevantes para su representación. Gracias a esto, se capturan de manera dinámica y contextual las relaciones entre palabras, superando limitaciones de ventanas de contexto reducidas.

En concreto, la **atención de producto punto escalado** implementa una serie de multiplicaciones matriciales que combinan las queries y las keys, dividiendo luego por la raíz cuadrada de la dimensión de las keys (un factor de escala que estabiliza el entrenamiento), y aplicando después una función softmax para normalizar las puntuaciones. Opcionalmente, se utiliza una máscara que impide atender posiciones no deseadas,por ejemplo, en el decodificador, para que en el entrenamiento autoregresivo no se tenga acceso a futuros tokens. El resultado de la distribución de atención se multiplica por la matriz de values, generando la salida final que incorpora tanto la información local del token como el contexto global de la secuencia.

Más allá de la simple predicción del siguiente token en tareas de modelado de lenguaje, la arquitectura Transformer permite **integrar capas de atención** dentro de soluciones de clasificación de texto, manteniendo el contexto completo de la frase. Al añadir una cabecera de clasificación sobre el token especial \[CLS], por ejemplo, es posible aprovechar la riqueza contextual construida por la red para asignar etiquetas de sentimiento, categorías de intención o cualquier otra etiqueta supervisada, sin perder la coherencia informacional de la secuencia original.

La variante **encoder–decoder** de los Transformers está especialmente diseñada para tareas de traducción automática. En ella, el codificador lee la secuencia de origen completa, aplicando varias capas de auto-atención y redes feed-forward, mientras que el decodificador genera la secuencia destino paso a paso, atendiéndose tanto a sus propios tokens previos (auto-atención enmascarada) como a la representación del codificador (*cross-attention*). Gracias a que ambos procesos se pueden paralelizar por posición, la eficiencia es muy superior a la de los modelos recurrentes: todas las posiciones de entrada y salida se procesan simultáneamente.

El **decodificador**, piedra angular de los modelos de generación de texto, opera de forma autoregresiva, prediciendo el siguiente token condicionado a todos los anteriores generados. Modelos como GPT, LLaMA o Granite son variantes puramente de decodificador; en ellos, durante el entrenamiento se emplea *teacher forcing* (se introducen siempre los tokens correctos del paso anterior), mientras que en la fase de inferencia el sistema enmascara las posiciones futuras, forzando la generación de forma estrictamente autoregresiva. En la práctica, las implementaciones en PyTorch de estos modelos se basan en el módulo `nn.Transformer`, con pequeñas adaptaciones para la gestión de máscaras y la arquitectura del head de predicción.

En el caso de **BERT**, se aprovecha únicamente la parte del codificador, potenciando la atención bidireccional. Durante el preentrenamiento, BERT lleva a cabo dos tareas complementarias: por un lado, en el **Masked Language Modeling** se enmascaran tokens aleatorios en la entrada y el modelo debe predecirlos correctamente; por otro lado, en la **Next Sentence Prediction** el sistema recibe un par de oraciones y debe decidir si la segunda prosigue lógicamente a la primera. De este modo, la arquitectura de BERT, al incorporar atención sobre toda la secuencia desde ambos lados, aprende representaciones altamente contextualizadas. Posteriormente, mediante fine-tuning, puede ajustarse a tareas tan diversas como resumen automático, respuesta a preguntas o análisis de sentimiento, bastando añadir un pequeño head específico y actualizar los pesos con los datos de la tarea.

La **preparación de datos** para estos modelos exige inicializar un tokenizador adecuado, por ejemplo invocando `get_tokenizer('bert-base-uncased')` en PyTorch o Hugging Face, y definir un conjunto de símbolos especiales,\[CLS], \[SEP], \[MASK], \[PAD],asignándoles los índices correspondientes en el vocabulario. El tokenizador fragmenta el texto en subunidades (WordPiece o SentencePiece), genera las secuencias de IDs y, en paralelo, construye máscaras de atención que indiquen al modelo qué posiciones son relevantes y cuáles deben ignorarse.

En la fase de decodificación de un transformer encoder–decoder, el método recibe dos entradas fundamentales: la **secuencia destino** y la **"memoria"** procedente del codificador. La secuencia destino se incrusta mediante el mismo embedding que la fuente y se le aplica la codificación posicional. A continuación, pasa por un bloque de atención enmascarada, luego por la atención cruzada que relaciona cada posición destino con todas las posiciones de la secuencia codificada y, finalmente, por capas feed-forward. De este modo, el Transformer gestiona de forma integrada los procesos de codificación y decodificación, manteniendo la coherencia contextual en todo momento.

Durante la generación de texto para tareas de traducción, la **atención cruzada** resulta especialmente crítica, ya que calcula las puntuaciones de atención entre cada posición del destino y todas las posiciones de la fuente, permitiendo al modelo consultar directamente la información relevante de la entrada en cada paso de la salida. Al combinar múltiples cabeceras de atención en paralelo, el sistema es capaz de atender simultáneamente a distintos tipos de relaciones: por un lado, patrones léxicos cortos; por otro, correspondencias semánticas de largo alcance.

El entrenamiento de estos grandes modelos Transformer suele ser **intensivo en recursos**; sin embargo, existen varias técnicas para optimizar la eficiencia y reducir costes sin sacrificar la calidad. Una de las más sencillas de implementar es la **acumulación de gradientes**: en lugar de procesar batches muy grandes que excedan la memoria de la GPU, se procesan varios mini-batches de forma secuencial, acumulando los gradientes sin actualizar los pesos hasta haber procesado un número prefijado de ellos. De esta forma, se simula el efecto de un batch grande, reduciendo el uso de memoria y, al mismo tiempo, estabilizando las actualizaciones al computar gradientes menos ruidosos.

Otra estrategia muy extendida es el **entrenamiento en precisión mixta** (mixed-precision training). Consiste en realizar gran parte de las operaciones matriciales en media precisión (FP16), lo que reduce drásticamente el consumo de memoria y acelera los cálculos en hardware moderno optimizado para estos formatos. Para evitar problemas de desbordamiento o pérdida de información en gradientes muy pequeños, se utiliza un ***loss scaler*** que ajusta dinámicamente la escala de la pérdida al retropropagar y luego reescala los gradientes antes de actualizar los pesos. Frameworks como PyTorch ofrecen utilidades integradas (`torch.cuda.amp`) que automatizan gran parte del proceso.

Cuando hablamos de modelos que no caben en la memoria de un solo dispositivo, o cuando buscamos repartir la carga de forma óptima, entran en juego las técnicas de **entrenamiento distribuido**. En **data parallelism**, cada GPU recibe un fragmento distinto del batch de datos, calcula la pérdida y los gradientes, y luego sincroniza estos gradientes entre dispositivos antes de actualizar los parámetros (por ejemplo, usando `DistributedDataParallel` en PyTorch). Por su parte, el **model parallelism** reparte las capas del propio modelo entre distintas GPUs, de modo que cada dispositivo almacena solo una parte de la arquitectura. En escenarios de extremo tamaño, se combinan ambas estrategias, o se implementan variantes más sofisticadas como el *tensor parallelism* y el *pipeline parallelism*, que fragmentan las operaciones a nivel de matriz y por etapas de la red respectivamente.

Finalmente, la elección de un **optimizador eficiente** puede marcar la diferencia entre un entrenamiento que converge en pocas épocas y uno que no logra alcanzar el rendimiento deseado. Dos de los optimizadores más populares en el entrenamiento de Transformers son **AdamW** y **LAMB**. AdamW incorpora un término de decaimiento de peso desacoplado de la tasa de aprendizaje, mejorando la generalización y evitando la sobreacumulación de regularización L2. LAMB, por su parte, fue diseñado para escalar a grandes *batch sizes*: calcula un ratio de confianza capa por capa ("layer-wise adaptive moments"), ajustando las actualizaciones de forma que cada parte de la red reciba el tamaño de paso óptimo, lo cual acelera la convergencia en conjuntos de datos extensos sin comprometer la estabilidad.

Conjugar estos avances ,codificación posicional, atención multi-cabecera, arquitecturas encoder–decoder, preentrenamiento de BERT, generación autoregresiva, junto con técnicas como acumulación de gradientes, mixed-precision, entrenamiento distribuido y optimizadores especializados, permite entrenar Transformers monumentales en plazos razonables y con recursos moderados, abriendo la puerta a aplicaciones más innovadoras y escalables en el ámbito del procesamiento de lenguaje natural y más allá.

#### Técnicas actuales

Además de las codificaciones posicionales clásicas y las variantes de atención mencionadas, en la última generación de Transformers han surgido diversas técnicas para enriquecer la representación del orden, ampliar el contexto efectivo y dotar a los modelos de mecanismos de memoria más flexibles y escalables.

**Learned positional encoding.** En lugar de emplear las fórmulas senoidales y cosenoidales fijas, se pueden parametrizar las codificaciones de posición como vectores entrenables. Cada posición $i$ dispone de un vector $P_i \in \mathbb{R}^d$ que se inicializa aleatoriamente y se ajusta durante el entrenamiento. Esta aproximación —utilizada, por ejemplo, en algunos variantes de BERT,  permite al modelo aprender directamente qué tipo de información posicional resulta más útil para la tarea concreta, sin imponer una forma matemática preconcebida.

**Relative position encoding** A diferencia de las codificaciones absolutas, que asignan a cada posición de un token un identificador fijo e independiente del contexto, las codificaciones relativas buscan capturar directamente la distancia entre tokens. En lugar de codificar *dónde* se encuentra un token en una secuencia, se enfoca en *cuán lejos* está un token de otro.

En la práctica, esta técnica se implementa modificando la forma en que se calcula la atención en los modelos Transformer. Se introduce un término adicional, conocido como "sesgo relativo", que depende únicamente de la diferencia entre las posiciones de los tokens que interactúan. Este sesgo puede ser un valor escalar o un vector específico para cada posible distancia relativa.

La ventaja de esta aproximación es que otorga a la atención una propiedad de invarianza ante traslaciones. Es decir, si una secuencia se desplaza a otra parte del texto, el modelo puede seguir razonando sobre las relaciones internas entre tokens de forma coherente. Esto mejora significativamente la capacidad del modelo para generalizar en tareas donde lo relevante no es la posición absoluta, sino la cercanía o lejanía entre elementos del contexto.

**Rotary position embedding (RoPE).** RoPE propone incorporar la información posicional directamente rotando los subespacios de la query y la key. Cada par de dimensiones $(2k,2k+1)$ de $Q$ y $K$ se rota mediante un ángulo proporcional a la posición:

$$
\begin{pmatrix}
Q_{i,2k} \\
Q_{i,2k+1}
\end{pmatrix}
\mapsto
\begin{pmatrix}
\cos(\theta_{i,k}) & -\sin(\theta_{i,k}) \\
\sin(\theta_{i,k}) &  \cos(\theta_{i,k})
\end{pmatrix}
\begin{pmatrix}
Q_{i,2k} \\
Q_{i,2k+1}
\end{pmatrix},
$$

y de igual forma para $K$. Aquí $\theta_{i,k}=i/10000^{2k/d}$. Gracias a esta rotación, las puntuaciones de atención resultan naturalmente dependientes de la diferencia de posición, sin necesidad de agregar sesgos explícitos, y facilita la extrapolación a longitudes mayores que las vistas en entrenamiento.

**Extensión de contexto (Longer Context).** La capacidad de procesar secuencias muy largas es clave en aplicaciones como documentos enteros o streams de diálogo. Para superar el límite cuadrático de la atención clásica, se han desarrollado enfoques de atención dispersa (sparse), ventanas deslizantes o combinaciones jerárquicas que reducen la complejidad a casi lineal. Modelos como Longformer, BigBird o Reformer emplean patrones de atención local y global, por ejemplo, cada token atiende exhaustivamente a sus vecinos más cercanos y de forma puntual a posiciones estratégicas, lo que permite manejar decenas de miles de tokens manteniendo la calidad de la representación.

**Memoria de contexto (Context Memory).** Más allá de estirar ventanas, se puede dotar al Transformer de una capa de memoria explícita. A medida que se procesa la secuencia, ciertas representaciones intermedias (por ejemplo, salidas de capas previas) se almacenan en un buffer o "memoria" externa. En procesamientos posteriores, el modelo puede leer esta memoria mediante un segundo mecanismo de atención, extendiendo su alcance efectivo a posiciones arbitrariamente distantes sin requerir re-procesar toda la secuencia origen.

**Compressive Transformer.** Este diseño va un paso más allá: introduce dos niveles de memoria. Una **memoria a corto plazo** almacena representaciones recientes y se vacía continuamente, mientras una **memoria comprimida** retiene resúmenes de representaciones más antiguas. Cada vez que el buffer a corto plazo alcanza cierta longitud, sus activaciones se condensan (por ejemplo, mediante un autoencoder) y se mueven a la memoria comprimida, de la que luego pueden recuperarse mediante atención. Así se consigue un historial extenso sin explotar la memoria GPU de forma incontrolada.

**Memoria externa no diferenciable.** Algunos trabajos incorporan elementos de memoria que no se entrenan de forma end-to-end por retropropagación, sino que funcionan como una base de datos de vectores a la que se consulta mediante búsquedas aproximadas (ANN). El modelo emite una query, se realiza un nearest neighbor search sobre el repositorio de memorias y se trae de vuelta la representación más similar, que luego se concatena o combina con la generación. Este enfoque, inspirado en estructuras como los Memory Networks clásicos puede aprovechar índices externos altamente optimizados.

**kNN-LM (k-Nearest Neighbors Language Model).** Es una aplicación práctica de memoria externa: una vez entrenado un Transformer, se almacena cada representación de token en un datastore. Durante la generación o predicción, el modelo obtiene la distribución de probabilidad estándar y la mezcla con una distribución estimada a partir de los *k* vecinos más cercanos en el datastore, ponderados según su similitud al query actual. Este refuerzo mejora la fidelidad del modelo a datos de entrenamiento extensos y facilita la adaptación sin reentrenamiento completo.

**SPALM (Sparse and Local Memory Augmented LM).** SPALM combina atención dispersa con módulos de memoria localizados: cada token sólo consulta un subconjunto de memorias cercanas y unas pocas solicitudes globales. Al mantener la mayoría de acceso al entorno inmediato y dirigir sólo una fracción de la atención a la memoria global, logra un punto de equilibrio entre capacidad de contexto extendido y eficiencia computacional.

**Memorizing Transformer.** Este modelo integra un banco de memoria jerárquico y diferenciable: durante el entrenamiento, las representaciones "memorables" se extraen de la activación de ciertas neuronas y se agrupan en un diccionario de claves y valores. En inferencia, una capa de atención cruza las queries con las claves de memoria, recuperando valores asociadas que enriquecen la generación. De este modo, el modelo "memoriza" fragmentos de entrenamiento que luego puede reintroducir de forma selectiva, mejorando la consistencia y la reproducción de hechos aprendidos sin necesidad de expandir infinitamente las capas internas.

En conjunto, estas innovaciones persiguen dos metas: **capturar con precisión la estructura posicional** en secuencias de longitud variable y **extender el alcance contextual** más allá de los límites físicos de la GPU. Al combinar codificaciones avanzadas (relativas, rotatorias, aprendibles) con mecanismos de memoria jerarquizados (búferes, compresión, memoria externa) y atajos de recuperación de información (kNN-LM, Memorizing Transformer, SPALM), los Transformers del futuro podrán procesar diálogos extensos, documentos enteros y flujos de datos continuos sin sacrificar eficiencia ni capacidad de generalización.

