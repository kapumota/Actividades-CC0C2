{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Direct Preference Optimization (DPO) usando Hugging Face**\n",
    "\n",
    "Los modelos de lenguaje a gran escala (LLMs) han revolucionado el campo del procesamiento de lenguaje natural (NLP) al lograr un rendimiento excepcional en diversas tareas. Sin embargo, resulta desafiante alinear estos modelos con las preferencias humanas. Por ello, surge el método Direct Preference Optimization (DPO), que optimiza directamente los modelos basados en LLM según las preferencias de los usuarios, mejorando su alineación con las expectativas humanas. \n",
    "\n",
    "En este cuaderno práctico, utilizaremos la librería de refuerzo de transformers (`trl`) de Hugging Face para implementar DPO y ajustar finamente los LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Configuración**\n",
    "\n",
    "#### Instalación de las librerías requeridas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install trl      # para el entrenamiento de optimización\n",
    "!pip install peft     # para crear la arquitectura LoRA\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Importación de las librerías necesarias**\n",
    "\n",
    "*Se recomienda importar todas las librerías requeridas en un mismo lugar (aquí):*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "import requests\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer,TrainingArguments, GPT2Tokenizer, set_seed, GenerationConfig\n",
    "from trl import DPOConfig, DPOTrainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Creación y configuración el modelo y el tokenizador**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga el modelo GPT-2\n",
    "modelo = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Carga un modelo de referencia\n",
    "model_ref = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Carga el tokenizador GPT-2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Establece el token de relleno al token de fin de secuencia\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Establece el lado de padding a \"right\" para evitar problemas de desbordamiento con FP16\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Deshabilita el uso de la caché durante la pasada hacia adelante del modelo\n",
    "modelo.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí puedes revisar la arquitectura del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Configuración de modelo cuantizado (Opcional)**\n",
    "\n",
    "Si deseas un entrenamiento más eficiente en memoria y dispones de un entorno con GPU, puedes descargar el cuaderno completo, descomentar los bloques de código siguientes para crear un modelo cuantizado y continuar el entrenamiento en GPU. \n",
    "\n",
    "Esto se debe a que necesitarás GPUs para el paquete `bitsandbytes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U bitsandbytes # este paquete es requerido para la cuantización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Nota:***  *Puedes ejecutar el paquete instalado reiniciando el Kernel.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''## Modelo cuantizado -- solo disponible en GPU\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# Configura los parámetros de cuantización\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    # Carga el modelo en formato cuantizado de 4 bits\n",
    "    load_in_4bit=True,\n",
    "    # Habilita la doble cuantización para una mayor precisión\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    # Usa cuantización no uniforme de 4 bits (nf4)\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    # Usa bfloat16 como tipo de dato de cómputo durante la cuantización\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Carga el modelo GPT-2 con la configuración de cuantización especificada\n",
    "modelo = AutoModelForCausalLM.from_pretrained(\"gpt2\", quantization_config=quantization_config)\n",
    "\n",
    "# Carga un modelo de referencia con la misma configuración de cuantización\n",
    "model_ref = AutoModelForCausalLM.from_pretrained(\"gpt2\", quantization_config=quantization_config)\n",
    "\n",
    "# Carga el tokenizador GPT-2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Establece el token de relleno al token de fin de secuencia\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Establece el lado de padding a \"right\" para evitar problemas de desbordamiento con FP16\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Deshabilita el uso de la caché durante la pasada hacia adelante del modelo\n",
    "modelo.config.use_cache = False\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Preprocesamiento del conjunto de datos**\n",
    "\n",
    "El conjunto de datos `ultrafeedback_binarized` en Hugging Face es una colección de prompts y respuestas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga el conjunto de datos desde la ubicación especificada\n",
    "ds = load_dataset(\"BarraHome/ultrafeedback_binarized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este conjunto de datos incluye seis particiones (splits). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada registro posee varias características, entre las cuales debes seleccionar tres: `\"chosen\"`, `\"rejected\"` y `\"prompt\"`. Esto significa que para cada prompt se proporciona una respuesta preferida y una rechazada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"train_prefs\"][0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes revisar un registro de muestra, donde verás las tres características principales: el prompt, la respuesta rechazada y la respuesta elegida.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"train_prefs\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, coloca el conjunto de datos en el formato que acepta el entrenador DPO:\n",
    "\n",
    "| Chosen | Rejected | Prompt |\n",
    "| --- | --- | --- |\n",
    " | Developing a daily habit of drawing can be challenging <br>but with consistent practice, and a few tips. | One way to develop a habit of drawing daily is <br>to allocate a specific time interval for drawing. | How can I develop a habit of drawing daily?|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puedes reducir el volumen de datos (debido a limitaciones de recursos) seleccionando el primer 5 % de ejemplos de cada partición del conjunto de datos\n",
    "for key in ds:\n",
    "    #cnt = round(ds[key].__len__()*0.05)\n",
    "    cnt = 50\n",
    "    ds[key] = ds[key].select(range(cnt))\n",
    "\n",
    "# Define una función para procesar los datos\n",
    "def process(row):\n",
    "    # elimina columnas no deseadas\n",
    "    del row[\"prompt_id\"]\n",
    "    del row[\"messages\"]\n",
    "    del row[\"score_chosen\"]\n",
    "    del row[\"score_rejected\"]\n",
    "    # obtiene el texto real de la respuesta\n",
    "    row[\"chosen\"] = row[\"chosen\"][-1][\"content\"]\n",
    "    row[\"rejected\"] = row[\"rejected\"][-1][\"content\"]\n",
    "\n",
    "    return row\n",
    "\n",
    "# Aplica la función de procesamiento al conjunto de datos\n",
    "ds = ds.map(\n",
    "    process,\n",
    "    num_proc=multiprocessing.cpu_count(),\n",
    "    load_from_cache_file=False,\n",
    ")\n",
    "\n",
    "# Separa el conjunto de datos en entrenamiento y evaluación\n",
    "train_dataset = ds['train_prefs']\n",
    "eval_dataset = ds['test_prefs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a revisar un registro de datos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, definimos la configuración de LoRA para un afinamiento eficiente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración PEFT (Parameter-Efficient Fine-Tuning)\n",
    "peft_config = LoraConfig(\n",
    "    # Rango de las matrices de adaptación rango bajo\n",
    "    r=4,\n",
    "    # Módulos objetivo a los que se aplicará la adaptación de rango bajo\n",
    "    target_modules=['c_proj','c_attn'],\n",
    "    # Tipo de tarea para la adaptación de baja-rank\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # Factor de escala para las matrices de adaptación de rango bajo\n",
    "    lora_alpha=8,\n",
    "    # Probabilidad de dropout para las matrices de adaptación de rango bajo\n",
    "    lora_dropout=0.1,\n",
    "    # Modo de sesgo para la adaptación de rango bajo\n",
    "    bias=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Configuración DPO**\n",
    "\n",
    "Primero, define los argumentos de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración DPO\n",
    "training_args = DPOConfig(\n",
    "    # Parámetro beta para la función de pérdida DPO.\n",
    "    # beta es el parámetro de temperatura para la pérdida DPO, típicamente en el rango 0.1–0.5.\n",
    "    beta=0.1,\n",
    "    # Directorio de salida para el entrenamiento\n",
    "    output_dir=\"dpo\",\n",
    "    # Número de épocas de entrenamiento\n",
    "    num_train_epochs=5,\n",
    "    # Tamaño de lote por dispositivo durante el entrenamiento\n",
    "    per_device_train_batch_size=1,\n",
    "    # Tamaño de lote por dispositivo durante la evaluación\n",
    "    per_device_eval_batch_size=1,\n",
    "    # Si se eliminan columnas no utilizadas del conjunto de datos\n",
    "    remove_unused_columns=False,\n",
    "    # Número de pasos entre registros de progreso\n",
    "    logging_steps=10,\n",
    "    # Número de pasos de acumulación de gradiente\n",
    "    gradient_accumulation_steps=1,\n",
    "    # Tasa de aprendizaje para la optimización\n",
    "    learning_rate=1e-4,\n",
    "    # Estrategia de evaluación (por ejemplo, tras cada época)\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    # Número de pasos de calentamiento para el planificador de tasa de aprendizaje\n",
    "    warmup_steps=2,\n",
    "    # Si se usa precisión de 16 bits (float16)\n",
    "    fp16=False,\n",
    "    # Número de pasos entre guardado de puntos de control\n",
    "    save_steps=500,\n",
    "    # Límite máximo de puntos de control a conservar\n",
    "    #save_total_limit=2,\n",
    "    # Backend de reporte (usar 'none' para desactivar; también puedes reportar en wandb o tensorboard)\n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Entrenamiento DPO**\n",
    "\n",
    "El siguiente paso es crear el entrenador usando la clase `DPOTrainer`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegura que el token de padding sea el token EOS\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Crea un entrenador DPO\n",
    "# Este entrenador manejará el fine-tuning del modelo usando la técnica DPO\n",
    "trainer = DPOTrainer(\n",
    "    # Modelo a afinar\n",
    "    modelo,\n",
    "    # Modelo de referencia (no se usa en este caso porque LoRA ya está aplicado)\n",
    "    ref_model=None,\n",
    "    # Configuración de entrenamiento DPO\n",
    "    args=training_args,\n",
    "    # Parámetro beta para la pérdida DPO\n",
    "    beta=0.1,\n",
    "    # Conjunto de datos de entrenamiento\n",
    "    train_dataset=train_dataset,\n",
    "    # Conjunto de datos de evaluación\n",
    "    eval_dataset=eval_dataset,\n",
    "    # Tokenizer del modelo\n",
    "    tokenizer=tokenizer,\n",
    "    # Configuración PEFT\n",
    "    peft_config=peft_config,\n",
    "    # Longitud máxima del prompt\n",
    "    max_prompt_length=512,\n",
    "    # Longitud máxima de la secuencia\n",
    "    max_length=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten en cuenta que, al usar LoRA en el modelo base, es eficiente dejar `ref_model=None`, de modo que `DPOTrainer` descargará el adaptador para la inferencia de referencia.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Entrenamiento del modelo**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ten en cuenta que entrenar el modelo en CPU puede llevar mucho tiempo y puede provocar que el kernel se bloquee por problemas de memoria. Si esto sucede, puedes omitir el entrenamiento cargando el modelo preentrenado que se proporciona en la siguiente sección y continuar desde ahí.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicia el proceso de entrenamiento\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a recuperar y graficar la pérdida de entrenamiento frente a la pérdida de evaluación:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recupera el historial de registros y guárdalo en un DataFrame\n",
    "log = pd.DataFrame(trainer.state.log_history)\n",
    "log_t = log[log['loss'].notna()]\n",
    "log_e = log[log['eval_loss'].notna()]\n",
    "\n",
    "# Grafica las pérdidas de entrenamiento y evaluación\n",
    "plt.plot(log_t[\"epoch\"], log_t[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(log_e[\"epoch\"], log_e[\"eval_loss\"], label=\"eval_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga el modelo DPO entrenado en el último punto de control\n",
    "dpo_model = AutoModelForCausalLM.from_pretrained('./dpo/checkpoint-250')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Carga del modelo entrenado**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si encuentras dificultades al ejecutar la celda de entrenamiento por limitaciones de recursos, puedes descargar el modelo ya afinado:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define la URL y el nombre de archivo\n",
    "url = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/YIDeT3qihEpWChdXN_RmTg/DPO-tar.gz'\n",
    "filename = './DPO.tar'\n",
    "\n",
    "# Descarga el archivo\n",
    "response = requests.get(url)\n",
    "\n",
    "# Guarda el archivo localmente\n",
    "with open(filename, 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Extrae el archivo tar\n",
    "if tarfile.is_tarfile(filename):\n",
    "    with tarfile.open(filename, 'r') as tar:\n",
    "        tar.extractall()\n",
    "        print(\"Archivos extraídos:\", tar.getnames())\n",
    "else:\n",
    "    print(\"El archivo descargado no es un tar válido.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, lo cargamos en el modelo para continuar con la inferencia:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga el modelo DPO entrenado que acabas de descargar\n",
    "dpo_model = AutoModelForCausalLM.from_pretrained('./DPO')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Generación**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga el tokenizer de GPT-2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fija una semilla para reproducibilidad\n",
    "set_seed(42)\n",
    "\n",
    "# Define la configuración de generación para el modelo DPO\n",
    "# Estos parámetros controlan cómo se genera el texto\n",
    "generation_config = GenerationConfig(\n",
    "    # Usa muestreo para producir texto más diverso\n",
    "    do_sample=True,\n",
    "    # Parámetro top-k para el muestreo\n",
    "    top_k=1,\n",
    "    # Temperatura para controlar la aleatoriedad\n",
    "    temperature=0.1,\n",
    "    # Número máximo de tokens nuevos a generar\n",
    "    max_new_tokens=25,\n",
    "    # Usa el token de fin de secuencia como token de relleno\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Define el texto de entrada para la generación\n",
    "PROMPT = \"Is a higher octane gasoline better for your car?\"\n",
    "# Codifica el prompt con el tokenizer\n",
    "inputs = tokenizer(PROMPT, return_tensors='pt')\n",
    "\n",
    "# Genera texto con el modelo DPO\n",
    "outputs = dpo_model.generate(**inputs, generation_config=generation_config)\n",
    "# Decodifica y muestra la respuesta\n",
    "print(\"Respuesta DPO:\\t\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "# Carga el modelo GPT-2 preentrenado\n",
    "gpt2_model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "# Genera texto con GPT-2\n",
    "outputs = gpt2_model.generate(**inputs, generation_config=generation_config)\n",
    "# Decodifica y muestra la respuesta\n",
    "print(\"\\nRespuesta GPT-2:\\t\", tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Aunque el modelo se entrenó con pocos datos durante solo 5 épocas, se observa que la respuesta generada por el modelo ajustado con DPO es más concisa y directa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicios** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 1: Preprocesar el conjunto de datos `argilla/ultrafeedback-binarized-preferences-cleaned`\n",
    "\n",
    "Este conjunto de datos contiene **prompts** generados por usuarios junto con sus respuestas categorizadas como **chosen** o **rejected**, lo que lo hace ideal para entrenar modelos que aprendan las preferencias de los usuarios.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cargar el conjunto de datos desde `argilla/ultrafeedback-binarized-preferences-cleaned`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fijar la variable `cnt` en 50 y seleccionar los primeros 50 ejemplos para reducir el volumen de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Completa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Crea una función `process` que reciba una fila del dataset y elimine columnas no deseadas. \n",
    "\n",
    "Columnas a eliminar: `source`, `chosen-rating`, `chosen-model`, `rejected-rating`, `rejected-model`. Luego, aplicar esta función con `map` sobre los datos de entrenamiento.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Completa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Divide el conjunto en entrenamiento y evaluación\n",
    "\n",
    "Calcula el tamaño de la partición de entrenamiento como el 80 % del total y el resto (20 %) será para evaluación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Ejercicio 2: Inferencia de prompts y comparación con GPT-2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inicializa el Tokenizer de GPT-2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Completa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Crea un objeto `generation_config` con los parámetros de generación\n",
    "\n",
    "* `do_sample=True`      (habilita muestreo para mayor diversidad)\n",
    "* `top_k=1`             (considera solo el token más probable en cada paso)\n",
    "* `temperature=0.1`     (controla la aleatoriedad de la generación)\n",
    "* `max_new_tokens=25`   (número máximo de tokens nuevos)\n",
    "* `pad_token_id=tokenizer.eos_token_id`  (token de relleno)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Completa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Crea una función llamada `generate_dpo_response` que reciba un prompt como entrada y genere una respuesta usando el modelo DPO (`dpo_model`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Completa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Crea otra función llamada `generate_gpt2_response` que reciba un prompt como entrada y genere una respuesta usando el modelo GPT-2 (`gpt2_model`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Completa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Llama a ambas funciones con un prompt y compara las respuestas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Completa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "prev_pub_hash": "21ff78b44c97c4a9c4f0d7965c976d0f5a40a6c0de593f10a90787e44e4637df"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
