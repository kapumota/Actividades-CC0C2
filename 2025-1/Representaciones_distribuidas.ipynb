{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Representaciones distribuidas**\n",
    "\n",
    "Las representaciones distribuidas han transformado el campo del procesamiento de lenguaje natural (NLP) y el aprendizaje automático. A diferencia de enfoques basados en codificaciones locales, como one-hot encoding, que generan vectores dispersos de alta dimensión con valores mayoritariamente en cero, las representaciones distribuidas aprenden vectores densos de dimensión fija en un espacio continuo. \n",
    "\n",
    "Gracias a la hipótesis distributiva, según la cual palabras con contextos similares comparten significados próximos, estos métodos capturan relaciones semánticas complejas, optimizan el uso de memoria y facilitan el aprendizaje de significados de palabras nuevas o raras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Características principales**\n",
    "\n",
    "Veamos algunos características de estos métodos:\n",
    "\n",
    "- A diferencia de las representaciones locales, las distribuidas pueden capturar relaciones complejas entre palabras, como sinónimos, antónimos o términos que suelen aparecer en contextos similares.\n",
    "\n",
    "- Al representar palabras como vectores de tamaño fijo en un espacio continuo, se reduce la dimensionalidad del problema comparado con métodos de representación más simples pero de alta dimensionalidad, como el one-hot encoding.\n",
    "\n",
    "- Estos modelos pueden generalizar para entender palabras nuevas o raras a partir de sus componentes (por ejemplo, entender palabras compuestas a partir de los significados de sus partes).\n",
    "\n",
    "**Ejemplos y modelos**\n",
    "\n",
    "- Word2Vec: Probablemente el ejemplo más conocido de representaciones distribuidas. Word2Vec utiliza redes neuronales para aprender representaciones vectoriales de palabras a partir de grandes conjuntos de datos de texto. Ofrece dos arquitecturas principales: CBOW (Continuous Bag of Words) y Skip-gram, cada una diseñada para aprender representaciones que predigan palabras en función de sus contextos o viceversa.\n",
    "\n",
    "- GloVe (Global Vectors for Word Representation): Un modelo que aprende representaciones de palabras a partir de las estadísticas co-ocurrenciales de palabras en un corpus. La idea es que las relaciones semánticas entre palabras pueden ser capturadas observando qué tan frecuentemente aparecen juntas en un gran corpus.\n",
    "\n",
    "- Embeddings contextuales: Modelos más recientes como ELMo, BERT y GPT ofrecen una evolución de las representaciones distribuidas, generando vectores de palabras que varían según el contexto en el que aparecen, lo que permite capturar usos y significados múltiples de una misma palabra dependiendo de la oración en la que se encuentre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Embeddings de palabras**\n",
    "\n",
    "Los embeddings de palabras son representaciones vectoriales densas y de baja dimensión de palabras, diseñadas para capturar el significado semántico, sintáctico y relaciones entre ellas. A diferencia de las representaciones de texto más antiguas, como el one-hot encoding, que son dispersas (la mayoría de los valores son cero) y de alta dimensión, los embeddings de palabras se representan en un espacio vectorial continuo donde palabras con significados similares están ubicadas cercanamente en el espacio vectorial.\n",
    "\n",
    "**Características de los embeddings de palabras**\n",
    "\n",
    "- Cada palabra se representa como un vector denso, lo que significa que cada dimensión tiene un valor real, a diferencia de los vectores dispersos de otras técnicas de representación.\n",
    "\n",
    "- Los embeddings generalmente tienen un tamaño de dimensión fijo y relativamente pequeño (por ejemplo, 100, 200, 300 dimensiones) independientemente del tamaño del vocabulario.\n",
    "\n",
    "- Estos vectores intentan capturar el contexto y el significado de una palabra, no solo su presencia o ausencia. Palabras que se usan en contextos similares tendrán embeddings similares.\n",
    "\n",
    "- Pueden ayudar a los modelos de aprendizaje automático a generalizar mejor a palabras no vistas durante el entrenamiento, dado que las palabras con significados similares se mapean a puntos cercanos en el espacio vectorial.\n",
    "\n",
    "\n",
    "En 2013, un trabajo fundamental de Mikolov [Efficient Estimationof Word Representations in Vector Space](https://arxiv.org/abs/1301.3781) demostraron que su modelo de representación de palabras basado en una red neuronal conocido como `Word2vec`, basado en la `similitud distributiva`, puede capturar relaciones de analogía de palabras como: \n",
    "\n",
    "$$King - Man + Woman \\approx Queen$$\n",
    "\n",
    "Conceptualmente, Word2vec toma un gran corpus de texto como entrada y \"aprende\" a representar las palabras en un espacio vectorial común en función de los contextos en los que aparecen en el corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Embeddings de palabras pre-entrenadas**\n",
    "\n",
    "Podemos embeddings de Word2vec previamente entrenadas y buscar las palabras más similares (clasificadas por similitud de coseno) a una palabra determinada. \n",
    "\n",
    "Tomemos un ejemplo de un modelo word2vec previamente entrenado y cómo podemos usarlo para buscar la mayoría de las palabras similares. Usaremos los embeddings de vectores de Google News. https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
    "\n",
    "Se pueden encontrar algunos otros modelos de embeddings de palabras previamente entrenados y detalles sobre los medios para acceder a ellos a través de gensim en: https://github.com/RaRe-Technologies/gensim-data\n",
    "\n",
    "El código que sigue cubre los pasos clave. Aquí encontramos las palabras que semánticamente son más similares a la palabra \"beautiful\"; la última línea devuelve el vector de embeddings de la palabra \" beautiful \":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dos cosas a tener en cuenta al utilizar modelos previamente entrenados:\n",
    "\n",
    "* Los tokens/palabras siempre están en minúsculas. Si una palabra no está en el vocabulario, el modelo genera una excepción.\n",
    "* Por lo tanto, siempre es una buena idea encapsular esas declaraciones en bloques `try/except`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Representaciones distribuidas avanzadas**\n",
    "\n",
    "En la evolución de las representaciones distribuidas, surgieron cuatro líneas de investigación fundamentales que atendieron necesidades concretas: mejorar la generalización, reducir la carga computacional, incorporar conocimiento morfológico y compartir representaciones entre tareas. A continuación describimos cada enfoque, su motivación y la fórmula matemática que explica su funcionamiento.\n",
    "\n",
    "#### **Neural network language model (NNLM)**\n",
    "\n",
    "**Contexto**  \n",
    "Antes de 2003, las representaciones basadas en conteo (coocurrencias, SVD, PMI) carecían de capacidad para modelar interacciones no lineales. Bengio et al. propusieron entrenar de forma conjunta los embeddings y una red feed‑forward que, para una ventana de contexto fija $(w_{t-n+1},\\dots,w_{t-1})$, estimara la probabilidad de la siguiente palabra $w_t$.\n",
    "\n",
    "**Ecuación clave**  \n",
    "\n",
    "$$\n",
    "P(w_t \\mid w_{t-n+1},\\dots,w_{t-1})\n",
    "=\n",
    "\\frac{\\exp\\bigl(u_{w_t}^{\\!\\top}\\,f\\bigl(W\\,[\\,e_{w_{t-n+1}},\\dots,e_{w_{t-1}}]\\bigr)\\bigr)}\n",
    "     {\\sum_{w\\in V}\\exp\\bigl(u_{w}^{\\!\\top}\\,f\\bigl(W\\,[\\,e_{w_{t-n+1}},\\dots,e_{w_{t-1}}]\\bigr)\\bigr)},\n",
    "$$  \n",
    "\n",
    "donde:  \n",
    "- $e_{w}\\in\\mathbb{R}^d$ es el embedding de cada palabra de contexto.  \n",
    "- $W$ y $u_w$ son parámetros entrenables de la red.  \n",
    "- $f$ es una función oculta no lineal (por ejemplo, tanh).  \n",
    "- El denominador softmax normaliza sobre todo el vocabulario $V$.  \n",
    "\n",
    "Este planteamiento mejoró la predicción de secuencias y sentó las bases de los embeddings neuronales.\n",
    "\n",
    "\n",
    "#### **C&W Model (Collobert & Weston)**\n",
    "\n",
    "**Contexto**  \n",
    "La necesidad de compartir embeddings entre múltiples tareas (clasificación, análisis sintáctico, etc.) y de evitar el alto coste de la normalización softmax condujo a Collobert y Weston (2008) a diseñar un esquema de aprendizaje por ranking con redes convolucionales.\n",
    "\n",
    "**Ecuación clave**  \n",
    "\n",
    "Definiendo una ventana real $x^+$ y variantes corruptas $x^-$, el objetivo hinge es:  \n",
    "$$\n",
    "\\mathcal{L}_{\\mathrm{C\\&W}}\n",
    "=\n",
    "\\sum_{x^+}\\sum_{x^-}\n",
    "\\max\\bigl(\\,0,\\,1 - s(x^+) + s(x^-)\\bigr),\n",
    "$$  \n",
    "donde $s(x)$ es una puntuación escalar obtenida aplicando convoluciones y capas densas sobre la secuencia de embeddings concatenados. Esta pérdida garantiza que los ejemplos reales obtengan siempre al menos un margen de 1 frente a los negativos, acelerando el entrenamiento y facilitando el multitasking.\n",
    "\n",
    "\n",
    "#### **CBOW y Skip‑gram**\n",
    "\n",
    "**Contexto**  \n",
    "Word2Vec introdujo dos tareas inversas que, combinadas con técnicas de muestreo, hicieron posible entrenar embeddings de alta calidad en vocabularios enormes.\n",
    "\n",
    "- **CBOW** promedia el contexto para predecir la palabra central.\n",
    "- **Skip‑gram** invierte la tarea, usando la palabra central para predecir su contexto.\n",
    "\n",
    "**Ecuación clave (Negative Sampling)**  \n",
    "En lugar de softmax completo, usan:\n",
    "$$\n",
    "\\mathcal{L}_{\\text{NS}}\n",
    "=\n",
    "-\\log\\sigma\\bigl(u_{w_t}^{\\!\\top}v_C\\bigr)\n",
    "\\;-\\;\n",
    "\\sum_{i=1}^k\n",
    "\\mathbb{E}_{w_i\\sim P_n}\n",
    "\\bigl[\\log\\sigma\\bigl(-u_{w_i}^{\\!\\top}v_C\\bigr)\\bigr],\n",
    "$$\n",
    "donde  \n",
    "- $v_C$ es el embedding del contexto (o de la palabra central).  \n",
    "- $\\sigma(x)=1/(1+e^{-x})$.  \n",
    "- $P_n$ es una distribución de ruido habitual (frecuencias a la 3/4).  \n",
    "- $k$ es el número de muestras negativas.  \n",
    "\n",
    "Gracias a esta reformulación, la complejidad por ejemplo pasó de $O(|V|)$ a $O(k)$.\n",
    "\n",
    "\n",
    "#### **Método híbrido caracter‑palabra**\n",
    "\n",
    "**Contexto**  \n",
    "Las lenguas ricas en morfología y el problema de las palabras OOV inspiraron la combinación de embeddings de caracteres y de palabra. Mediante CNNs o LSTMs se aprenden vectores de caracteres o n‑gramas, que luego se fusionan con el embedding de la palabra completa.\n",
    "\n",
    "**Ecuación clave**  \n",
    "Para un término $w$ compuesto por $L$ n‑gramas de caracteres $c_i$, la representación final es:  \n",
    "$$\n",
    "v_w\n",
    "=\n",
    "W_1\\,e_w\n",
    "\\;+\\,\n",
    "W_2\\,\n",
    "\\Bigl(\\tfrac{1}{L}\\sum_{i=1}^L c_i\\Bigr),\n",
    "$$  \n",
    "con $W_1$ y $W_2$ como matrices de combinación entrenables. FastText ejemplifica esta idea descomponiendo cada palabra en todos sus n‑gramas y sumando sus embeddings, logrando robustez frente a variaciones ortográficas y morfológicas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Representaciones distribuidas de frases**\n",
    "\n",
    "Las representaciones distribuidas de frases constituyen un paso intermedio entre los embeddings de palabras y los de documentos completos. Si bien las palabras aportan unidades atómicas de significado, las frases introducen composiciones semánticas y relaciones sintácticas complejas que los modelos deben capturar para tareas como análisis de sentimientos, clasificación de oraciones y recuperación de información. \n",
    "\n",
    "En este contexto, las dos familias clásicas de métodos son los basados en Bag‑of‑Words (BoW), que extienden de forma lineal la lógica de los embeddings de palabras, y los basados en autoencoders, que emplean arquitecturas neuronales para codificar y decodificar la secuencia completa.\n",
    "\n",
    "#### **Basadas en bolsas de palabras**\n",
    "\n",
    "Los métodos Bag‑of‑Words para frases aprovechan la disponibilidad de embeddings de palabras preentrenados y definen la representación de una frase como agregación de los vectores de sus tokens. La simplicidad y eficiencia de estos esquemas han garantizado su vigencia en entornos con grandes volúmenes de datos.\n",
    "\n",
    "**Promedio simple de embeddings**\n",
    "\n",
    "Sea una frase $S = (w_1, w_2, \\,\\dots\\,, w_n)$ con embeddings de palabras $v_i = e_{w_i} \\in \\mathbb{R}^d$. El promedio simple define:\n",
    "\n",
    "$$\n",
    "  v_{S} = \\frac{1}{n} \\sum_{i=1}^n v_i.\n",
    "$$\n",
    "\n",
    "Este método asume que cada token contribuye de igual manera a la semántica global. A pesar de ignorar el orden y las interacciones no lineales, su bajo coste computacional $O(n\\times d)$ lo convierte en una opción popular, especialmente para tareas de similitud semántica donde la magnitud de la frase importa más que su estructura interna.\n",
    "\n",
    "**Promedios ponderados y smooth inverse frequency (SIF)**\n",
    "\n",
    "Para reflejar la relevancia diferencial de cada palabra, se introducen pesos basados en estadísticas de corpus. El esquema SIF (Arora et al., 2017) propone:\n",
    "\n",
    "$$\n",
    "  \\alpha_i = \\frac{a}{a + p(w_i)},\n",
    "  \\quad\n",
    "  v_{S} = \\frac{1}{\\sum_{i=1}^n \\alpha_i} \\sum_{i=1}^n \\alpha_i v_i,\n",
    "$$\n",
    "\n",
    "donde $p(w_i)$ es la frecuencia de $w_i$ en un gran corpus y $a$ un parámetro de suavizado (por ejemplo, $10^{-3}$). Después se suprime la primera componente principal $u$ de todo el conjunto de vectores de frases:\n",
    "\n",
    "$$\n",
    "  v_{S}' = v_{S} - u\\bigl(u^\\top v_{S}\\bigr).\n",
    "$$\n",
    "\n",
    "La eliminación de la componente dominante reduce el efecto de palabras muy frecuentes y mejora la discriminación entre frases semánticamente similares.\n",
    "\n",
    "**Representación TF‑IDF ponderada**\n",
    "\n",
    "Como alternativa, se utiliza el peso TF‑IDF de cada término:\n",
    "\n",
    "$$\n",
    "  \\alpha_i = \\mathrm{tf}(w_i, S) \\times \\log\\frac{N}{\\mathrm{df}(w_i)},\n",
    "  \\quad\n",
    "  v_{S} = \\frac{1}{\\sum_{i=1}^n \\alpha_i} \\sum_{i=1}^n \\alpha_i v_i,\n",
    "$$\n",
    "\n",
    "con $N$ el número total de documentos y $\\mathrm{df}(w_i)$ la cantidad de documentos que contienen $w_i$. Este esquema potencia términos distintivos y atenúa los muy comunes.\n",
    "\n",
    "**Max‑pooling, min‑pooling y concatenaciones**\n",
    "\n",
    "Otros esquemas no lineales incluyen:\n",
    "\n",
    "- **Max‑pooling:** $v_S(j) = \\max_{1\\le i\\le n} v_i(j)$ para cada dimensión $j$.\n",
    "- **Min‑pooling:** $v_S(j) = \\min_{1\\le i\\le n} v_i(j)$.\n",
    "- **Concatenación:** combinación de $\\langle\\mathrm{mean},\\mathrm{max},\\mathrm{min}\\rangle$ generando un vector de dimensión $3d$.\n",
    "\n",
    "Aunque capturan características de forma más rica, incrementan la dimensionalidad y el coste de almacenamiento.\n",
    "\n",
    "**Interpretación empírica y aplicaciones**\n",
    "\n",
    "Los métodos BoW han mostrado:\n",
    "\n",
    "- **Buen desempeño en similitud semántica:** la correlación coseno entre $v_S$ y otro vector de referencia se alinea con la percepción humana de similitud.\n",
    "- **Eficiencia en recuperación de información:** indexación de vectores en aproximaciones de \"nearest neighbor\" para búsqueda rápida.\n",
    "\n",
    "No obstante, son sensibles a la presencia de stopwords y carecen de modelado de negaciones y dependencia sintáctica.\n",
    "\n",
    "**Limitaciones sintácticas y semánticas**\n",
    "\n",
    "| Limitación                  | Descripción                                                                         |\n",
    "|-----------------------------|-------------------------------------------------------------------------------------|\n",
    "| Invarianza al orden         | No distingue \"rojo coche rápido\" de \"rápido coche rojo\"                           |\n",
    "| Escasa interacción léxica   | No captura modismos ni expresiones idiomáticas                                      |\n",
    "| Vulnerable a palabras vacías| Stopwords frecuentes pueden dominar la media sin corrección de ponderación adecuada |\n",
    "| Sin dependencia gramatical  | No modela relaciones sujeto-verbo u objetos directos e indirectos                    |\n",
    "\n",
    "En entornos donde la sintaxis sea clave (por ejemplo, detección de sarcasmo o análisis profundo de relaciones), se prefieren métodos más complejos.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Basadas en autoencoder**\n",
    "\n",
    "Los autoencoders resuelven las debilidades de BoW al aprender codificaciones latentes que capturan orden, sintaxis e interacciones no lineales.\n",
    "\n",
    "**Arquitectura de encoder-decoder**\n",
    "\n",
    "Un autoencoder de secuencia implementa:\n",
    "\n",
    "- **Encoder:** procesa $(x_1,\\dots,x_n)$ (embeddings de palabra) mediante una red recurrente:\n",
    "  $$\n",
    "    h_t = f(h_{t-1}, x_t),\n",
    "    \\quad\n",
    "    z = h_n,\n",
    "  $$\n",
    "  donde $f$ puede ser LSTM o GRU.\n",
    "\n",
    "- **Decoder:** genera la secuencia reconstruida:\n",
    "  $$\n",
    "    s_t = g(s_{t-1}, z),\n",
    "    \\quad\n",
    "    \\hat x_t = \\mathrm{softmax}(W_o s_t),\n",
    "  $$\n",
    "\n",
    "La pérdida de reconstrucción se define como entropía cruzada acumulada:\n",
    "\n",
    "$$\n",
    "  \\mathcal{L}_{AE} = -\\sum_{t=1}^n \\log P( x_t \\mid s_{t-1}, z ).\n",
    "$$\n",
    "\n",
    "Esta configuración favorece latentes $z$ que capturan la información esencial de la frase.\n",
    "\n",
    "**Autoencoder denoising**\n",
    "\n",
    "Al introducir un operador de ruido $T(\\cdot)$ que borra o altera tokens, el encoder recibe $\\tilde x = T(x)$ y aprende a reconstruir $x$. La pérdida es:\n",
    "\n",
    "$$\n",
    "  \\mathcal{L}_{DAE} = -\\sum_{t=1}^n \\log P( x_t \\mid f(\\tilde x) ).\n",
    "$$\n",
    "\n",
    "Este mecanismo impulsa robustez frente a errores menores y parsinómias de tokenización.\n",
    "\n",
    "**Autoenconder variacional (VAE)**\n",
    "\n",
    "El VAE aplica un enfoque probabilístico:\n",
    "\n",
    "- **Encoder estocástico:** $q_\\phi(z\\mid x) = \\mathcal{N}(\\mu(x), \\mathrm{diag}(\\sigma^2(x)))$.\n",
    "- **Decoder condicional:** $p_\\theta(x\\mid z)$.\n",
    "\n",
    "La función objetivo maximiza la evidencia libre inferior:\n",
    "\n",
    "$$\n",
    "  \\mathcal{L}_{VAE}\n",
    "  = \\mathbb{E}_{q_\\phi} [ \\log p_\\theta(x\\mid z) ]\n",
    "    - \\mathrm{D}_{KL}\\bigl(q_\\phi(z\\mid x) \\| p(z)\\bigr).\n",
    "$$\n",
    "\n",
    "Este esquema favorece latentes continuos y permite generar frases nuevas mediante muestreo en el espacio $z$.\n",
    "\n",
    "**Vectores Skip‑Thought**\n",
    "\n",
    "Skip‑Thought extiende la reconstrucción a las oraciones adyacentes:\n",
    "\n",
    "1. Encoder produce $h_i = f(s_i)$.\n",
    "2. Dos decoders generan $s_{i-1}$ y $s_{i+1}$.\n",
    "3. La pérdida combina ambas reconstrucciones:\n",
    "   $$\n",
    "     \\mathcal{L}_{SkipThought}\n",
    "     = -\\sum_t [ \\log P(s_{i-1}[t]\\mid h_i) + \\log P(s_{i+1}[t]\\mid h_i) ].\n",
    "   $$\n",
    "\n",
    "Los vectores $h_i$ codifican información semántica inter-oracional.\n",
    "\n",
    "**Modelos basados en atención (Transformers)**\n",
    "\n",
    "Aunque no estrictamente autoencoders, los Transformers ofrecen representaciones de frases mediante atención múltiple:\n",
    "\n",
    "$$\n",
    "  \\mathrm{Attention}(Q,K,V) = \\mathrm{softmax}\\bigl(\\tfrac{QK^\\top}{\\sqrt{d_k}}\\bigr)V.\n",
    "$$\n",
    "\n",
    "En BERT, el token especial `[CLS]` produce un vector $h_{CLS}$ que sirve de representación de frase. La autoatención captura dependencias de largo alcance y orden sintáctico.\n",
    "\n",
    "**Comparativa y aplicaciones prácticas**\n",
    "\n",
    "\n",
    "| Método            | Captura orden | Robustez OOV | Eficiencia  | Uso típico                            |\n",
    "|-------------------|---------------|--------------|-------------|---------------------------------------|\n",
    "| BoW promedio      | No            | Baja         | Alta        | Búsqueda, similitud rápida            |\n",
    "| BoW ponderado     | No            | Media        | Alta        | Recuperación con stopword handling    |\n",
    "| Autoencoder Seq2Seq | Sí         | Media        | Media       | Generación de texto, compresión de secuencias |\n",
    "| VAE               | Sí            | Alta         | Baja        | Generación creativa de frases         |\n",
    "| Skip‑Thought      | Sí            | Alta         | Baja        | Arquitectura preentrenada general     |\n",
    "| Transformers      | Sí            | Alta         | Media/Baja  | Tareas de clasificación de frases     |\n",
    "\n",
    "Las representaciones de frases alimentan distintas aplicaciones:\n",
    "\n",
    "- **Clasificación de sentimientos:** embeddings de frase como características de entrada a clasificadores lineales o no lineales.\n",
    "- **Respuesta a preguntas (QA):** mapeo de pregunta y contextos a vectores comparables.\n",
    "- **Detección de plagio y similitud:** medición de coseno entre vectores de frases largas.\n",
    "- **Agrupamiento de opiniones:** clustering de reseñas por similitud semántica.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Representaciones distribuidas de oraciones**\n",
    "\n",
    "Las oraciones, a diferencia de palabras o frases cortas, contienen estructuras sintácticas más complejas y relaciones semánticas de largo alcance, lo que impone nuevos retos a los modelos de representación. Mientras que los embeddings de palabras capturan información léxica y los vectores de frases medianas se centran en composiciones sencillas, las representaciones de oraciones deben integrar:\n",
    "\n",
    "1. **Información de orden y estructura:** la secuencia de tokens y la dependencia gramatical.\n",
    "2. **Semántica de alto nivel:** intenciones, relaciones argumentales y tono.\n",
    "3. **Contexto extraoracional:** en algunos casos, información del párrafo o documento de origen.\n",
    "\n",
    "Explora dos grandes enfoques: las representaciones generales de oraciones, obtenidas mediante preentrenamiento en tareas de inferencia textual, y las representaciones dirigidas a tareas específicas, donde la arquitectura se adapta y optimiza para el objetivo final.\n",
    "\n",
    "##### Representación general de oraciones\n",
    "\n",
    "Los modelos de representación general buscan generar vectores que funcionen bien en una amplia variedad de tareas sin modificación adicional. Dos ejemplos paradigmáticos son **InferSent** y el **Universal Sentence Encoder (USE)**.\n",
    "\n",
    "**InferSent: Bi‑LSTM con max‑pooling**\n",
    "\n",
    "**Arquitectura:** InferSent emplea una red bidireccional LSTM (Bi‑LSTM) para procesar la oración:\n",
    "\n",
    "- Para cada token $w_t$, el encoder genera una representación hacia adelante\n",
    "  $$\n",
    "    \\overrightarrow{h}_t = \\mathrm{LSTM}_f(\\overrightarrow{h}_{t-1}, e_{w_t}),\n",
    "  $$\n",
    "  y otra hacia atrás\n",
    "  $$\n",
    "    \\overleftarrow{h}_t = \\mathrm{LSTM}_b(\\overleftarrow{h}_{t+1}, e_{w_t}).\n",
    "  $$\n",
    "\n",
    "- El vector de la oración se construye mediante *max‑pooling* dimensional:\n",
    "  $$\n",
    "    v_{\\mathrm{sent}}(j) = \\max_{1 \\le t \\le n} \\bigl[\\overrightarrow{h}_t(j),\\overleftarrow{h}_t(j)\\bigr]\n",
    "  $$\n",
    "  donde $j$ recorre las $d$ dimensiones de los estados ocultos.\n",
    "\n",
    "**Entrenamiento:** Se entrena en la tarea de **inferencia textual natural (NLI)**, usando datasets como SNLI y MultiNLI. Dado un par de oraciones\n",
    "- oración base $u$,\n",
    "- hipótesis $v$,\n",
    "InferSent crea un vector combinado:\n",
    "  $$\n",
    "    x = [\\,u,\\; v,\\; u - v,\\; u \\odot v\\,],\n",
    "  $$\n",
    "que se pasa a una red feed‑forward con función de activación ReLU y capa softmax para predecir las clases {entailment, contradiction, neutral}. La función de pérdida es la entropía cruzada:\n",
    "  $$\n",
    "    \\mathcal{L}_{\\text{NLI}} = -\\sum_{c \\in \\{e,n,c\\}} y_c \\log p_c(x)\n",
    "  $$\n",
    "con $y_c$ la etiqueta real y $p_c(x)$ la probabilidad predicha.\n",
    "\n",
    "**Ventajas:**\n",
    "- Captura dependencias de largo alcance mediante LSTM.  \n",
    "- Max‑pooling extrae rasgos sintácticos y semánticos clave.  \n",
    "\n",
    "**Limitaciones:**\n",
    "- Entrenamiento costoso en datos NLI.  \n",
    "- No adapta el embedding para tareas fuera del dominio NLI sin fine‑tuning.\n",
    "\n",
    "**Universal Sentence Encoder (USE)**\n",
    "\n",
    "Google desarrolló el USE con dos variantes que equilibran eficiencia y precisión:\n",
    "\n",
    "1. **Deep Averaging Network (DAN):**  \n",
    "   - Se calculan embeddings de palabras y bi‑gramas:\n",
    "     $$ v_i = e_{w_i},\\quad g_i = e_{(w_i,w_{i+1})}. $$\n",
    "   - Se promedian:\n",
    "     $$ m = \\frac{1}{n-1} \\sum_{i=1}^{n-1} (v_i + g_i). $$\n",
    "   - Se proyecta mediante capas densas con ReLU:\n",
    "     $$ h = \\mathrm{ReLU}(W_1 m + b_1),\\quad v_{\\mathrm{sent}} = \\mathrm{ReLU}(W_2 h + b_2). $$\n",
    "   - El modelo es rápido y de baja latencia, adecuado para aplicaciones en línea.\n",
    "\n",
    "2. **Transformer Encoder:**  \n",
    "   - Cada token y bi‑grama recibe un vector de embeddings.  \n",
    "   - Se aplica *multi-head self-attention* en varias capas:\n",
    "     $$\n",
    "       \\mathrm{Attention}(Q,K,V) = \\mathrm{softmax}\\bigl(\\tfrac{QK^\\top}{\\sqrt{d_k}}\\bigr)V,\n",
    "     $$\n",
    "     donde $Q,K,V$ provienen de proyecciones lineales de las entradas.  \n",
    "   - Tras $L$ capas, se extrae el vector del token especial [CLS] o se promedian todos los vectores de salida para obtener $v_{\\mathrm{sent}}$.\n",
    "\n",
    "**Preentrenamiento:** USE se entrena con tareas de clasificación de parágrafos y predicción de texto, combinando señales de similitud semántica, minería de parágrafos duplicados y clasificación NLI. La función de pérdida suele ser entropía cruzada sobre múltiples objetivos simultáneos (multi-task), por ejemplo:\n",
    "  $$\n",
    "    \\mathcal{L}_{\\mathrm{USE}} = \\lambda_1 \\mathcal{L}_{\\mathrm{sim}} + \\lambda_2 \\mathcal{L}_{\\mathrm{NLI}} + \\lambda_3 \\mathcal{L}_{\\mathrm{dup}},\n",
    "  $$\n",
    "con pesos $\\lambda_i$.\n",
    "\n",
    "**Comparativa:**\n",
    "\n",
    "| Variante       | Precisión en transferencia | Latencia | Parámetros   |\n",
    "|----------------|-----------------------------|----------|--------------|\n",
    "| DAN            | Media                       | Muy baja | ~20M         |\n",
    "| Transformer    | Alta                        | Media    | ~110M        |\n",
    "\n",
    "**Aplicaciones generales**\n",
    "\n",
    "Las representaciones entrenadas en NLI o en tareas multi‑tarea demuestran desempeño sólido en:\n",
    "- **Clasificación de sentimientos**         \n",
    "- **Recuperación de información**           \n",
    "- **Detección de similitud semántica**      \n",
    "- **Respuesta a preguntas (QA) extractiva**\n",
    "\n",
    "Además, al ser preentrenamientos, pueden reutilizarse sin modificaciones en tareas con pocos datos propios.\n",
    "\n",
    "#### **Representación dirigida por tarea**\n",
    "\n",
    "Cuando el objetivo final difiere significativamente del preentrenamiento general, adaptar la representación de oración al dominio y la tarea específica mejora el rendimiento.\n",
    "\n",
    "**Atención supervisada sobre tokens relevantes**\n",
    "\n",
    "Se añade una capa de atención que pondera cada token $h_t$ según su importancia para la tarea:\n",
    "\n",
    "- Cálculo de puntuaciones:\n",
    "  $$\n",
    "    e_t = v_a^\\top \\tanh(W_h h_t + W_s s),\n",
    "  $$\n",
    "  donde $s$ puede ser un vector de estado inicial o contexto externo, y $v_a,W_h,W_s$ son parámetros aprendidos.\n",
    "\n",
    "- Normalización con softmax:\n",
    "  $$\n",
    "    \\alpha_t = \\frac{\\exp(e_t)}{\\sum_{k=1}^n \\exp(e_k)}.\n",
    "  $$\n",
    "\n",
    "- Vector de oración ponderado:\n",
    "  $$\n",
    "    v_{\\mathrm{task}} = \\sum_{t=1}^n \\alpha_t h_t.\n",
    "  $$\n",
    "\n",
    "Este mecanismo permite al modelo enfocarse en fragmentos críticos (por ejemplo, negaciones o entidades) y reduce el ruido de tokens irrelevantes.\n",
    "\n",
    "**Multi-task Learning y pérdida conjunta**\n",
    "\n",
    "Al entrenar simultáneamente en la tarea principal y tareas auxiliares (por ejemplo, etiquetado de entidades, detección de sentimientos), se define una pérdida combinada:\n",
    "  $$\n",
    "    \\mathcal{L} = \\lambda_p \\mathcal{L}_{\\mathrm{principal}} + \\sum_{i=1}^m \\lambda_i \\mathcal{L}_{\\mathrm{aux}_i},\n",
    "  $$\n",
    "con $\\lambda_p,\\lambda_i$ pesos de cada tarea y pérdidas específicas $\\mathcal{L}_{\\mathrm{aux}}$ (entropía cruzada, CRF, etc.). Esta configuración refuerza las características compartidas y mejora la generalización.\n",
    "\n",
    "**Fine‑tuning de modelos preentrenados (Transfer Learning)**\n",
    "\n",
    "Métodos modernos como BERT incorporan un preentrenamiento masivo y capas de Transformer. Para tareas de clasificación de oraciones se procede a:\n",
    "\n",
    "1. **Añadir cabecera de clasificación:** un vector de salida `[CLS]` se conecta a una capa densa:\n",
    "   $$\n",
    "     \\hat y = \\mathrm{softmax}(W_{cls} h_{[CLS]} + b_{cls}).\n",
    "   $$\n",
    "2. **Fine‑tuning end‑to‑end:** todos los parámetros de BERT y la capa adicional se actualizan mínimo un par de epochs con tasa de aprendizaje baja.\n",
    "3. **Pérdida de entropía cruzada:**\n",
    "   $$\n",
    "     \\mathcal{L}_{\\mathrm{BERT-tune}} = -\\sum_{c} y_c \\log \\hat y_c.\n",
    "   $$\n",
    "\n",
    "Este enfoque adapta las representaciones de oraciones a características propias de la tarea (voz activa/pasiva, tono, polaridad).\n",
    "\n",
    "**Atención multi-head y pooling adaptativo**\n",
    "\n",
    "Al utilizar Transformers, se pueden extraer representaciones dirigidas mediante *attention pooling*:\n",
    "\n",
    "1. **Multi-head Attention Output:** de la última capa se obtienen vectores $H = [h_1,\\dots,h_n]$.\n",
    "2. **Cálculo de logits de atención para cada cabecera:**\n",
    "   $$\n",
    "     A = \\mathrm{softmax}\\bigl(W_A H^\\top\\bigr),\n",
    "   $$\n",
    "   produciendo pesos $\\alpha_{t}^{(i)}$ por head.\n",
    "3. **Pooling adaptativo:**\n",
    "   $$\n",
    "     v_{\\mathrm{mh}} = \\bigl[\\sum_t \\alpha_t^{(1)} h_t;\\dots;\\sum_t \\alpha_t^{(k)} h_t\\bigr],\n",
    "   $$\n",
    "   concatenando los outputs de $k$ cabeceras para capturar perspectivas múltiples.\n",
    "\n",
    "**Comparativa de enfoques dirigidos**\n",
    "\n",
    "| Método                    | Adaptación | Dependencia de datos | Costo computacional |\n",
    "|---------------------------|------------|----------------------|---------------------|\n",
    "| Capa atención simple      | Media      | Moderada             | Baja–Media          |\n",
    "| Multi-task Learning       | Alta       | Alta                 | Media–Alta          |\n",
    "| Fine‑tuning BERT          | Muy alta   | Moderada–Alta        | Alta                |\n",
    "| Attention pooling adapt.  | Media      | Moderada             | Media               |\n",
    "\n",
    "Cada técnica presenta un balance entre la cantidad de datos etiquetados necesarios, la flexibilidad para tareas nuevas y el coste de entrenamiento.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Representaciones distribuidas de documentos** \n",
    "\n",
    "Doc2vec nos permite aprender directamente las representaciones de textos de longitud arbitraria (frases, oraciones, párrafos y documentos), teniendo en cuenta el contexto de las palabras del texto.\n",
    "\n",
    "Esto es similar a Word2vec en términos de su arquitectura general, excepto que, además de los vectores de palabras, también aprende un \"vector de párrafo\" que aprende una representación del texto completo (es decir, con palabras en contexto). Cuando se aprende con un corpus grande de muchos textos, los vectores de párrafo son únicos para un texto determinado (donde \"texto\" puede significar cualquier fragmento de texto de longitud arbitraria), mientras que los vectores de palabras se compartirán en todos los textos.  \n",
    "\n",
    "Hay dos arquitecturas del modelo Doc2Vec, que es una extensión de Word2Vec diseñada para generar representaciones vectoriales no solo para palabras sino también para piezas de texto más grandes como oraciones, párrafos y documentos. Estas representaciones vectoriales son útiles para muchas tareas de procesamiento del lenguaje natural, como la clasificación de textos y la búsqueda semántica. Aquí están las dos arquitecturas: \n",
    "\n",
    "**Memoria distribuida (DM)**: \n",
    "\n",
    "En el modelo DM de Doc2Vec, cada palabra y el párrafo (o documento) entero tienen su propio vector de aprendizaje único en una \"Paragraph Matrix\" y en una \"Word Matrix\", respectivamente. \n",
    "\n",
    "Durante el entrenamiento, el modelo intenta predecir la siguiente palabra en un contexto dada una ventana de palabras y el vector único del párrafo/documento. \n",
    "\n",
    "Los vectores de las palabras y del párrafo se pueden promediar o concatenar antes de enviarlos a una capa de clasificador, que intenta predecir la palabra siguiente. \n",
    "\n",
    "El objetivo es que al final del entrenamiento, el vector del párrafo capture la esencia del texto, lo que hace posible usar este vector para tareas de clasificación o comparación de similitud. \n",
    "\n",
    "**Bolsa de palabras distribuidas (DBOW)**: \n",
    "\n",
    "El modelo DBOW funciona de manera inversa al DM. Ignora el contexto de las palabras y, en su lugar, fuerza al modelo a predecir las palabras en un párrafo/documento dada solo la identificación del párrafo (es decir, su vector único). \n",
    "\n",
    "No hay una capa de promedio o concatenación; el modelo directamente predice las palabras a partir del vector del párrafo. \n",
    "\n",
    "Al igual que en el modelo DM, el vector del párrafo se entrena para representar el contenido completo del párrafo/documento. \n",
    "\n",
    "DBOW es eficaz para grandes conjuntos de datos donde la semántica puede ser capturada incluso sin el orden exacto de las palabras. \n",
    "\n",
    "Ambos métodos son útiles para aprender representaciones vectoriales que reflejan el significado de los párrafos o documentos, aunque capturan diferentes aspectos de los datos: DM toma en cuenta el orden de las palabras, mientras que DBOW se centra en la ocurrencia de las palabras. Estos vectores resultantes pueden ser utilizados en diversas tareas, tales como agrupación de documentos, clasificación y búsqueda por similitud semántica. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Representación orientada a tareas**\n",
    "\n",
    "En entornos de procesamiento de lenguaje natural (NLP) y recuperación de información (IR), los documentos adoptan variadas formas y tamaños, desde párrafos breves hasta informes completos. Si bien los embeddings generales de documentos (como Doc2Vec en sus variantes DM y DBOW) ofrecen vectores eficaces para tareas amplias, en escenarios específicos es fundamental **afinar** estas representaciones para reflejar las necesidades particulares de la aplicación final.\n",
    "\n",
    "La **representación orientada a tareas** surge para reducir la brecha entre la generalidad del embedding base y la especialización requerida por tareas como resumen automático, clasificación de textos o sistemas de pregunta‑respuesta (QA).\n",
    "\n",
    "Exploramos dos estrategias principales: el **fine‑tuning** de embeddings preentrenados mediante retropropagación y el **aprendizaje multi‑tarea** (\"multi-task learning\"), donde múltiples objetivos se entrenan de forma conjunta. Ambas metodologías se apoyan en formulaciones matemáticas rigurosas para adaptar vectores de documento a dominios concretos.\n",
    "\n",
    "#### **Fine‑tuning de embeddings de documento**\n",
    "\n",
    "El proceso de fine‑tuning consiste en tomar un modelo preentrenado, por ejemplo, un encoder de documentos en arquitectura Transformer o un Doc2Vec y continuar su entrenamiento en un conjunto etiquetado para la tarea meta. Imagina que disponemos de un conjunto de documentos $\\{D_i\\}_{i=1}^N$, cada uno con su embedding inicial $v_i^{(0)}\\in \\mathbb{R}^d$, generado por la red base. Para una tarea de clasificación, asociamos a cada documento una etiqueta $y_i\\in \\{1,\\dots,C\\}$.\n",
    "\n",
    "**Función objetivo y retropropagación**\n",
    "\n",
    "Se define una capa de clasificación sobre el embedding:\n",
    "\n",
    "$$\n",
    "  \\hat p_i = \\mathrm{softmax}(W_{c} v_i + b_{c}),\n",
    "$$\n",
    "\n",
    "donde $W_{c}\\in \\mathbb{R}^{C\\times d}$ y $b_{c}\\in\\mathbb{R}^C$ son parámetros de la capa de salida, y\n",
    "\n",
    "$$\n",
    "  \\hat p_{i,j} = \\frac{\\exp\\bigl((W_{c} v_i + b_{c})_j\\bigr)}{\\sum_{k=1}^C \\exp\\bigl((W_{c} v_i + b_{c})_k\\bigr)}.\n",
    "$$\n",
    "\n",
    "La pérdida de entropía cruzada para un solo ejemplo es:\n",
    "\n",
    "$$\n",
    "  \\mathcal{L}_i = -\\sum_{j=1}^C \\mathbb{I}_{[y_i=j]} \\log\\bigl(\\hat p_{i,j}\\bigr).\n",
    "$$\n",
    "\n",
    "El fine‑tuning también actualiza los parámetros internos $\\theta$ del encoder de documento (sea Transformer, RNN o Doc2Vec). El objetivo total sobre el dataset es:\n",
    "\n",
    "$$\n",
    "  \\mathcal{L}_{\\mathrm{fine}} = \\frac{1}{N} \\sum_{i=1}^N \\mathcal{L}_i + \\lambda\\,\\lVert \\theta \\rVert^2,\n",
    "$$\n",
    "\n",
    "donde $\\lambda$ aplica regularización L2 para evitar sobreajuste. El descenso por gradiente (o variantes como Adam) actualiza simultáneamente $W_c, b_c$ y $\\theta$:\n",
    "\n",
    "$$\n",
    "  \\theta \\leftarrow \\theta - \\eta \\frac{\\partial \\mathcal{L}_{\\mathrm{fine}}}{\\partial \\theta},\n",
    "  \\quad\n",
    "  W_{c} \\leftarrow W_{c} - \\eta \\frac{\\partial \\mathcal{L}_{\\mathrm{fine}}}{\\partial W_{c}},\n",
    "$$\n",
    "\n",
    "y $\\eta$ es la tasa de aprendizaje. Este ajuste continuo permite que el embedding capture patrones cruciales para la clasificación de documentos, tales como la presencia de términos discriminativos, la estructura argumental y la coherencia temática.\n",
    "\n",
    "**Ejemplo: fine‑tuning para resumen extractivo**\n",
    "\n",
    "En resumen automático extractivo, se entrena un modelo que aprueba o rechaza cada frase $s_{i,j}$ de un documento $D_i$. Se obtiene el embedding de frase $v_{i,j}$ y se aplica:\n",
    "\n",
    "$$\n",
    "  \\hat p_{i,j} = \\sigma\\bigl(w_{s}^{\\top} v_{i,j} + b_{s}\\bigr),\n",
    "$$\n",
    "\n",
    "con $\\sigma(x) = 1/(1+e^{-x})$. La pérdida binaria log‑loss:\n",
    "\n",
    "$$\n",
    "  \\mathcal{L}_{i,j} = -\\bigl[y_{i,j}\\log\\hat p_{i,j} + (1-y_{i,j})\\log(1-\\hat p_{i,j})\\bigr].\n",
    "$$\n",
    "\n",
    "Fine‑tuning ajusta tanto $w_s, b_s$ como $\\theta$ del encoder de frase, permitiendo que la representación enfatice contenidos relevantes para el resumen.\n",
    "\n",
    "**Ventajas y desafíos del fine‑tuning**\n",
    "\n",
    "- **Ventajas:**\n",
    "  - Alta precisión en el dominio específico.  \n",
    "  - Aprovechamiento de conocimiento general preentrenado.  \n",
    "\n",
    "- **Desafíos:**\n",
    "  - Riesgo de sobreajuste con pocos datos etiquetados.  \n",
    "  - Coste computacional elevado si el encoder es muy grande.  \n",
    "  - Necesidad de ajustes cuidadosos de la tasa de aprendizaje $\\eta$ y regularización $\\lambda$.  \n",
    "\n",
    "#### **Aprendizaje multi‑tarea (Multi‑Task Learning)**\n",
    "\n",
    "El **multi-task learning** optimiza simultáneamente varias tareas relacionadas, compartiendo representaciones subyacentes y promoviendo la transferencia de conocimiento entre tareas. Para documentos, esto puede incluir tareas como clasificación temática, análisis de sentimiento y extracción de entidades.\n",
    "\n",
    "**Formulación matemática**\n",
    "\n",
    "Supongamos $T$ tareas, cada una con función de pérdida $\\mathcal{L}^{(t)}$. Un ejemplo de tareas podría ser:\n",
    "\n",
    "1. **Resumen extractivo**: $\\mathcal{L}^{(1)}$.  \n",
    "2. **Clasificación de tema**: $\\mathcal{L}^{(2)}$.  \n",
    "3. **Detección de sentimiento**: $\\mathcal{L}^{(3)}$.\n",
    "\n",
    "Se comparte el encoder de documento con parámetros $\\theta_s$ (“shared”), y cada tarea posee parámetros específicos $\\theta_t$ (\"task‑specific\"). La pérdida total se define como:\n",
    "\n",
    "$$\n",
    "  \\mathcal{L}_{\\mathrm{MTL}} = \\sum_{t=1}^T \\lambda_t \\frac{1}{N_t} \\sum_{i=1}^{N_t} \\mathcal{L}_i^{(t)}(\\theta_s,\\theta_t),\n",
    "$$\n",
    "\n",
    "con pesos $\\lambda_t>0$ que equilibran la importancia de cada tarea y $N_t$ el número de ejemplos de la tarea $t$. Las actualizaciones por gradiente son:\n",
    "\n",
    "$$\n",
    "  \\theta_s \\leftarrow \\theta_s - \\eta \\sum_{t=1}^T \\lambda_t \\frac{\\partial \\mathcal{L}^{(t)}}{\\partial \\theta_s},\n",
    "$$\n",
    "$$\n",
    "  \\theta_t \\leftarrow \\theta_t - \\eta \\lambda_t \\frac{\\partial \\mathcal{L}^{(t)}}{\\partial \\theta_t},\\quad t=1,\\dots,T.\n",
    "$$\n",
    "\n",
    "Este esquema aprovecha la señal adicional de tareas relacionadas para mejorar la calidad del embedding compartido $\\theta_s$, reduciendo la probabilidad de sobreajuste y mejorando la generalización.\n",
    "\n",
    "**Ejemplo: tareas combinadas de QA y clasificación**\n",
    "\n",
    "En un sistema de pregunta‑respuesta, podemos combinar:\n",
    "- **Respuesta extractiva (QA):** $\\mathcal{L}^{(1)}$ entropía cruzada sobre spans de texto.  \n",
    "- **Clasificación de tema:** $\\mathcal{L}^{(2)}$ entropía cruzada sobre categorías.\n",
    "\n",
    "La capa compartida $\\theta_s$ aprende características sintácticas útiles para identificar tanto las secciones relevantes de texto como el dominio temático, mientras que $\\theta_1$ y $\\theta_2$ se especializan en cada tarea.\n",
    "\n",
    "**Regularización implícita y explícita**\n",
    "\n",
    "- **Regularización implícita:** el multi-task actúa como un \"armado inductivo\", ya que la necesidad de resolver varias tareas impide sobreajustar una sola.  \n",
    "- **Regularización explícita:** se pueden añadir términos penalizadores similares a L2 o dropout en $\\theta_s$ para mejorar la robustez.\n",
    "\n",
    "**Weighting dinámico de tareas**\n",
    "\n",
    "Determinar pesos $\\lambda_t$ adecuados es crítico. Estrategias dinámicas incluyen:\n",
    "1. **Uncertainty weighting** (Kendall et al., 2018): el peso se ajusta según la varianza de la pérdida de cada tarea, definiendo $\\lambda_t = 1/(2\\sigma_t^2)$ y aprendiendo $\\sigma_t$.  \n",
    "2. **GradNorm** (Chen et al., 2018): equilibra magnitudes de gradiente para que ninguna tarea domine la actualización de $\\theta_s$.  \n",
    "Ambos métodos conducen a mejoras en la convergencia y a representaciones más equilibradas.\n",
    "\n",
    "**Comparativa y consideraciones de diseño**\n",
    "\n",
    "| Aspecto                | Fine‑tuning simple  | Multi‑Task Learning      | Ventaja clave                               |\n",
    "|------------------------|---------------------|--------------------------|---------------------------------------------|\n",
    "| Dependencia de datos   | Alta                | Moderada                 | MTL necesita múltiples anotaciones          |\n",
    "| Costo computacional    | Alto (por tarea)    | Compartido                | MTL entrena un solo encoder para varias     |\n",
    "| Riesgo de sobreajuste  | Alto (pocos datos)  | Bajo                     | MTL regulariza implícitamente               |\n",
    "| Adaptabilidad          | Alta                | Media                    | Fine‑tuning específico para cada tarea      |\n",
    "\n",
    "\n",
    "**Extensiones y tendencias actuales**\n",
    "\n",
    "1. **Meta‑Learning para NLP:** se entrena un modelo que aprende a adaptarse rápidamente a tareas nuevas con pocos ejemplos (learning-to-learn).  \n",
    "2. **Adapters en Transformers:** módulos ligeros insertados entre capas preentrenadas, entrenables por tarea, reducen drásticamente el número de parámetros ajustados.  \n",
    "3. **Prompt‑Tuning:** en modelos de lenguaje grande, se optimizan vectores de \"prompt\" para dirigir el encoder a la tarea deseada sin modificar parámetros básicos.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
