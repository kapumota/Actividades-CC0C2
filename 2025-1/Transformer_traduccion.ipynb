{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Transformers para traducción de lenguaje**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **¿Por qué transformers?**\n",
    "\n",
    "En el campo del procesamiento de lenguaje natural (NLP), a menudo se trabaja con datos secuenciales, como oraciones. Antes de los transformers, los modelos más comunes eran las redes neuronales recurrentes (RNNs) y las redes de memoria a largo plazo (LSTMs). Estos modelos procesan los datos de forma secuencial, es decir, leen una oración palabra por palabra, una tras otra. Esto los hace lentos y menos eficientes para secuencias largas. Además, las RNNs y LSTMs pueden tener dificultades para mantener la información de etapas anteriores de la secuencia, algo vital para comprender el contexto.\n",
    "\n",
    "Los transformers introdujeron una nueva forma de procesar secuencias. En lugar de leer palabra por palabra en orden, pueden mirar toda la secuencia de una vez. Este enfoque los hace más rápidos y eficientes. También pueden comprender mejor el contexto de cada palabra en una oración, sin importar su longitud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuración\n",
    "\n",
    "Antes de comenzar, asegúrate de tener instaladas todas las librerías necesarias. Puedes ejecutar los siguientes comandos para instalarlas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U torchdata==0.5.1\n",
    "#!pip install -U spacy==3.7.2\n",
    "#!pip install -Uqq portalocker==2.7.0\n",
    "#!pip install -qq torchtext==0.14.1\n",
    "#!pip install -Uq nltk==3.8.1\n",
    "\n",
    "#!python -m spacy download de\n",
    "#!python -m spacy download en\n",
    "\n",
    "#!pip install pdfplumber==0.9.0\n",
    "#!pip install fpdf==1.7.2\n",
    "\n",
    "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/Multi30K_de_en_dataloader.py'\n",
    "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/transformer.pt'\n",
    "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/input_de.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importación de librerías requeridas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import multi30k, Multi30k\n",
    "import torch\n",
    "from typing import Iterable, List\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "# También puedes usar esta sección para suprimir advertencias generadas por tu código:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define símbolos especiales e índices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Asegúrate de que los tokens estén en el orden de sus índices para insertarlos correctamente en el vocabulario\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cargador de datos (dataLoader)**\n",
    "\n",
    "En el conjunto de datos Multi30K inglés-alemán, primero cargas los datos y descompones las oraciones en palabras o fragmentos más pequeños, llamados tokens. A partir de estos tokens creas una lista única o vocabulario. Luego, cada token se convierte en un número específico usando ese vocabulario. Dado que las oraciones pueden tener longitudes diferentes, se añade padding para igualarlas al mismo tamaño en un batch. \n",
    "\n",
    "Todos estos datos procesados se organizan en un `DataLoader` de PyTorch, lo que facilita su uso para entrenar redes neuronales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Multi30K_de_en_dataloader.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Has configurado los dataloaders para entrenamiento y prueba.  Dado el trabajo exploratorio, usa un tamaño de batch de uno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, _ = get_translation_dataloaders(batch_size = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Inicializa un iterador para el data loader de validación:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_itr=iter(train_dataloader)\n",
    "data_itr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obtener ejemplos diversos, puedes iterar por múltiples muestras ya que el dataset está ordenado por longitud:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(1000):\n",
    "    german, english= next(data_itr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset está estructurado como secuencia-batch-feature, en lugar de batch-feature-secuencia. Para compatibilidad con las funciones auxiliares, puedes transponer los tensores:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german=german.T\n",
    "english=english.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes imprimir el texto convirtiendo los índices a palabras usando `index_to_german` e `index_to_english`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(10):\n",
    "    german, english= next(data_itr)\n",
    "\n",
    "    print(\"Muestra {}\".format(n))\n",
    "    print(\"Entrada en alemán\")\n",
    "    print(index_to_german(german))\n",
    "    print(\"Objetivo en inglés\")\n",
    "    print(index_to_eng(english))\n",
    "    print(\"_________\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define tu dispositivo (CPU o GPU) para entrenamiento. Verificarás si hay una GPU disponible y la usarás de lo contrario, emplearás la CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que cubriste la preparación de datos, pasemos a entender los componentes clave del transformer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Conceptos importantes**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Enmascaramiento (masking)**\n",
    "\n",
    "Durante el entrenamiento, toda la secuencia está visible para el modelo y se utiliza como entrada para aprender patrones. En cambio, durante la predicción no se dispone de la parte futura de la secuencia. Para simular esta carencia de datos futuros, se emplea el enmascaramiento (masking), garantizando que el modelo aprenda a predecir sin ver los tokens siguientes reales. Esto es crucial para asegurar que ciertas posiciones no sean atendidas. \n",
    "\n",
    "La función `generate_square_subsequent_mask` produce una matriz triangular superior, que impide que, durante la decodificación, un token atienda a tokens futuros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz,device=DEVICE):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por otro lado, la función `create_mask` genera tanto las máscaras de origen (source) y destino (target) como las máscaras de relleno (padding) basadas en las secuencias proporcionadas. Las máscaras de padding evitan que el modelo atienda a los tokens de relleno, simplificando la atención."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(src, tgt,device=DEVICE):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Codificación posicional (positional encoding)**\n",
    "\n",
    "El modelo transformer no tiene conocimiento inherente del orden de los tokens en la secuencia. Para proporcionarle esta información, se añaden codificaciones posicionales a los embeddings de los tokens. Estas codificaciones siguen un patrón fijo basado en su posición en la secuencia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Agrega información posicional a los tokens de entrada\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Embedding de tokens (token embedding)**\n",
    "\n",
    "El embedding de tokens, también conocido como embedding de palabras o representación de palabras, convierte palabras o tokens en vectores numéricos en un espacio vectorial continuo. Cada palabra única en el corpus se representa por un vector de longitud fija cuyos valores reflejan propiedades lingüísticas, como significado, contexto o relaciones con otras palabras.\n",
    "\n",
    "La clase `TokenEmbedding` siguiente convierte tokens numéricos en embeddings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Arquitectura Transformer para traducción de lenguaje**\n",
    "\n",
    "La traducción de lenguaje con un modelo transformer se basa en una arquitectura encoder-decoder. A continuación se desglosan los componentes esenciales y el procedimiento de entrenamiento para una comprensión clara.\n",
    "\n",
    "#### **Tokenización y codificación posicional**\n",
    "\n",
    "Primero, el texto en el idioma origen (secuencia de entrada) se tokeniza, dividiéndolo en palabras o subpalabras. Estos tokens se convierten en representaciones numéricas y, para conservar la información de orden, se les añaden codificaciones posicionales.\n",
    "\n",
    "#### **Procesamiento del encoder**\n",
    "\n",
    "Estos tokens numéricos se pasan luego por el encoder, compuesto por varias capas que incluyen mecanismos de self-attention y redes feed-forward. Esta arquitectura permite al transformer procesar la secuencia completa de una vez, a diferencia de modelos secuenciales como LSTMs o GRUs.\n",
    "\n",
    "#### **Decodificación con teacher forcing**\n",
    "\n",
    "Durante el entrenamiento, el texto en el idioma destino (secuencia correcta de salida) también se tokeniza y convierte en tokens numéricos. El *teacher forcing* es una técnica de entrenamiento en la que el decoder recibe los tokens reales de destino como entrada. El decoder utiliza tanto la salida del encoder como los tokens generados previamente (comenzando con un token especial de inicio) para predecir el siguiente token de la secuencia.\n",
    "\n",
    "#### **Generación de salida y cálculo de la pérdida**\n",
    "\n",
    "El decoder genera la traducción token a token. En cada paso, predice el siguiente token de la secuencia destino. La secuencia predicha se compara con la secuencia real usando una función de pérdida, normalmente entropía cruzada, que cuantifica lo bien que coinciden las predicciones del modelo con la secuencia objetivo.\n",
    "\n",
    "#### **Seq2SeqTransformer**\n",
    "\n",
    "La clase `Seq2SeqTransformer` representa el núcleo del modelo transformer para traducción de lenguaje. Para entrenarla de manera efectiva, se abordarán:\n",
    "\n",
    "* **Carga de datos:** Preparar los datos de entrenamiento (texto origen y su traducción).\n",
    "* **Inicialización del modelo:** Configurar encoder, decoder, codificaciones posicionales y demás componentes.\n",
    "* **Configuración del optimizador:** Elegir un optimizador (por ejemplo, Adam) y programar la tasa de aprendizaje.\n",
    "* **Bucle de entrenamiento:** Iterar sobre los datos durante varias épocas, aplicando *teacher forcing*.\n",
    "* **Monitoreo de la pérdida:** Registrar y, opcionalmente, graficar las pérdidas por época."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        outs =outs.to(DEVICE)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Inferencia**\n",
    "\n",
    "El diagrama siguiente ilustra el proceso de predicción de secuencia o inferencia. Puedes comenzar alimentando los índices de la secuencia que deseas traducir al **encoder**, representado en la sección naranja inferior izquierda. \n",
    "\n",
    "Los embeddings resultantes del encoder se envían al **decoder**, resaltado en verde. Además, se introduce un token de inicio (`<bos>`) al principio de la entrada del decoder, como se muestra en la base del segmento verde. \n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/predict_transformers.png\" alt=\"transformer\" width=\"600\">\n",
    "\n",
    "La salida del decoder se mapea a un vector de tamaño igual al del vocabulario mediante una capa lineal. \n",
    "\n",
    "A continuación, una función softmax convierte esas puntuaciones en probabilidades. La probabilidad más alta, determinada por la función `argmax`, proporciona el índice de la palabra predicha dentro de la secuencia traducida. Este índice predicho se retroalimenta al decoder junto con la secuencia inicial, preparando el siguiente paso para determinar la próxima palabra en la traducción. \n",
    "\n",
    "Este proceso autorregresivo se muestra con la flecha que va desde la parte superior del decoder (verde) hasta la base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comencemos con un modelo ya entrenado. Para ello, carga los pesos del transformer desde el archivo `'transformer.pt'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.load_state_dict(torch.load('transformer.pt', map_location=DEVICE, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que el dataset está organizado por longitud de secuencia, iteremos para obtener una secuencia más larga:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(100):\n",
    "    src ,tgt= next(data_itr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muestra la secuencia de origen en alemán que deseas traducir y la secuencia objetivo en inglés que esperas que el modelo produzca:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"engish target\",index_to_eng(tgt))\n",
    "print(\"german input\",index_to_german(src))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtén el número de tokens de la muestra en alemán:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = src.shape[0]\n",
    "num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construye una máscara para indicar qué entradas participan en el cálculo de atención. En una tarea de traducción todos los tokens de la secuencia origen están disponibles, por lo que la máscara será `False` en todas las posiciones:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool).to(DEVICE )\n",
    "src_mask[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extrae la primera muestra del batch de secuencias a traducir. Aunque ahora parezca redundante, será útil cuando trabajes con batches más grandes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_=src[:,0].unsqueeze(1)\n",
    "print(src_.shape)\n",
    "print(src.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Alimenta los tokens de la secuencia al transformer junto con la máscara. El resultado, guardado en `memory`, son los embeddings producidos por el encoder:\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/Transformersencoder.png\" alt=\"trasfoemr\" width=\"400\">\n",
    "\n",
    "La **memoria** (memory), que es la salida del encoder, encapsula la secuencia original que queremos traducir y sirve como entrada para el decoder:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = transformer.encode(src_, src_mask)\n",
    "memory.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para iniciar la generación de la secuencia de salida, comienza con el símbolo de inicio  (`<bos>`):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = torch.ones(1, 1).fill_(BOS_IDX).type(torch.long).to(DEVICE)\n",
    "ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por convención de nombres, el término **target** se usa para denotar la predicción. En este contexto, \"target\" se refiere a las palabras que siguen a la predicción actual. \n",
    "\n",
    "Estas pueden combinarse con la secuencia de origen para realizar predicciones posteriores. Por tanto, construimos una máscara de destino donde todos los valores son `False`, indicando que no se debe ignorar ninguna posición:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_mask = (generate_square_subsequent_mask(ys.size(0)).type(torch.bool)).to(DEVICE)\n",
    "tgt_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Alimentamos la salida del encoder (`memory`) y la predicción previa (por ahora solo el token de inicio) al decoder:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = transformer.decode(ys, memory, tgt_mask)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "La salida del decoder es un embedding enriquecido de la palabra predicha. A continuación, eliminamos la dimensión de batch reordenando:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.transpose(0, 1)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que el decoder produce su salida, esta se pasa por la capa de salida para obtener los valores de logits sobre el vocabulario de 10 837 palabras. Más adelante solo necesitarás el último token, así que puedes usar ```out[:, -1]```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = transformer.generator(out[:, -1])\n",
    "logit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El proceso se ilustra de manera concisa en la imagen siguiente:\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/decoder_start.png\" alt=\"trasfoemr\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La palabra predicha se determina identificando el valor de logit más alto, lo cual indica la traducción más probable del modelo para la entrada en una posición específica; esta posición corresponde al índice del siguiente token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  _, next_word_index = torch.max(logit, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Salida en ingles:\",index_to_eng(next_word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solo necesitas un entero para el índice:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_word_index=next_word_index.item()\n",
    "next_word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, concatena la palabra recién predicha a las predicciones anteriores, permitiendo que el modelo considere toda la secuencia de palabras generadas al realizar su próxima predicción.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = torch.cat([ys,torch.ones(1, 1).type_as(src.data).fill_(next_word_index)], dim=0)\n",
    "ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para predecir la palabra siguiente en la traducción, actualiza la máscara de destino y utiliza el decodificador del transformer para obtener las probabilidades de las palabras. La palabra con la probabilidad más alta es entonces seleccionada como predicción. \n",
    "\n",
    "Ten en cuenta que la salida del codificador contiene toda la información que necesitas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actualiza la máscara de destino para la longitud de la secuencia actual.\n",
    "tgt_mask = (generate_square_subsequent_mask(ys.size(0)).type(torch.bool)).to(DEVICE)\n",
    "tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decodifica la secuencia actual utilizando el transformer y recupera la salida\n",
    "out = transformer.decode(ys, memory, tgt_mask)\n",
    "out = out.transpose(0, 1)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[:, -1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtiene las probabilidades de palabras para la última palabra predicha.\n",
    "prob = transformer.generator(out[:, -1])\n",
    "# Encuentra el índice de palabra con la probabilidad más alta.\n",
    "_, next_word_index = torch.max(prob, dim=1)\n",
    "# Imprime la palabra en inglés predicha.\n",
    "print(\"Salida en inglés:\", index_to_eng(next_word_index))\n",
    "# Convierte el valor del tensor a un escalar de Python.\n",
    "next_word_index = next_word_index.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, actualiza la predicción.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = torch.cat([ys,torch.ones(1, 1).type_as(src.data).fill_(next_word_index)], dim=0)\n",
    "print(\"Salida en inglés\",index_to_eng(ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El proceso puede resumirse de la siguiente manera: comenzando con la salida inicial del codificador y el token `<BOS>`, la salida del decodificador se retroalimenta en el decodificador hasta que la secuencia traducida se decodifica por completo. \n",
    "\n",
    "Este ciclo continúa hasta que la longitud de la nueva secuencia traducida coincida con la de la secuencia original. Como se muestra en la siguiente imagen, la función `greedy_decode` también está incluida.\n",
    "    \n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/decoder.png\" alt=\"trasfoemr\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(modelo, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = modelo.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = modelo.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = modelo.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se recupera los índices para el idioma alemán y genera la máscara correspondiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src\n",
    "src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool).to(DEVICE )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establece un valor razonable para la longitud máxima de la secuencia objetivo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=src.shape[0]+5\n",
    "max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplica la función `greedy_decode` a los datos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys=greedy_decode(transformer, src, src_mask, max_len, start_symbol=BOS_IDX)\n",
    "print(\"Inglés \",index_to_eng(ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Observa que funciona, pero no es exactamente lo mismo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Inglés \",index_to_eng(tgt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Entrenamiento vs. inferencia en traducción automática neuronal**\n",
    "\n",
    "Durante la fase de inferencia, cuando el modelo se despliega para tareas reales de traducción, el decodificador genera la secuencia sin tener acceso a la secuencia objetivo esperada. En su lugar, basa sus predicciones en la salida del codificador y en los tokens que ha generado hasta el momento. \n",
    "\n",
    "El proceso es autorregresivo, con el decodificador prediciendo continuamente el siguiente token hasta que produce un token de fin de secuencia, indicando que la traducción está completa.\n",
    "\n",
    "La principal diferencia entre las etapas de entrenamiento e inferencia radica en las entradas al decodificador. Durante el entrenamiento, el decodificador se beneficia de la exposición de los ground truth, recibiendo los tokens exactos de la secuencia objetivo de manera incremental a través de una técnica conocida como \"teacher forcing\". \n",
    "\n",
    "Este enfoque contrasta marcadamente con algunas otras arquitecturas de redes neuronales que confían en las predicciones anteriores de la propia red como entradas durante el entrenamiento. Una vez finalizado el entrenamiento, los conjuntos de datos utilizados se asemejan a los empleados en modelos de redes neuronales más convencionales, proporcionando una base familiar para la comparación y evaluación.\n",
    "\n",
    "Primero, importa la pérdida `CrossEntropyLoss` y crea un objeto de pérdida de entropía cruzada. La pérdida **no** se calculará cuando el token con índice `PAD_IDX` sea una entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "loss_fn = CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elimina la última muestra del objetivo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_input = tgt[:-1, :]\n",
    "print(index_to_eng(tgt_input))\n",
    "print(index_to_eng(tgt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crea las máscaras requeridas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "print(f\"Forma de src_mask: {src_mask.shape}\")\n",
    "print(f\"Forma de tgt_mask: {tgt_mask.shape}\")\n",
    "print(f\"Forma de src_padding_mask: {src_padding_mask.shape}\")\n",
    "print(f\"Forma de tgt_padding_mask: {tgt_padding_mask.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la máscara de destino, cada columna subsecuente revela incrementamente más tokens al introducir valores de infinito negativo, desbloqueándolos. Puedes mostrar la máscara de destino para visualizar el progreso o identificar específicamente qué tokens están siendo enmascarados en cada paso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tgt_mask)\n",
    "[index_to_eng( tgt_input[t==0])  for t in tgt_mask] #index_to_eng(tgt_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando llamas a `modelo(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask)`, se invoca el método forward de la clase `Seq2SeqTransformer`. \n",
    "\n",
    "Este proceso genera *logits* para la secuencia objetivo, los cuales pueden luego convertirse en tokens reales tomando la predicción de mayor probabilidad en cada paso de la secuencia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Función de pérdida**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profundicemos en cómo puedes calcular la pérdida a partir de tu `src` y `tgt_input`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = transformer(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "print(\"Formato de la salida \",logits.shape)\n",
    "print(\"Formato del objetivo\",tgt_input.shape)\n",
    "print(\"Formato de la fuente \",src.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durante la fase de entrenamiento, un aspecto intrigante y sofisticado del proceso es la doble funcionalidad del objetivo. Actúa simultáneamente como entrada para el decodificador del transformer y como estándar contra el cual se mide la exactitud de la predicción. \n",
    "\n",
    "Para mayor claridad en las discusiones, nos referimos al objetivo usado como entrada para el decodificador como \"entrada al decodificador\". \n",
    "\n",
    "Por otro lado, el \"ground truth para la predicción\", que es la secuencia objetivo desplazada a la derecha, dado que es un modelo autorregresivo, se conocerá simplemente como \"objetivo de salida\" en lo sucesivo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ground truth para la predicción se desplaza simplemente a la derecha y se llama `tgt_out`. \n",
    "\n",
    "Puedes imprimir los tokens:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_out = tgt[1:, :]\n",
    "print(tgt_out.shape)\n",
    "[index_to_eng(t)  for t in tgt_out]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los índices de los tokens representan las clases que deseas predecir. Al aplanar el tensor, cada índice se convierte en una muestra distinta, sirviendo como objetivo para la pérdida de entropía cruzada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_out_flattened = tgt_out.reshape(-1)\n",
    "print(tgt_out_flattened.shape)\n",
    "tgt_out_flattened"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este modelo autorregresivo, muestra los tokens de entrada del objetivo después de aplicar la máscara. Junto a ellos, puedes mostrar la salida objetivo, ilustrando cómo el modelo predice hábilmente valores pasados basándose en los presentes. Esta visualización clara resalta la capacidad del modelo para usar la información actual e inferir lo que ha precedido, una característica clave de su naturaleza autorregresiva.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\"Entrada: {} objetivo: {}\".format(index_to_eng( tgt_input[m==0]),index_to_eng( t))  for m,t in zip(tgt_mask,tgt_out)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, calcula la pérdida ya que la salida del decodificador del transformers se proporciona como entrada a la función de pérdida de entropía cruzada junto con los valores de la secuencia objetivo. \n",
    "\n",
    "Dado que la salida del transformer tiene las dimensiones longitud de secuencia, tamaño de lote y características (`vocab_size`), es necesario remodelar esta salida para alinearla con el formato de entrada estándar requerido por la función de pérdida de entropía cruzada. \n",
    "\n",
    "Este paso asegura que la pérdida se calcule correctamente, comparando la secuencia predicha con los ground truths en cada paso de tiempo en todo el lote usando el método reshape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **El cálculo de la función pérdida**\n",
    "\n",
    "Eso es todo para el cálculo de la pérdida, pero si tienes curiosidad sobre cómo se calcula la pérdida aquí, esto es lo que sucede internamente al calcular la pérdida de entropía cruzada. \n",
    "\n",
    "Primero, verifica la forma de los tensores antes y después del remodelado:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits.reshape(-1, logits.shape[-1]) transforma el tensor logits en un tensor 2D con forma [longitud_de_secuencia * tamaño_de_lote, tamaño_del_vocabulario]. Este cambio de forma se realiza para alinear tanto los logits predichos como las salidas objetivo para el cálculo de la pérdida.\n",
    "print(\"La forma de logits es:\", logits.shape)\n",
    "logits_flattened = logits.reshape(-1, logits.shape[-1])\n",
    "print(\"La forma de logit_flats es:\", logits_flattened.shape)\n",
    "\n",
    "# tgt_out.reshape(-1) convierte el tensor tgt_out en un tensor 1D al aplanarlo a lo largo de las dimensiones de secuencia y tamaño de lote. Esto se hace para alinearlo con los logits transformados.\n",
    "print(\"La forma de tgt_outs es:\", tgt_out.shape)\n",
    "tgt_out_flattened = tgt_out.reshape(-1)\n",
    "print(\"La forma de tgt_out_flats es:\", tgt_out_flattened.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentro de la función de pérdida, los logits se transformarán en probabilidades entre `[0,1]` que suman 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando la función de pérdida de la entropia cruzada\n",
    "probs = torch.nn.functional.softmax(logits_flattened, dim=1)\n",
    "probs[1].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifica las probabilidades para algunos tokens aleatorios:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tu respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (5):\n",
    "    # using argmax, you can retrieve the index of the token that is predicted with the highest probaility\n",
    "    print(\"Id del token predicho:\",probs[i].argmax().item(), \"probabilidad predicha:\",probs[i].max().item())\n",
    "    # you can also check the actual token from the tgt_out_flat\n",
    "    print(\"Id del token actual:\",tgt_out_flattened[i].item(), \"probabilidad predicha:\", probs[i,tgt_out_flattened[i]].item(),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar que para muchos tokens el modelo está haciendo un buen trabajo al predecir el token, mientras que para algunos de los tokens el modelo no asigna una alta probabilidad al token real que se debe predecir. La diferencia entre la probabilidad predicha para dichos tokens es la razón por la que la pérdida no sumaría 0.\n",
    "\n",
    "Ahora, puedes proceder a calcular la diferencia entre la probabilidad del token real (1) y las probabilidades predichas para cada token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_log_likelihood = torch.nn.functional.nll_loss(probs, tgt_out_flattened)\n",
    "# Obteniendo el valor de perdida \n",
    "loss = neg_log_likelihood\n",
    "\n",
    "# Imprime el valor total de perdida\n",
    "print(\"Función de pérdida:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Evaluación**\n",
    "\n",
    "Siguiendo los procedimientos mencionados anteriormente, puedes desarrollar una función capaz de hacer predicciones y, posteriormente, calcular la pérdida correspondiente en los datos de validación. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(modelo):\n",
    "    modelo.eval()\n",
    "    losses = 0\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        logits = modelo(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(val_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Entrenando el modelo**\n",
    "\n",
    "Incorporando los pasos descritos anteriormente, procedemos a entrenar el modelo. Además de estos procedimientos específicos, el proceso de entrenamiento general se ajusta a los métodos convencionales empleados en el entrenamiento de redes neuronales. \n",
    "\n",
    "Ahora, escribimos una función para entrenar el modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(modelo, optimizer, train_dataloader):\n",
    "    modelo.train()\n",
    "    losses = 0\n",
    "\n",
    "    # Envuelve train_dataloader con tqdm para registrar el progreso\n",
    "    train_iterator = tqdm(train_dataloader, desc=\"Training\", leave=False)\n",
    "\n",
    "    for src, tgt in train_iterator:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        src_mask = src_mask.to(DEVICE)\n",
    "        tgt_mask = tgt_mask.to(DEVICE)\n",
    "        src_padding_mask = src_padding_mask.to(DEVICE)\n",
    "        tgt_padding_mask = tgt_padding_mask.to(DEVICE)\n",
    "\n",
    "        logits = modelo(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "        logits = logits.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "        # Actualiza la barra de progreso de tqdm con la pérdida actual\n",
    "        train_iterator.set_postfix(loss=loss.item())\n",
    "\n",
    "    return losses / len(list(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La configuración para el modelo de traducción incluye un tamaño de vocabulario de origen y destino determinado por los idiomas del conjunto de datos, un tamaño de embedding de 512, 8 cabecera de atención, una dimensión oculta para la red feed-forward de 512 y un tamaño de lote de 128. \n",
    "\n",
    "El modelo está estructurado con tres capas tanto en el encoder como en el decoder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crea un loader de entrenamiento con un tamaño de lote de 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, val_dataloader = get_translation_dataloaders(batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crea un modelo de transformer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "transformer = transformer.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializa los pesos del modelo transformer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepara el optimizador Adam.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializa las listas de pérdidas de entrenamiento y validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainLoss=[]\n",
    "ValLoss=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrena el modelo durante 10 épocas usando las funciones anteriores.\n",
    "\n",
    ">Ten en cuenta que entrenar el modelo usando CPUs puede ser un proceso que lleva mucho tiempo.\n",
    ">Si no tienes acceso a  GPUs, puede pasar directamente a \"cargar el modelo guardado\" y proceder a cargar el modelo preentrenado usando el código proporcionado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer, train_dataloader)\n",
    "    TrainLoss.append(train_loss)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer)\n",
    "    ValLoss.append(val_loss)\n",
    "    print((f\"Epoca: {epoch}, Pérdida de entrenamiento: {train_loss:.3f}, Pérdida de validación: {val_loss:.3f}, \"f\"Tiempo de epocas = {(end_time - start_time):.3f}s\"))\n",
    "torch.save(transformer.state_dict(), 'transformer_de_to_en_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dibujamos la pérdida de los datos de entrenamiento y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(TrainLoss) + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, TrainLoss, 'r', label='Pérdida de entrenamiento')\n",
    "plt.plot(epochs,ValLoss, 'b', label='Pérdida de validación')\n",
    "plt.title('Pérdida de entrenamiento y validación')\n",
    "plt.xlabel('Epocas')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Carga del modelo guardado**\n",
    "\n",
    "Si quieres omitir el entrenamiento y cargar el modelo preentrenado que se proporciona, descomenta la siguiente celda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/transformer_de_to_en_model.pt'\n",
    "#transformer.load_state_dict(torch.load('transformer_de_to_en_model.pt',map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Traducción y evaluación**\n",
    "\n",
    "Usando la función `greedy_decode` que definiste anteriormente, puedes crear una función traductor que genere la traducción al inglés de un texto alemán de entrada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traduce la frase de entrada al idioma objetivo\n",
    "def translate(modelo: torch.nn.Module, src_sentence: str):\n",
    "    modelo.eval()\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        modelo,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, veamos algunas traducciones de muestra:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(5):\n",
    "    german, english = next(data_itr)\n",
    "\n",
    "    print(\"Oración en alemán:\", index_to_german(german).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
    "    print(\"Traducción al inglés:\", index_to_eng(english).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
    "    print(\"Traducción del modelo:\", translate(transformer, index_to_german(german)))\n",
    "    print(\"_________\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluación con la puntuación BLEU**\n",
    "\n",
    "Para evaluar las traducciones generadas, se introduce una función `calculate_bleu_score`. Esta calcula la puntuación BLEU, una métrica común para la calidad de la traducción automática, comparando la traducción generada con traducciones de referencia. La puntuación BLEU ofrece una medida cuantitativa de la precisión de la traducción.\n",
    "\n",
    "El código también incluye un ejemplo de cómo calcular la puntuación BLEU para una traducción generada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu_score(generated_translation, reference_translations):\n",
    "    # convierte las traducciones generadas y las traducciones de referencia al formato esperado por sentence_bleu\n",
    "    references = [reference.split() for reference in reference_translations]\n",
    "    hypothesis = generated_translation.split()\n",
    "\n",
    "    # calcula la puntuación BLEU\n",
    "    bleu_score = sentence_bleu(references, hypothesis)\n",
    "\n",
    "    return bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_translation = translate(transformer,\"Ein brauner Hund spielt im Schnee .\")\n",
    "\n",
    "reference_translations = [\n",
    "    \"A brown dog is playing in the snow .\",\n",
    "    \"A brown dog plays in the snow .\",\n",
    "    \"A brown dog is frolicking in the snow .\",\n",
    "    \"In the snow, a brown dog is playing .\"\n",
    "\n",
    "]\n",
    "\n",
    "bleu_score = calculate_bleu_score(generated_translation, reference_translations)\n",
    "print(\" Puntuación de BLEU:\", bleu_score, \"for\",generated_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejercicio: Traducir un documento**\n",
    "\n",
    "En este ejercicio, implementarás una función que traduzca un PDF en alemán a inglés. Para lograrlo, utilizarás el mismo modelo de transformer de secuencia a secuencia discutido previamente y realizarás las modificaciones necesarias.\n",
    "\n",
    "1. **Definir la función de traducción**\n",
    "   Crea una función llamada `translate_pdf` que reciba los siguientes parámetros:\n",
    "\n",
    "   * `input_file`: La ruta al archivo PDF de entrada que se va a traducir.\n",
    "   * `translator_model`: Un modelo o función que manejará la traducción del texto.\n",
    "   * `output_file`: La ruta donde se guardará el PDF traducido.\n",
    "\n",
    "2. **Leer y traducir el PDF**\n",
    "   Utiliza `pdfplumber` para abrir y leer el texto de cada página del PDF de entrada. Traduce el texto extraído usando el `translator_model`.\n",
    "\n",
    "3. **Formatear y escribir el texto traducido en un nuevo PDF**\n",
    "\n",
    "   * Usa `textwrap` para ajustar el texto traducido de manera que se adapte al ancho de página A4.\n",
    "   * Crea un nuevo PDF con `FPDF` y añade el texto traducido ajustado.\n",
    "   * Guarda el nuevo PDF con el texto traducido en `output_file`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import textwrap\n",
    "from fpdf import FPDF\n",
    "\n",
    "def translate_pdf(input_file, translator_model,output_file):\n",
    "    #Completa la función"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí tienes un documento en alemán para que lo conviertas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/input_de.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora llama a translate_pdf para el archivo alemán como entrada a la función y verifica el archivo de salida para el archivo traducido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path =\n",
    "output_file = 'output_en.pdf'\n",
    "# Llama a translate_pdf() definido anteriormente para el archivo de entrada\n",
    "print(\"El archivo PDF traducido se guarda como:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "prev_pub_hash": "f583ab330d392f3fbc803e1d84830f575a94e0d7cc0f8b3af49ded45fd51cc14"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
