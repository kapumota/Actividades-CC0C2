{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e35a97d0",
   "metadata": {},
   "source": [
    "## Examen Parcial CC0C2\n",
    "\n",
    "**Nombre y Apellidos:**\n",
    "\n",
    "**Código:**\n",
    "\n",
    "### Reglas para el Examen Parcial\n",
    "\n",
    "- Queda terminantemente prohibido el uso de herramientas como ChatGPT, WhatsApp, o cualquier herramienta similar durante la realización de esta prueba. El uso de estas herramientas, por cualquier motivo, resultará en la anulación inmediata de la evaluación. Puedes utilizar los cuadernos y datos alojados en github.\n",
    "\n",
    "- Las respuestas deben presentarse con una explicación detallada, utilizando términos técnicos apropiados. La mera descripción sin el uso de terminología técnica, especialmente términos discutidos en clase, se considerará insuficiente y podrá resultar en que la respuesta sea marcada como incorrecta. \n",
    "\n",
    "- Cada estudiante debe presentar su propio trabajo. Los códigos iguales o muy parecidos entre sí serán considerados como una violación a la integridad académica, implicando una copia, y serán sancionados de acuerdo con las políticas de la universidad. \n",
    "\n",
    "- Todos los estudiantes deben subir sus repositorios de código a la plataforma del curso, según las instrucciones proporcionadas. La fecha y hora de la última actualización del repositorio serán consideradas como la hora de entrega. \n",
    "\n",
    "- La claridad, orden, y presentación general de las evaluaciones serán tomadas en cuenta en la calificación final. Se espera un nivel de profesionalismo en la documentación y presentación del código y las respuestas escritas. \n",
    "\n",
    "\n",
    "#### Instrucciones de entrega para la prueba calificada \n",
    "\n",
    "- Presenta la dirección de tu repositorio personal donde se encuentre este cuaderno con tus respuestas desarrolladas.\n",
    "- Todo cambio fuera de la hora y fecha del examen realizado dentro del repositorio no se tomará en cuenta y se procederá a anular la evaluación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a55574d-3519-42cb-bd20-357ab7b6abcd",
   "metadata": {},
   "source": [
    "### Problema 1\n",
    "\n",
    "El subsampling en el contexto de los modelos de Word2Vec es una técnica utilizada para reducir el número de veces que se entrenan palabras muy frecuentes. Se basa en la idea de que las palabras extremadamente comunes (como preposiciones y conjunciones) proporcionan menos información de contexto valiosa en comparación con las palabras menos frecuentes. En la práctica, cada palabra en el conjunto de entrenamiento tiene una probabilidad calculada de ser \"saltada\" durante el entrenamiento, dependiendo de su frecuencia. Esto ayuda a acelerar el entrenamiento y a mejorar la calidad de las representaciones de palabras menos frecuentes, que podrían verse oscurecidas por palabras de alta frecuencia.\n",
    "\n",
    "El negative sampling es una técnica de optimización para reducir la complejidad computacional de actualizar los pesos en la red neuronal en modelos como Word2Vec. En lugar de actualizar los pesos de todas las palabras del vocabulario para cada ejemplo de entrenamiento (lo cual es muy costoso computacionalmente), el negative sampling actualiza solo un pequeño número de \"palabras negativas\" (ejemplos negativos seleccionados aleatoriamente) junto con la palabra objetivo (ejemplo positivo). Esto no solo acelera significativamente el entrenamiento sino que también mejora la calidad de las representaciones vectoriales al enfocarse en distinguir la palabra objetivo de un pequeño subconjunto de palabras negativas.\n",
    "\n",
    "La correlación de Spearman es una medida estadística que evalúa la fuerza y la dirección de la asociación entre dos variables clasificadas. A diferencia de la correlación de Pearson, que requiere que las variables sean de escala intervalo o de razón y aproximadamente normales, la correlación de Spearman no hace suposiciones sobre la distribución de los datos y se basa en rangos. Es especialmente útil en el contexto de Word2Vec cuando se evalúa cómo las similitudes coseno calculadas entre vectores de palabras se comparan con juicios humanos de similitud (usualmente dados en estudios donde las personas califican qué tan similares son las palabras). Al correlacionar estos dos conjuntos de rankings (el calculado y el humano), se puede obtener una medida de cuán bien el modelo captura relaciones semánticas que coinciden con las percepciones humanas.\n",
    "\n",
    "#### Ejercicios:\n",
    "\n",
    "1. Implementa los modelos CBOW y Skip-gram en Python sin utilizar bibliotecas de alto nivel como Gensim (2 puntos).\n",
    "    - Escribe el código para inicializar los pesos de la red, realizar el entrenamiento mediante descenso de gradiente y calcular la función de pérdida.\n",
    "    - Añade mecanismos de subsampling y negative sampling para mejorar la eficiencia del entrenamiento. \n",
    "2. Analiza cómo diferentes hiperparámetros afectan la calidad de los embeddings vectoriales (2 puntos).\n",
    "    - Entrena modelos Word2Vec con diferentes tamaños de ventana, dimensiones de vector y tasas de aprendizaje. Utiliza un conjunto de datos estándar como el corpus de texto de Wikipedia.\n",
    "    - Evalúa los modelos usando tareas de analogía de palabras y calcula la correlación de Spearman entre las similitudes humanas y las  calculadas por el modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21a4488-65b4-4c7b-8f76-d072a5cc976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b81d8e-c946-40df-a842-581816d28be9",
   "metadata": {},
   "source": [
    "### Pregunta 2\n",
    "\n",
    "La factorización de matrices GloVe y PPMI son dos métodos utilizados en el procesamiento del lenguaje natural (NLP) para capturar relaciones semánticas entre palabras a partir de grandes corpus de texto. Ambos métodos se utilizan para generar representaciones vectoriales de palabras, lo que permite que las relaciones semánticas y sintácticas entre palabras se reflejen en el espacio vectorial. \n",
    "\n",
    "1 . GloVe (Global Vectors for Word Representation)\n",
    "GloVe es un modelo de aprendizaje no supervisado para obtener representaciones vectoriales de palabras. Fue desarrollado por investigadores de Stanford y combina elementos de dos enfoques principales en NLP: factorización de matrices y modelos basados en ventana de contexto (como word2vec). La idea principal detrás de GloVe es que las co-ocurrencias de palabras en un corpus pueden proporcionar información semántica valiosa.\n",
    "\n",
    "El modelo GloVe construye una matriz de co-ocurrencia global que tabula cuántas veces cada palabra aparece en el contexto de otras palabras dentro de un corpus. Luego, esta matriz se factoriza para reducir su dimensión, resultando en vectores de palabras más densos. El objetivo de la factorización es mantener la estructura semántica donde la distancia entre dos vectores de palabras refleje la similitud semántica entre las palabras correspondientes.\n",
    "\n",
    "2 . PPMI (Positive Pointwise Mutual Information)\n",
    "La PPMI es una técnica que se usa para calcular la asociación entre palabras basada en cuán frecuentemente aparecen juntas en comparación con cuán frecuentemente aparecen por separado. El \"Pointwise Mutual Information\" (PMI) de dos palabras mide la probabilidad de co-ocurrencia de las palabras en relación con las probabilidades de que cada palabra ocurra por sí sola. Sin embargo, PMI puede tener valores negativos, lo que puede ser problemático en algunos escenarios de modelado.\n",
    "\n",
    "Para solucionarlo, se utiliza PPMI, donde todos los valores negativos de PMI se reemplazan por cero, enfocándose solo en las asociaciones positivas. En NLP, la PPMI a menudo se usa como una técnica de pre-procesamiento para construir matrices de características que luego pueden ser factorizadas (similar a SVD en GloVe) para obtener representaciones vectoriales de palabras.\n",
    "\n",
    "Para implementar PPMI, primero construiremos una matriz de co-ocurrencia y luego convertiremos sus valores a PPMI. Usaremos numpy para las operaciones matemáticas y collections para construir la matriz de co-ocurrencia.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0261ec-a5a5-478a-acf4-d70a216e6479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import product\n",
    "\n",
    "# Función para construir la matriz de co-ocurrencia\n",
    "def co_occurrence_matrix(corpus, window_size=2):\n",
    "    vocab = set(corpus)\n",
    "    vocab = {word: i for i, word in enumerate(vocab)}\n",
    "    co_occurrences = defaultdict(Counter)\n",
    "\n",
    "    for i in range(len(corpus)):\n",
    "        token = corpus[i]\n",
    "        left = max(0, i-window_size)\n",
    "        right = min(len(corpus), i+window_size+1)\n",
    "\n",
    "        for j in range(left, right):\n",
    "            if i != j:\n",
    "                co_occurrences[token][corpus[j]] += 1\n",
    "\n",
    "    matrix = np.zeros((len(vocab), len(vocab)))\n",
    "\n",
    "    for token1, neighbors in co_occurrences.items():\n",
    "        for token2, count in neighbors.items():\n",
    "            matrix[vocab[token1], vocab[token2]] = count\n",
    "\n",
    "    return matrix, vocab\n",
    "\n",
    "# Función para calcular PPMI\n",
    "def ppmi_matrix(co_matrix, eps=1e-8):\n",
    "    total_sum = np.sum(co_matrix)\n",
    "    row_sums = np.sum(co_matrix, axis=1)\n",
    "    col_sums = np.sum(co_matrix, axis=0)\n",
    "\n",
    "    ppmi = np.maximum(\n",
    "        np.log((co_matrix * total_sum) / (row_sums[:, None] * col_sums[None, :] + eps)),\n",
    "        0\n",
    "    )\n",
    "    return ppmi\n",
    "\n",
    "# Ejemplo de uso\n",
    "corpus = \"the quick brown fox jumps over the lazy dog\".split()\n",
    "co_matrix, vocab = co_occurrence_matrix(corpus, window_size=2)\n",
    "ppmi = ppmi_matrix(co_matrix)\n",
    "\n",
    "print(ppmi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a29399-8799-4b5b-b251-175f1a5d5749",
   "metadata": {},
   "source": [
    "Implementar GloVe desde cero es más complejo debido a la optimización necesaria para ajustar los vectores de palabras. Sin embargo, puedes usar la biblioteca gensim, que tiene una implementación eficiente de GloVe. Utiliza el código realizado en clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e09653-32b6-4a06-b7fc-c48e011c5890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Crear modelo Word2Vec con los mismos parámetros que GloVe\n",
    "modelo = Word2Vec(sentences=[corpus], vector_size=100, window=5, min_count=1, sg=0, workers=4, epochs=10)\n",
    "\n",
    "# Guardar y cargar el modelo (simulando una carga de GloVe)\n",
    "model.wv.save_word2vec_format('model.bin')\n",
    "glove_model = KeyedVectors.load_word2vec_format('model.bin', binary=True)\n",
    "\n",
    "# Usar el modelo\n",
    "print(glove_model['fox'])  # Muestra el vector para la palabra \"fox\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee3b58c-fb61-4251-97df-74050d05c742",
   "metadata": {},
   "source": [
    "#### Ejercicios\n",
    "\n",
    "1. Modifica el tamaño de la ventana de contexto en la función co_occurrence_matrix para diferentes valores (por ejemplo, 1, 3, y 5) y observa cómo cambia la matriz PPMI resultante. Analiza cómo el tamaño de la ventana afecta las relaciones semánticas capturadas (1 punto).\n",
    "2. Implementa una función que identifique y muestre las palabras con mayor asociación (mayores valores PPMI) para una palabra dada. Utiliza esta función para explorar las relaciones semánticas de varias palabras clave en un corpus más grande (1 punto).\n",
    "3. Usa la biblioteca gensim para entrenar un modelo GloVe con un corpus más grande (por ejemplo, un conjunto de datos de reseñas de productos o artículos de noticias). Ajusta diferentes hiperparámetros como el tamaño del vector, el tamaño de la ventana, y el número de iteraciones. Evalúa los vectores de palabras resultantes en tareas de analogía y similaridad (1 punto).\n",
    "4. Realiza una comparación cualitativa y cuantitativa de las representaciones de palabras obtenidas a través de PPMI y GloVe. Considera aspectos como la capacidad de capturar sinónimos, antónimos y relaciones semánticas complejas. Discute en qué casos un método podría ser preferido sobre el otro (1 punto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f07e76-d8e0-44ff-bb9b-7bb860785832",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dbda1f-635d-4517-9739-c3f448c99b21",
   "metadata": {},
   "source": [
    "### Pregunta 3\n",
    "\n",
    "El desarrollo de modelos de redes neuronales recurrentes (RNNs) ha sido fundamental en el avance del procesamiento de secuencias de tiempo y lenguaje natural. Estos modelos son especialmente útiles en tareas como el reconocimiento de voz, la traducción automática y la generación de texto. Sin embargo, las RNNs básicas enfrentan desafíos significativos, como la desaparición y la explosión del gradiente, que obstaculizan su capacidad para aprender dependencias a largo plazo en los datos. Las unidades de memoria de largo y corto plazo (LSTM) y las unidades recurrentes con compuertas (GRU) se desarrollaron como soluciones a estos problemas, mejorando la capacidad de las redes para aprender de datos secuenciales a largo plazo.\n",
    "\n",
    "Una RNN básica procesa información secuencial mediante la actualización de su estado oculto con cada nuevo elemento de la secuencia. La naturaleza recurrente de estas redes les permite mantener una forma de 'memoria' sobre los elementos anteriores de la secuencia, utilizando la siguiente fórmula básica para actualizar el estado oculto en cada paso de tiempo $t$:\n",
    "\n",
    "$$\n",
    "h_t = \\sigma(W_{ih} x_t + W_{hh} h_{t-1} + b_h)\n",
    "$$\n",
    "\n",
    "Donde $x_t$ es la entrada en el tiempo $t$, $h_t$ es el estado oculto en el tiempo $t$, $W_{ih}$ y $W_{hh}$ son los pesos de entrada y recurrentes, respectivamente, $b_h$ es el término de sesgo, y $\\sigma$ es una función de activación no lineal como tanh o ReLU.\n",
    "\n",
    "\n",
    "El entrenamiento de RNNs implica ajustar estos pesos mediante retropropagación a través del tiempo, lo que puede llevar a dos problemas principales:\n",
    "\n",
    "1. **Desaparición del gradiente:** Si los gradientes de los pesos son muy pequeños, disminuyen exponencialmente a medida que se propagan hacia atrás a través de cada paso de tiempo. Esto hace que sea difícil para la RNN aprender dependencias a largo plazo, ya que los gradientes se vuelven insignificantes para ajustar los pesos efectivamente en pasos de tiempo anteriores.\n",
    "\n",
    "2. **Explosión del gradiente:** En contraste, si los gradientes son demasiado grandes, pueden crecer exponencialmente durante la retropropagación, lo que lleva a actualizaciones de peso grandes e inestables, y por ende, a un modelo que diverge y no aprende de manera efectiva.\n",
    "\n",
    "#### Unidad de memoria de largo y corto plazo (LSTM)\n",
    "\n",
    "Para abordar estos problemas, se introdujeron las LSTMs, que incorporan un diseño más complejo que permite controlar el flujo de información. Las LSTMs utilizan varias \"puertas\" para regular tanto el almacenamiento como la eliminación de información en el estado de la celda:\n",
    "\n",
    "- **Puerta de olvido $(f_t)$** decide qué parte de la información anterior se mantiene:\n",
    "  $$\n",
    "  f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "  $$\n",
    "\n",
    "- **Puerta de entrada ($i_t$) y candidato de celda ($\\tilde{c}_t$)** deciden qué nueva información se añade al estado de la celda:\n",
    "\n",
    "  $$\n",
    "  i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
    "  $$\n",
    "  $$\n",
    "  \\tilde{c}_t = \\tanh(W_c \\cdot [h_{t-1}, x_t] + b_c)\n",
    "  $$\n",
    "\n",
    "- **Actualización del estado de la celda ($c_t$)** combina la información antigua y nueva:\n",
    "  $$\n",
    "  c_t = f_t \\ast c_{t-1} + i_t \\ast \\tilde{c}_t\n",
    "  $$\n",
    "\n",
    "- **Puerta de salida ($o_t$)** y el estado oculto resultante ($h_t$) que determina qué parte del estado de la celda afectará la salida:\n",
    "  $$\n",
    "  o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
    "  $$\n",
    "  $$\n",
    "  h_t = o_t \\ast \\tanh(c_t)\n",
    "  $$\n",
    "\n",
    "#### Unidad recurrente compuerta (GRU)\n",
    "\n",
    "Las GRUs simplifican la arquitectura de las LSTMs combinando las puertas de entrada y olvido en una sola puerta de actualización y omitiendo el uso de un estado de celda separado:\n",
    "\n",
    "- **Puerta de actualización ($z_t$)** decide cuánto del estado anterior se debe mantener:\n",
    "  $$\n",
    "  z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z)\n",
    "  $$\n",
    "\n",
    "- **Puerta de reinicio ($r_t$)** decide cuánto del pasado se debe olvidar antes de calcular el nuevo candidato de estado:\n",
    "  $$\n",
    "  r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r)\n",
    "  $$\n",
    "\n",
    "- **Candidato de estado oculto ($\\tilde{h}_t$)** y la actualización del estado oculto:\n",
    "  $$\n",
    "  \\tilde{h}_t = \\tanh(W_h \\cdot [r_t \\ast h_{t-1}, x_t] + b_h)\n",
    "  $$\n",
    "  \n",
    "  $$\n",
    "  h_t = (1 - z_t) \\ast h_{t-1} + z_t \\ast \\tilde{h}_t\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead46557-86c0-4b63-a6cd-78fafa796d30",
   "metadata": {},
   "source": [
    "#### Ejercicios\n",
    "\n",
    "1. ¿Qué papel juegan los reguladores como dropout o L2 regularization específicamente en el contexto de RNNs y LSTM para evitar el sobreajuste en tareas de modelado de lenguaje? (1 punto)\n",
    "2. Considerando la complejidad computacional de BPTT, ¿cuáles son las limitaciones prácticas cuando se usa con RNNs en secuencias muy largas? ¿Cómo podrías mitigar estos problemas en un entorno de producción? (1 punto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c0b968-53e3-4fd5-bff8-0618b4d8ef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parte 3\n",
    "## Parte 3\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_to_hidden = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = torch.tanh(self.input_to_hidden(combined))\n",
    "        return hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # Gates\n",
    "        self.input_to_inputgate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.input_to_forgetgate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.input_to_outputgate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.input_to_cellgate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "\n",
    "        # Calcular puertas\n",
    "        input_gate = torch.sigmoid(self.input_to_inputgate(combined))\n",
    "        forget_gate = torch.sigmoid(self.input_to_forgetgate(combined))\n",
    "        output_gate = torch.sigmoid(self.input_to_outputgate(combined))\n",
    "        cell_gate = torch.tanh(self.input_to_cellgate(combined))\n",
    "\n",
    "        # actualizacion del estado\n",
    "        cell = forget_gate * cell + input_gate * cell_gate\n",
    "        hidden = output_gate * torch.tanh(cell)\n",
    "\n",
    "        return hidden, cell\n",
    "\n",
    "    def init_hidden_and_cell(self):\n",
    "        return torch.zeros(1, self.hidden_size), torch.zeros(1, self.hidden_size)\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_to_updategate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.input_to_resetgate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.input_to_newgate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "\n",
    "        # Calcular puertas\n",
    "        update_gate = torch.sigmoid(self.input_to_updategate(combined))\n",
    "        reset_gate = torch.sigmoid(self.input_to_resetgate(combined))\n",
    "        new_hidden = torch.tanh(self.input_to_newgate(torch.cat((input, reset_gate * hidden), 1)))\n",
    "\n",
    "        # actualizar estado oculto\n",
    "        hidden = update_gate * hidden + (1 - update_gate) * new_hidden\n",
    "\n",
    "        return hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2a9703-79cb-4435-b9bc-239985f6c3f2",
   "metadata": {},
   "source": [
    "Extiende la implementación de LSTM para incluir embeddings de palabras y una capa de clasificación, y entrenar el modelo en una tarea de predicción de la siguiente palabra en secuencias de texto (3 puntos).\n",
    "\n",
    "- Agrega una capa de embedding al modelo LSTM para procesar entradas de texto.\n",
    "- Incluye una capa de salida que mapee el estado oculto a las predicciones de palabras.\n",
    "- Implementa una función de pérdida adecuada para la clasificación de palabras.\n",
    "- Preprocesa  un corpus de texto grande (utiliza los datos dados en clase por ejemplo) para convertir texto a índices utilizando un vocabulario predefinido.\n",
    "- Genera datos de entrenamiento como pares de secuencias de entrada y palabras objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da3c614-442c-44d5-92bf-4b61280d9c0e",
   "metadata": {},
   "source": [
    "Realiza un análisis de sensibilidad de los hiperparámetros en modelos LSTM y GRU para entender su impacto en la capacidad de aprendizaje de dependencias a largo plazo en textos (3 puntos)\n",
    "\n",
    "- Selecciona un corpus de texto y prepara datos para el entrenamiento de modelos de lenguaje basados en LSTM y GRU.\n",
    "- Experimenta con diferentes valores para los hiperparámetros como el tamaño de las puertas, la tasa de aprendizaje, el tamaño del estado oculto y la longitud de BPTT.\n",
    "- Utiliza técnicas como validación cruzada para evaluar el impacto de estos cambios en la precisión del modelo y en su capacidad para generar texto coherente.\n",
    "- Analiza cómo la modificación de los parámetros de las puertas y la longitud de BPTT afecta la estabilidad del entrenamiento y la convergencia del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ca146e-2886-4112-b00a-f3ae28d46139",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcf1093-7292-469a-b041-3c7120f4ee45",
   "metadata": {},
   "source": [
    "### Pregunta 4\n",
    "El script proporcionado es un ejemplo completo de cómo implementar un modelo de red neuronal recurrente (RNN) utilizando PyTorch para generar texto de manera automática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01cc564-c049-4af8-8153-26e0f5816d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías necesarias para trabajar con tensores y redes neuronales.\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "# Datos de entrada: una lista de frases.\n",
    "text = ['hey how are you','good i am fine','have a nice day']\n",
    "\n",
    "# Creación de un conjunto de caracteres únicos presentes en las frases.\n",
    "chars = set(''.join(text))\n",
    "# Creación de un diccionario que mapea cada caracter a un índice único.\n",
    "int2char = dict(enumerate(chars))\n",
    "# Creación de un diccionario inverso que mapea cada índice a su caracter correspondiente.\n",
    "char2int = {char: ind for ind, char in int2char.items()}\n",
    "\n",
    "# Determinación de la longitud máxima de las frases para normalizar la longitud de todas.\n",
    "maxlen = len(max(text, key=len))\n",
    "print(\"La longitud mayor tiene {} caracteres\".format(maxlen))\n",
    "\n",
    "# Añadir espacios a las frases más cortas para igualar la longitud máxima.\n",
    "for i in range(len(text)):\n",
    "  while len(text[i])<maxlen:\n",
    "    text[i] += ' '\n",
    "\n",
    "# Inicialización de listas para secuencias de entrada y objetivo.\n",
    "input_seq = []\n",
    "target_seq = []\n",
    "\n",
    "# Creación de secuencias de entrada y objetivo.\n",
    "for i in range(len(text)):\n",
    "    input_seq.append(text[i][:-1])\n",
    "    target_seq.append(text[i][1:])\n",
    "    print(\"Secuencia entrada: {}\\nSecuencia objetivo: {}\".format(input_seq[i], target_seq[i]))\n",
    "\n",
    "# Conversión de caracteres a índices para procesamiento numérico.\n",
    "for i in range(len(text)):\n",
    "    input_seq[i] = [char2int[character] for character in input_seq[i]]\n",
    "    target_seq[i] = [char2int[character] for character in target_seq[i]]\n",
    "\n",
    "# Definición de tamaños para la codificación one-hot.\n",
    "dict_size = len(char2int)\n",
    "seq_len = maxlen - 1\n",
    "batch_size = len(text)\n",
    "\n",
    "# Función para codificar las secuencias en formato one-hot.\n",
    "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
    "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
    "    for i in range(batch_size):\n",
    "        for u in range(seq_len):\n",
    "          features[i, u, sequence[i][u]] = 1\n",
    "    return features\n",
    "\n",
    "# Aplicación de la codificación one-hot a las secuencias de entrada.\n",
    "input_seq = one_hot_encode(input_seq, dict_size, seq_len, batch_size)\n",
    "print(\"Forma de entrada: {} --> (Batch Size, Sequence Length, One-Hot Encoding Size)\".format(input_seq.shape))\n",
    "\n",
    "# Conversión de las secuencias de entrada a tensores de PyTorch.\n",
    "input_seq = torch.from_numpy(input_seq)\n",
    "target_seq = torch.Tensor(target_seq)\n",
    "\n",
    "# Chequeo de disponibilidad de GPU y selección del dispositivo (GPU o CPU).\n",
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU es disponible\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU no disponible, CPU es usada\")\n",
    "\n",
    "# Definición de la clase del modelo RNN.\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Capa RNN que toma entradas y retorna la salida y un estado oculto.\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        # Capa lineal que procesa la salida del RNN.\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Inicialización del estado oculto a cero.\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "        return hidden\n",
    "\n",
    "# Instancia del modelo con parámetros específicos.\n",
    "model = Model(input_size=dict_size, output_size=dict_size, hidden_dim=12, n_layers=1)\n",
    "model.to(device)\n",
    "\n",
    "# Definición de hiperparámetros para el entrenamiento.\n",
    "n_epochs = 100\n",
    "lr=0.01\n",
    "\n",
    "# Configuración de la función de pérdida y el optimizador.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Bucle de entrenamiento del modelo.\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  optimizer.zero_grad()\n",
    "  input_seq = input_seq.to(device)\n",
    "  output, hidden = model(input_seq)\n",
    "  loss = criterion(output, target_seq.view(-1).long())\n",
    "  loss.backward() # Realización de backpropagation y cálculo de gradientes.\n",
    "  optimizer.step() # Actualización de los pesos del modelo.\n",
    "  \n",
    "  if epoch%10 == 0:\n",
    "    print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "    print(\"Loss: {:.4f}\".format(loss.item()))\n",
    "\n",
    "# Funciones para predicción y generación de texto basadas en el modelo entrenado.\n",
    "def predict(model, character):\n",
    "  character = np.array([[char2int[c] for c in character]])\n",
    "  character = one_hot_encode(character, dict_size, character.shape[1], 1)\n",
    "  character = torch.from_numpy(character)\n",
    "  character.to(device)\n",
    "    \n",
    "  out, hidden = model(character)\n",
    "\n",
    "  prob = nn.functional.softmax(out[-1], dim=0).data\n",
    "  char_ind = torch.max(prob, dim=0)[1].item()\n",
    "\n",
    "  return int2char[char_ind], hidden\n",
    "\n",
    "def sample(model, out_len, start='hey'):\n",
    "  model.eval() \n",
    "  start = start.lower()\n",
    "  chars = [ch for ch in start]\n",
    "  size = out_len - len(chars)\n",
    "  for ii in range(size):\n",
    "    char, h = predict(model, chars)\n",
    "    chars.append(char)\n",
    "\n",
    "  return ''.join(chars)\n",
    "\n",
    "# Ejemplo de uso de la función de generación de texto.\n",
    "sample(model, 15, 'good')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c017567c-c7c0-4d27-81f1-1abeee08518f",
   "metadata": {},
   "source": [
    "#### Ejercicios: \n",
    "\n",
    "1. Modifica el modelo existente para que funcione como un autoencoder. Esto implica que el modelo debe aprender a codificar una secuencia de entrada en un vector de características (estado oculto) y luego decodificar ese vector de vuelta a la secuencia original (1.5 puntos).\n",
    "     - Implementa las capas de codificación y decodificación dentro del mismo modelo.\n",
    "     - Experimenta  con diferentes estructuras como LSTM para mejorar la retención de información.\n",
    "     - Mide la calidad de la reconstrucción del texto y la eficiencia de compresión.\n",
    "\n",
    "2. Utiliza el modelo RNN actual y modifícalo para introducir secuencias más largas. Monitoriza los gradientes durante el entrenamiento para detectar signos de desaparición o explosión. (1.5 puntos)\n",
    "    - Implementa el  clipping de gradiente para prevenir la explosión del gradiente.\n",
    "    - Reemplaza la RNN por LSTM para abordar la desaparición del gradiente.\n",
    "    - Utiliza técnicas de visualización para observar la magnitud de los gradientes a lo largo de varias épocas.\n",
    "3. Implementa el dropout en las capas recurrentes y comparar los resultados. (1 punto)\n",
    "\n",
    "    - Ajusta el parámetro de weight decay en el optimizador y observar el efecto sobre el overfitting.\n",
    "    - Aplica early stopping basado en la validación del loss para detener el entrenamiento antes de que el modelo comience a sobreajustarse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff7c8ab-52f4-4a1f-a66d-7f3f4c32e486",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
