{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Carga de datos y procesamiento de texto para BERT**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT (Bidirectional Encoder Representations from Transformers) ha revolucionado las tareas de procesamiento de lenguaje natural al capturar información contextual tanto del contexto izquierdo como del derecho. \n",
    "\n",
    "Para aprovechar el poder de BERT, incluiremos diversos temas, incluidos la selección aleatoria de muestras, la tokenización, la construcción de vocabulario, el enmascaramiento de texto y la preparación de datos para las tareas de modelo de lenguaje enmascarado (MLM) y predicción de la siguiente oración (NSP). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Configuración**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install 'portalocker>=2.0.0'\n",
    "#! pip install 'torchtext==0.16.0'\n",
    "#! pip install transformers==4.39.1\n",
    "#! pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Importando librerías requeridas**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección importarás las librerías y módulos necesarios para preparar tu conjunto de datos para el entrenamiento con PyTorch. El enfoque está en el procesamiento de texto y en la creación de data loaders que se emplearán para entrenar tus modelos.\n",
    "\n",
    "* **DataLoader**: una utilidad de PyTorch que te permite cargar datos en lotes, facilitando la gestión de grandes volúmenes de información durante el entrenamiento.\n",
    "* **build\\_vocab\\_from\\_iterator**: función de `torchtext.vocab` que crea un objeto de vocabulario a partir de un iterador. Este vocabulario es clave para el procesamiento de texto, ya que mapea tokens (palabras) a números enteros.\n",
    "* **Vocab**: representa el vocabulario, es decir, la asignación de tokens a índices. Se utiliza para convertir datos de texto en una forma numérica que el modelo puede entender.\n",
    "* **Tensor, torch, nn, Transformer**: módulos y clases centrales de PyTorch para trabajar con tensores (la estructura de datos fundamental en PyTorch), capas de redes neuronales y la arquitectura de modelo Transformer.\n",
    "* **get\\_tokenizer**: función de `torchtext.data.utils` que devuelve un tokenizador para convertir cadenas de texto en listas de tokens.\n",
    "* **pad\\_sequence**: utilidad de `torch.nn.utils.rnn` que rellena secuencias para que todas tengan la misma longitud, un requisito común al procesar lotes de datos con secuencias de longitud variable.\n",
    "\n",
    "Esta configuración es esencial para procesar datos de texto, convertirlos a formato numérico y preparar lotes de datos para entrenar redes neuronales, especialmente en tareas como modelado de secuencias y clasificación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.vocab import Vocab\n",
    "from torch import Tensor\n",
    "from torch.nn import Transformer\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from itertools import chain\n",
    "from itertools import islice\n",
    "from torchtext.datasets import IMDB\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# También puedes usar esta sección para suprimir las advertencias generadas por tu código:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Tokenización y construcción del vocabulario**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Tokenización**\n",
    "\n",
    "* El `tokenizer` se inicializa para tokenizar texto usando reglas básicas de tokenización en inglés, convirtiendo muestras de texto en listas de tokens.\n",
    "\n",
    "* `yield_tokens` es una función generadora que itera a través de los datos, devolviendo versiones tokenizadas de las muestras de texto. Esta función facilita la construcción del vocabulario al proporcionar un flujo continuo de tokens.\n",
    "\n",
    "* `word_dict` define los tokens especiales utilizados en el procesamiento de texto, como padding `[PAD]`, inicio de clase `[CLS]`, separador `[SEP]`, máscara `[MASK]` y desconocido `[UNK]`, con sus índices correspondientes.\n",
    "\n",
    "* Los símbolos especiales y sus índices están definidos explícitamente para mayor claridad y se utilizan en toda la preparación de los datos.\n",
    "\n",
    "* Las funciones `text_to_index` y `index_to_en` son convertidores auxiliares. La primera convierte texto en una lista de índices numéricos basados en el vocabulario, y la segunda invierte este proceso, traduciendo una secuencia de índices de vuelta a texto legible en inglés.\n",
    "\n",
    "* **`CLS` (token de clasificación)**: Este token sirve como marcador de *inicio de secuencia (SOS)*. Representa el significado global de toda la oración. Se emplea comúnmente en tareas que requieren comprender la entrada completa, como la clasificación.\n",
    "\n",
    "* **`SEP` (token separador)**: Utilizado como marcador de *fin de secuencia (EOS)**. También actúa como delimitador en escenarios donde el modelo necesita entender y diferenciar varias oraciones, por ejemplo en tareas de pregunta-respuesta o pares de oraciones.\n",
    "\n",
    "* **`PAD` (Token de relleno)**: Este token se añade a las secuencias para garantizar que todas las entradas tengan la misma longitud. Durante el entrenamiento, es importante notar que el token `[PAD]`, típicamente con ID 0, no contribuye a los cálculos de gradiente.\n",
    "\n",
    "* **`MASK` (Token enmascarado)**: Utilizado para reemplazo de palabras en tareas como el modelado de lenguaje enmascarado. Permite a los modelos predecir la identidad de las palabras enmascaradas, facilitando el aprendizaje de representaciones bidireccionales.\n",
    "\n",
    "* **`UNK` (Token desconocido)**: Actúa como marcador para palabras que no se encuentran en el vocabulario del tokenizador. Este token reemplaza cualquier elemento desconocido o fuera de vocabulario en los datos de entrada.\n",
    "\n",
    "Estos componentes son fundamentales para el preprocesamiento de datos de texto, asegurando que estén en el formato correcto para el entrenamiento de modelos, incluyendo la tokenización, la conversión numérica y el manejo de tokens especiales necesarios para modelos como BERT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for label, data_sample in data_iter:\n",
    "        yield tokenizer(data_sample)\n",
    "\n",
    "# Define símbolos especiales e índices\n",
    "PAD_IDX, CLS_IDX, SEP_IDX, MASK_IDX, UNK_IDX = 0, 1, 2, 3, 4\n",
    "\n",
    "# Asegurate de que los tokens estén en el orden de sus índices para insertarlos correctamente en el vocabulario\n",
    "special_symbols = ['[PAD]', '[CLS]', '[SEP]', '[MASK]', '[UNK]']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Construcción de vocabulario**\n",
    "\n",
    "Esta sección se centra en construir el vocabulario a partir del conjunto de datos IMDB.\n",
    "\n",
    "* Puedes utilizar el conjunto de datos `IMDB` de `torchtext.datasets`, dividiéndolo en conjuntos de entrenamiento y de prueba.\n",
    "* El vocabulario se crea con la función `build_vocab_from_iterator`, incorporando símbolos especiales (`[PAD]`, `[CLS]`, `[SEP]`, `[MASK]`, `[UNK]`) al principio.\n",
    "* Se asigna `UNK_IDX` como índice predeterminado para las palabras desconocidas y se muestra por pantalla el tamaño total del vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crea divisiones de datos\n",
    "train_iter, test_iter = IMDB(split=('train', 'test'))\n",
    "all_data_iter = chain(train_iter, test_iter)\n",
    "# comprueba el tokenizador\n",
    "# lista(yield_tokens(all_data_iter))[5][:20]\n",
    "fifth_item_tokens = next(islice(yield_tokens(all_data_iter), 5, None))\n",
    "print(fifth_item_tokens[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crea vocabulario: el vocabulario se construye únicamente usando datos de entrenamiento\n",
    "vocab = build_vocab_from_iterator(yield_tokens(all_data_iter), specials=special_symbols, special_first=True)\n",
    "\n",
    "vocab.set_default_index(UNK_IDX)\n",
    "VOCAB_SIZE = len(vocab)\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, crea funciones que transformen los índices de los tokens en sus textos y viceversa. Las usarás más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_index=lambda text: [vocab(token) for token in tokenizer(text)]\n",
    "index_to_en = lambda seq_en: \" \".join([vocab.get_itos()[index] for index in seq_en])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a comprobar los mapeos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_en = [0, 1, 2, 3, 4, 5, 6]  # Secuencia de entrada de ejemplo\n",
    "english_sentence = index_to_en(seq_en)\n",
    "seq2 = [6, 16, 26131]\n",
    "english_sentence = index_to_en(seq2)\n",
    "\n",
    "print(english_sentence)\n",
    "\n",
    "text = \"I've seen R-rated films with male nudity. Nowhere, because they don't exist.\"  # Texto de entrada de ejemplo\n",
    "text_to_index = lambda text: [vocab[token] for token in tokenizer(text)]\n",
    "index_sequence = text_to_index(text)\n",
    "\n",
    "print(index_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Enmascaramiento de texto y preparación de datos para BERT**\n",
    "\n",
    "Esta sección presenta funciones para preparar los datos para las tareas de **modelado de lenguaje enmascarado (MLM)** y **predicción de la siguiente oración (NSP)** de BERT, pasos cruciales para ajustar finamente BERT en tareas específicas de NLP.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Enmascaramiento de texto**\n",
    "\n",
    "La función `Masking` aplica la estrategia MLM de BERT, decidiendo si cada token en una secuencia debe ser enmascarado, dejado sin cambios o reemplazado por un token aleatorio. Este proceso es esencial para entrenar al modelo a predecir palabras enmascaradas basándose en su contexto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, define una función que devuelva aleatoriamente `0` o `1` siguiendo una distribución de Bernoulli para el muestreo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulli_true_false(p):\n",
    "    # Crea una distribución de Bernoulli con probabilidad p\n",
    "    bernoulli_dist = torch.distributions.Bernoulli(torch.tensor([p]))\n",
    "    # Muestra de esta distribución y convierte 1 en True y 0 en False\n",
    "    return bernoulli_dist.sample().item() == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, define la función de enmascaramiento:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Masking(token):\n",
    "    # Decide si enmascarar este token (20% de probabilidad)\n",
    "    mask = bernoulli_true_false(0.2)\n",
    "\n",
    "    # Si mask es False, retornar inmediatamente con etiqueta '[PAD]'\n",
    "    if not mask:\n",
    "        return token, '[PAD]'\n",
    "\n",
    "    # Si mask es True, continuar con las siguientes operaciones\n",
    "    # Decide aleatoriamente la operación (50% de probabilidad cada una)\n",
    "    random_opp = bernoulli_true_false(0.5)\n",
    "    random_swich = bernoulli_true_false(0.5)\n",
    "\n",
    "    # Caso 1: si mask, random_opp y random_swich son True\n",
    "    if mask and random_opp and random_swich:\n",
    "        # Reemplaza el token con '[MASK]' y asignar una etiqueta de token aleatorio\n",
    "        mask_label = index_to_en(torch.randint(0, VOCAB_SIZE, (1,)))\n",
    "        token_ = '[MASK]'\n",
    "\n",
    "    # Caso 2: si mask y random_opp son True, pero random_swich es False\n",
    "    elif mask and random_opp and not random_swich:\n",
    "        # Deja el token sin cambios y asignar como etiqueta el mismo token\n",
    "        token_ = token\n",
    "        mask_label = token\n",
    "\n",
    "    # Caso 3: si mask es True, pero random_opp es False\n",
    "    else:\n",
    "        # Reemplaza el token con '[MASK]' y asignar como etiqueta el token original\n",
    "        token_ = '[MASK]'\n",
    "        mask_label = token\n",
    "\n",
    "    return token_, mask_label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos cómo funciona la estrategia de enmascaramiento aleatorio:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fija semilla para garantizar reproducibilidad\n",
    "torch.manual_seed(100)\n",
    "\n",
    "# Ejecuta 10 veces el proceso de enmascarado\n",
    "for l in range(10):\n",
    "    token = \"apple\"  # Token original\n",
    "    token_, label = Masking(token)  # Aplicar función de enmascarado\n",
    "\n",
    "    # Si el token permanece igual y la etiqueta es [PAD]\n",
    "    if token == token_ and label == \"[PAD]\":\n",
    "        print(\n",
    "            token_,\n",
    "            label,\n",
    "            f\"\\t El token actual *{token}* permanece sin cambios\"\n",
    "        )\n",
    "\n",
    "    # Si el token se enmascara y la etiqueta coincide con el token original\n",
    "    elif token_ == \"[MASK]\" and label == token:\n",
    "        print(\n",
    "            token_,\n",
    "            label,\n",
    "            f\"\\t El token actual *{token}* se enmascara con '{token_}'\"\n",
    "        )\n",
    "\n",
    "    # En cualquier otro caso, se ha reemplazado por un token aleatorio\n",
    "    else:\n",
    "        print(\n",
    "            token_,\n",
    "            label,\n",
    "            f\"\\t El token actual *{token}* se reemplaza por el token aleatorio #{label}#\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Preparación de datos para MLM**\n",
    "\n",
    "`prepare_for_mlm` prepara el texto tokenizado para el entrenamiento de MLM aplicando la estrategia de enmascaramiento. Devuelve secuencias de tokens enmascarados junto con sus etiquetas correspondientes, e incluye opcionalmente los tokens originales (sin procesar) como referencia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_mlm(tokens, include_raw_tokens=False):\n",
    "    \"\"\"\n",
    "    Prepara el texto tokenizado para el entrenamiento del modelo de lenguaje enmascarado (MLM) de BERT.\n",
    "    \"\"\"\n",
    "    bert_input = []      # Lista para almacenar oraciones procesadas para el MLM de BERT\n",
    "    bert_label = []      # Lista para almacenar las etiquetas de cada token (enmascarado, aleatorio o sin cambio)\n",
    "    raw_tokens_list = [] # Lista para almacenar los tokens originales si se requiere\n",
    "    current_bert_input = []\n",
    "    current_bert_label = []\n",
    "    current_raw_tokens = []\n",
    "\n",
    "    for token in tokens:\n",
    "        # Aplica la estrategia de enmascaramiento de BERT al token\n",
    "        masked_token, mask_label = Masking(token)\n",
    "\n",
    "        # Añade el token procesado y su etiqueta a las listas actuales\n",
    "        current_bert_input.append(masked_token)\n",
    "        current_bert_label.append(mask_label)\n",
    "\n",
    "        # Si se incluyen los tokens originales, añádelos a la lista correspondiente\n",
    "        if include_raw_tokens:\n",
    "            current_raw_tokens.append(token)\n",
    "\n",
    "        # Verifica si el token es un delimitador de oración (., ?, !)\n",
    "        if token in ['.', '?', '!']:\n",
    "            # Si la oración actual tiene más de dos tokens, considérala válida\n",
    "            if len(current_bert_input) > 2:\n",
    "                bert_input.append(current_bert_input)\n",
    "                bert_label.append(current_bert_label)\n",
    "                # Si se incluyen los tokens originales, añádelos a la lista principal\n",
    "                if include_raw_tokens:\n",
    "                    raw_tokens_list.append(current_raw_tokens)\n",
    "\n",
    "                # Reinicia las listas para la siguiente oración\n",
    "                current_bert_input = []\n",
    "                current_bert_label = []\n",
    "                current_raw_tokens = []\n",
    "            else:\n",
    "                # Si la oración es muy corta, descártala y reinicia las listas\n",
    "                current_bert_input = []\n",
    "                current_bert_label = []\n",
    "                current_raw_tokens = []\n",
    "\n",
    "    # Agrega cualquier token restante como una oración si hay alguno\n",
    "    if current_bert_input:\n",
    "        bert_input.append(current_bert_input)\n",
    "        bert_label.append(current_bert_label)\n",
    "        if include_raw_tokens:\n",
    "            raw_tokens_list.append(current_raw_tokens)\n",
    "\n",
    "    # Devuelve las listas preparadas para el entrenamiento MLM de BERT\n",
    "    return (bert_input, bert_label, raw_tokens_list) if include_raw_tokens else (bert_input, bert_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, veamos cómo las preparaciones de MLM transforman la entrada cruda en la entrada lista para el entrenamiento:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Fija la semilla de PyTorch en 100 para reproducibilidad\n",
    "torch.manual_seed(100)\n",
    "\n",
    "# Entrada original\n",
    "original_input = \"The sun sets behind the distant mountains.\"\n",
    "\n",
    "# Tokeniza la entrada\n",
    "tokens = tokenizer(original_input)\n",
    "\n",
    "# Prepara los tensores para el enmascarado de lenguaje (sin incluir la lista de tokens sin procesar)\n",
    "bert_input, bert_label = prepare_for_mlm(tokens, include_raw_tokens=False)\n",
    "\n",
    "# Muestra los resultados sin los tokens crudos\n",
    "print(\"Sin tokens crudos:\\t\",\n",
    "      \"\\n\\tEntrada original:\\t\", original_input,\n",
    "      \"\\n\\tbert_input:\\t\\t\", bert_input,\n",
    "      \"\\n\\tbert_label:\\t\\t\", bert_label)\n",
    "\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Fija de nuevo la semilla para que coincidan los resultados\n",
    "torch.manual_seed(100)\n",
    "\n",
    "# Prepara los tensores incluyendo la lista de tokens sin procesar\n",
    "bert_input, bert_label, raw_tokens_list = prepare_for_mlm(tokens, include_raw_tokens=True)\n",
    "\n",
    "# Muestra los resultados con los tokens crudos\n",
    "print(\"Con tokens crudos:\\t\",\n",
    "      \"\\n\\tEntrada original:\\t\", original_input,\n",
    "      \"\\n\\tbert_input:\\t\\t\", bert_input,\n",
    "      \"\\n\\tbert_label:\\t\\t\", bert_label,\n",
    "      \"\\n\\ttokens crudos:\\t\\t\", raw_tokens_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo tanto, a cada token en una oración se le asigna una etiqueta según la operación de enmascaramiento que se aplica sobre ese token. En este ejemplo, el primer \"the\" está **enmascarado**, por lo tanto, `bert_input` es `[MASK]` y su `bert_label` es `'The'`. \n",
    "\n",
    "Los tokens \"sun\", \"sets\", \"behind\" y el último \"the\" no se modifican, así que sus etiquetas correspondientes son `[PAD]`. “distant” está **enmascarado y reemplazado con un token aleatorio**, por lo tanto, `bert_input` es `[MASK]` y su `bert_label` es `'human-scaled'`. \n",
    "\n",
    "Finalmente, \"mountains\" y \".\" quedan **sin cambios**, por lo que sus etiquetas correspondientes son `[PAD]`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Preparación de datos para NSP**\n",
    "\n",
    "La función `process_for_nsp` prepara los datos para la tarea de NSP creando pares de oraciones. Etiqueta estos pares para indicar si la segunda oración es la continuación de la primera en el texto original, facilitando que el modelo aprenda las relaciones entre oraciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def process_for_nsp(input_sentences, input_masked_labels):\n",
    "    \"\"\"\n",
    "    Prepara los datos para la tarea NSP en el entrenamiento de BERT.\n",
    "\n",
    "    Args:\n",
    "    input_sentences (list): Lista de oraciones tokenizadas.\n",
    "    input_masked_labels (list): Lista correspondiente de etiquetas enmascaradas para las oraciones.\n",
    "\n",
    "    Returns:\n",
    "    bert_input (list): Lista de pares de oraciones para la entrada de BERT.\n",
    "    bert_label (list): Lista de etiquetas enmascaradas para los pares de oraciones.\n",
    "    is_next (list): Lista de etiquetas binarias donde 1 indica 'siguiente oración' y 0 indica 'no siguiente oración'.\n",
    "    \"\"\"\n",
    "    if len(input_sentences) < 2:\n",
    "        raise ValueError(\"Debe haber al menos dos elementos.\")\n",
    "\n",
    "    # Verifica que ambas listas tengan la misma longitud\n",
    "    if len(input_sentences) != len(input_masked_labels):\n",
    "        raise ValueError(\"Ambas listas deben tener el mismo número de elementos.\")\n",
    "\n",
    "    bert_input = []\n",
    "    bert_label = []\n",
    "    is_next = []\n",
    "\n",
    "    available_indices = list(range(len(input_sentences)))\n",
    "\n",
    "    # Mientras haya al menos dos índices disponibles\n",
    "    while len(available_indices) >= 2:\n",
    "        if random.random() < 0.5:\n",
    "            # Elige dos oraciones consecutivas para el escenario 'siguiente oración'\n",
    "            index = random.choice(available_indices[:-1])  # Excluye el último índice\n",
    "            # Añade los tokens '[CLS]' y '[SEP]' y construye el par de entrada\n",
    "            bert_input.append(\n",
    "                [\n",
    "                    ['[CLS]'] + input_sentences[index] + ['[SEP]'],\n",
    "                    input_sentences[index + 1] + ['[SEP]']\n",
    "                ]\n",
    "            )\n",
    "            bert_label.append(\n",
    "                [\n",
    "                    ['[PAD]'] + input_masked_labels[index] + ['[PAD]'],\n",
    "                    input_masked_labels[index + 1] + ['[PAD]']\n",
    "                ]\n",
    "            )\n",
    "            is_next.append(1)  # Etiqueta 1 indica que son oraciones consecutivas\n",
    "\n",
    "            # Elimina los índices usados\n",
    "            available_indices.remove(index)\n",
    "            if index + 1 in available_indices:\n",
    "                available_indices.remove(index + 1)\n",
    "        else:\n",
    "            # Elige dos oraciones aleatorias no consecutivas para el escenario 'no siguiente oración'\n",
    "            indices = random.sample(available_indices, 2)\n",
    "            bert_input.append(\n",
    "                [\n",
    "                    ['[CLS]'] + input_sentences[indices[0]] + ['[SEP]'],\n",
    "                    input_sentences[indices[1]] + ['[SEP]']\n",
    "                ]\n",
    "            )\n",
    "            bert_label.append(\n",
    "                [\n",
    "                    ['[PAD]'] + input_masked_labels[indices[0]] + ['[PAD]'],\n",
    "                    input_masked_labels[indices[1]] + ['[PAD]']\n",
    "                ]\n",
    "            )\n",
    "            is_next.append(0)  # Etiqueta 0 indica que no son oraciones consecutivas\n",
    "\n",
    "            # Elimina los índices usados\n",
    "            available_indices.remove(indices[0])\n",
    "            available_indices.remove(indices[1])\n",
    "\n",
    "    return bert_input, bert_label, is_next\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos algunas oraciones de ejemplo y creemos pares NSP:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aplana el tensor\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "# oraciones de entrada de ejemplo\n",
    "input_sentences = [[\"i\", \"love\", \"apples\"], [\"she\", \"enjoys\", \"reading\", \"books\"], [\"he\", \"likes\", \"playing\", \"guitar\"]]\n",
    "\n",
    "# crea etiquetas enmascaradas para las oraciones\n",
    "input_masked_labels = []\n",
    "for sentence in input_sentences:\n",
    "    _, current_masked_label = prepare_for_mlm(sentence, include_raw_tokens=False)\n",
    "    input_masked_labels.append(flatten(current_masked_label))\n",
    "\n",
    "# crea pares NSP y sus etiquetas\n",
    "random.seed(100)\n",
    "bert_input, bert_label, is_next = process_for_nsp(input_sentences, input_masked_labels)\n",
    "\n",
    "# Imprime la salida\n",
    "print(\"Entrada BERT:\")\n",
    "for pair in bert_input:\n",
    "    print(pair)\n",
    "print(\"Etiqueta BERT:\")\n",
    "for pair in bert_label:\n",
    "    print(pair)\n",
    "print(\"¿Es continuación?:\", is_next)\n",
    "print(\"-\" * 200)\n",
    "\n",
    "# Repite con otra semilla para la aleatoriedad\n",
    "random.seed(1000)\n",
    "bert_input, bert_label, is_next = process_for_nsp(input_sentences, input_masked_labels)\n",
    "\n",
    "# Imprimir la salida nuevamente\n",
    "print(\"Entrada BERT:\")\n",
    "for pair in bert_input:\n",
    "    print(pair)\n",
    "print(\"Etiqueta BERT:\")\n",
    "for pair in bert_label:\n",
    "    print(pair)\n",
    "print(\"¿Es continuación?:\", is_next)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos dos ejemplos muestran cómo los pares de oraciones se crean de forma aleatoria a partir del banco de oraciones y se etiquetan para la tarea NSP. Primero se añaden los símbolos especiales \\[CLS] y \\[SEP] a las oraciones de entrada. La etiqueta de BERT se genera mediante la función `prepare_for_mlm`. \n",
    "\n",
    "En el primer ejemplo, la segunda oración sigue a la primera; por lo tanto, `Is Next` vale 1. En el segundo ejemplo, la segunda oración no sigue a la primera; así que `Is Next` vale 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Finalizando las entradas de BERT**\n",
    "\n",
    "`prepare_bert_final_inputs` consolida los datos preparados para MLM y NSP en un formato adecuado para el entrenamiento de BERT, incluyendo la conversión de tokens a índices, el relleno de secuencias para obtener una longitud uniforme y la generación de etiquetas de segmento para distinguir entre pares de oraciones. \n",
    "\n",
    "Esta función es el paso final en la preparación de los datos para BERT, asegurando que estén en el formato correcto para un entrenamiento efectivo del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import torch\n",
    "\n",
    "def prepare_bert_final_inputs(bert_inputs, bert_labels, is_nexts, to_tenor=True):\n",
    "    \"\"\"\n",
    "    Prepara las listas finales de entrada para el entrenamiento de BERT.\n",
    "    \"\"\"\n",
    "    def zero_pad_list_pair(pair_, pad='[PAD]'):\n",
    "        pair = deepcopy(pair_)\n",
    "        max_len = max(len(pair[0]), len(pair[1]))\n",
    "        # Añade [PAD] a cada frase del par hasta alcanzar la longitud máxima\n",
    "        pair[0].extend([pad] * (max_len - len(pair[0])))\n",
    "        pair[1].extend([pad] * (max_len - len(pair[1])))\n",
    "        return pair[0], pair[1]\n",
    "\n",
    "    # Aplana la lista de listas\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    # Transforma tokens a índices del vocabulario\n",
    "    tokens_to_index = lambda tokens: [vocab[token] for token in tokens]\n",
    "\n",
    "    bert_inputs_final, bert_labels_final = [], []\n",
    "    segment_labels_final, is_nexts_final = [], []\n",
    "\n",
    "    for bert_input, bert_label, is_next in zip(bert_inputs, bert_labels, is_nexts):\n",
    "        # Crea etiquetas de segmento para cada par de oraciones\n",
    "        segment_label = [[1] * len(bert_input[0]), [2] * len(bert_input[1])]\n",
    "\n",
    "        # Aplica padding con ceros a bert_input, bert_label y segment_label\n",
    "        bert_input_padded = zero_pad_list_pair(bert_input)\n",
    "        bert_label_padded = zero_pad_list_pair(bert_label)\n",
    "        segment_label_padded = zero_pad_list_pair(segment_label, pad=0)\n",
    "\n",
    "        # Convierte a tensores\n",
    "        if to_tenor:\n",
    "            # Aplana los pares con padding, convierte tokens a índices y crea tensores\n",
    "            bert_inputs_final.append(\n",
    "                torch.tensor(tokens_to_index(flatten(bert_input_padded)), dtype=torch.int64)\n",
    "            )\n",
    "            bert_labels_final.append(\n",
    "                torch.tensor(tokens_to_index(flatten(bert_label_padded)), dtype=torch.int64)\n",
    "            )\n",
    "            segment_labels_final.append(\n",
    "                torch.tensor(flatten(segment_label_padded), dtype=torch.int64)\n",
    "            )\n",
    "            is_nexts_final.append(is_next)\n",
    "        else:\n",
    "            # Solo aplana y deja los valores como listas\n",
    "            bert_inputs_final.append(flatten(bert_input_padded))\n",
    "            bert_labels_final.append(flatten(bert_label_padded))\n",
    "            segment_labels_final.append(flatten(segment_label_padded))\n",
    "            is_nexts_final.append(is_next)\n",
    "\n",
    "    return bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes comprobar los resultados usando `bert_input`, `bert_label` e `is_next` del ejemplo anterior:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_final = prepare_bert_final_inputs(\n",
    "    bert_input,\n",
    "    bert_label,\n",
    "    is_next,\n",
    "    to_tenor=True\n",
    ")\n",
    "\n",
    "torch.set_printoptions(linewidth=10000)  # esto asegura que toda la salida se imprima en una sola línea\n",
    "\n",
    "print(\n",
    "    \"entrada:\\t\\t\", bert_input,\n",
    "    \"\\nentradas_finales:\\t\", bert_inputs_final,\n",
    "    \"\\netiquetas_bert_finales:\\t\", bert_labels_final,\n",
    "    \"\\netiquetas_segmento_finales:\\t\", segment_labels_final,\n",
    "    \"\\nes_siguiente_final:\\t\", is_nexts_final\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las oraciones se rellenan con ceros y cada token se mapea a su índice en el vocabulario (`[CLS]` >> 1, `he` >> 33, ..., `[SEP]` >> 2, `[PAD]` >> 0).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las etiquetas de máscara también se rellenan (padding) y se asignan a índices del vocabulario. En este caso, todos los tokens se **mantienen igual** excepto el token `he`, que está enmascarado:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"entrada:\\t\\t\", bert_input,\n",
    "      \"\\netiqueta_máscara:\\t\", bert_label,\n",
    "      \"\\netiquetas_finales:\\t\", bert_labels_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente se crean las etiquetas de segmento: los tokens de la primera oración se etiquetan con 1, los de la segunda oración con 2, y los rellenos con ceros se etiquetan con 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"\\nentradas_finales:\\t\", bert_inputs_final,\n",
    "    \"\\netiquetas_de_segmento:\\t\", segment_labels_final\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preparando el conjunto de datos**\n",
    "\n",
    "* Se crea un archivo CSV para almacenar el conjunto de datos preparado para el entrenamiento y prueba de BERT. Cada fila contiene el texto original, las entradas para BERT, las etiquetas, las etiquetas de segmento y la etiqueta de la tarea NSP.\n",
    "* Los datos del conjunto IMDB se tokenizan y se procesan primero para MLM y luego para NSP. Los resultados se formatean y se escriben en el archivo CSV, proporcionando un conjunto de datos completo para el entrenamiento del modelo BERT.\n",
    "\n",
    "Este proceso es fundamental para garantizar que los datos estén en el formato adecuado para un entrenamiento eficaz de BERT con el conjunto IMDB, centrándose en comprender el contexto del texto y las relaciones entre oraciones.\n",
    "\n",
    "> Este proceso de entrenamiento podría tardar alrededor de 2 a 3 horas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = 'train_bert_data_new.csv'\n",
    "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "    csv_writer = csv.writer(file)\n",
    "    csv_writer.writerow(['Original Text', 'BERT Input', 'BERT Label', 'Segment Label', 'Is Next'])\n",
    "\n",
    "    # Envuelve train_iter con tqdm para mostrar una barra de progreso\n",
    "    for n, (_, sample) in enumerate(tqdm(train_iter, desc=\"Procesando muestras\")):\n",
    "        # Tokeniza la entrada de la muestra\n",
    "        tokens = tokenizer(sample)\n",
    "        # Crea entradas y etiquetas para MLM\n",
    "        bert_input, bert_label = prepare_for_mlm(tokens, include_raw_tokens=False)\n",
    "        if len(bert_input) < 2:\n",
    "            continue\n",
    "        # Crea pares NSP, etiquetas de tokens y etiqueta is_next\n",
    "        bert_inputs, bert_labels, is_nexts = process_for_nsp(bert_input, bert_label)\n",
    "        # Añade relleno con ceros, mapear tokens a índices del vocabulario y crear etiquetas de segmento\n",
    "        bert_inputs, bert_labels, segment_labels, is_nexts = prepare_bert_final_inputs(bert_inputs, bert_labels, is_nexts)\n",
    "        # Convierte tensores a listas, listas a cadenas con formato JSON\n",
    "        for bert_input, bert_label, segment_label, is_next in zip(bert_inputs, bert_labels, segment_labels, is_nexts):\n",
    "            bert_input_str = json.dumps(bert_input.tolist())\n",
    "            bert_label_str = json.dumps(bert_label.tolist())\n",
    "            segment_label_str = ','.join(map(str, segment_label.tolist()))\n",
    "            # Escribe los datos en el archivo CSV fila por fila\n",
    "            csv_writer.writerow([sample, bert_input_str, bert_label_str, segment_label_str, is_next])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejercicios**\n",
    "\n",
    "Aprende a utilizar el BertTokenizer preentrenado de Hugging Face para la tokenización de texto, incluyendo el manejo de tokens especiales y la preparación del conjunto de datos IMDB para el entrenamiento de modelos de NLP, sin necesidad de construir manualmente un vocabulario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejercicio 1 - Inicialización del BERT Tokenizer**\n",
    "\n",
    "1. **Importar `BertTokenizer`**:\n",
    "   Comienza importando la clase `BertTokenizer` de la biblioteca `transformers`. Esta biblioteca ofrece acceso a una amplia gama de modelos de PLN y sus tokenizadores correspondientes.\n",
    "\n",
    "2. **Cargar el tokenizador preentrenado**:\n",
    "   Utiliza el método `from_pretrained` para cargar el tokenizador `bert-base-uncased`. Este tokenizador viene preconfigurado con un vocabulario adecuado para el modelo BERT entrenado con texto en inglés sin mayúsculas. Es ideal para comprender los fundamentos de la tokenización de BERT.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Cargar un tokenizador BERT preentrenado\n",
    "tokenizer = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejercicio 2 - Tokenización del conjunto de datos**\n",
    "\n",
    "1. **Define la función `yield_tokens`:**\n",
    "   Implementa una función llamada `yield_tokens` que reciba un iterador sobre el conjunto de datos. Esta función será responsable de procesar y tokenizar los datos de texto.\n",
    "\n",
    "2. **Tokeniza las muestras de texto:**\n",
    "   Dentro de la función `yield_tokens`, recorre el conjunto de datos. Para cada muestra de texto, utiliza el `BertTokenizer` para convertir el texto en secuencias de IDs de tokens. Asegúrate de gestionar textos extensos estableciendo el parámetro `truncation=True` y especificando un `max_length` para que todas las salidas tokenizadas tengan un tamaño manejable.\n",
    "\n",
    "3. **Devuelve los textos tokenizados:**\n",
    "   Tras tokenizar cada muestra de texto, devuelve (yield) la lista de IDs de tokens. Estas listas se usarán en pasos posteriores para construir las estructuras de datos adecuadas para entrenar modelos de PLN.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejercicio 3 -Construcción del vocabulario con tokens especiales**\n",
    "\n",
    "1. **Definir tokens especiales e índices**: Empieza definiendo índices para los tokens especiales como `[PAD]`, `[CLS]`, `[SEP]`, `[MASK]` y `[UNK]`. Crea una lista llamada `special_symbols` que incluya estos tokens, asegurándote de que estén en el orden correcto según sus índices.\n",
    "\n",
    "2. **Cargar el dataset**: Asegúrate de tener cargada la partición de entrenamiento del dataset IMDB. Estos datos se usarán para construir el vocabulario.\n",
    "\n",
    "3. **Construir el vocabulario**: Utiliza la función `build_vocab_from_iterator`, pasando como argumento la función generadora `yield_tokens`. Esta función recorre el dataset tokenizado y construye el vocabulario. Asegúrate de incluir los tokens especiales especificándolos en el parámetro `specials` de `build_vocab_from_iterator`.\n",
    "   *(Como estás usando un tokenizador preentrenado, no necesitas construir el vocabulario desde cero; en su lugar, puedes usar directamente el vocabulario del tokenizador.)*\n",
    "\n",
    "4. **Establecer el índice por defecto para tokens desconocidos**: Tras construir el vocabulario, usa el método `set_default_index` para asignar el índice correspondiente a los tokens desconocidos (`UNK_IDX`). Esto garantiza que cualquier token no encontrado en el vocabulario se maneje correctamente.\n",
    "   *(De nuevo, si usas un tokenizador preentrenado, puedes prescindir de esta construcción y emplear directamente el vocabulario existente.)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torchtext.datasets import IMDB\n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "prev_pub_hash": "6c8fa70732ed928bd648243a39ecd88f18424f64edfa13622ab688097a48508d"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
