{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Construcción y entrenamiento de un modelo de lenguaje simple con una red neuronal**\n",
    "\n",
    "Este proyecto sirve como una introducción al campo del modelado de lenguaje, enfocándose en la creación de un generador de texto diseñado para componer canciones de rap de los años 90.\n",
    "\n",
    "Utilizaremos modelos n-gramas basados en histogramas, implementados a través de la herramienta *Natural Language Toolkit* (NLTK). Este enfoque nos permite construir histogramas reveladores, que iluminan las cadencias matizadas de las frecuencias y distribuciones de palabras.\n",
    "\n",
    "Estos pasos iniciales sientan las bases para comprender las complejidades de los patrones lingüísticos. A medida que avancemos, entraremos en el dominio de las redes neuronales dentro del entorno de PyTorch. \n",
    "\n",
    "En este ámbito, diseñaremos una red neuronal *feedforward*, explorando conceptos como las capas de *embeddings*. También perfeccionaremos la capa de salida, adaptándola para un rendimiento óptimo en tareas de modelado de lenguaje.\n",
    "\n",
    "A lo largo de este recorrido, exploraremos diversas estrategias de entrenamiento ycon tareas fundamentales del procesamiento de lenguaje natural (NLP), incluyendo la tokenización y el análisis de secuencias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuración"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este cuaderno, utilizaremos las siguientes librerías:\n",
    "\n",
    "*   [`pandas`](https://pandas.pydata.org/) para la gestión de datos.  \n",
    "*   [`numpy`](https://numpy.org/) para realizar operaciones matemáticas.  \n",
    "*   [`sklearn`](https://scikit-learn.org/stable/) para funciones relacionadas con aprendizaje automático y flujos de trabajo de *aprendizaje automático*.  \n",
    "*   [`seaborn`](https://seaborn.pydata.org/) para la visualización de datos.  \n",
    "*   [`matplotlib`](https://matplotlib.org/) como herramienta adicional para la creación de gráficos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instalación de librerías requeridas\n",
    "\n",
    "Todas las librerías necesarias ya están preinstaladas en el entorno de docker del curso. Sin embargo, si ejecutas los comandos de este cuaderno en un entorno de Jupyter diferente (por ejemplo, **Watson Studio** o **Anaconda**), necesitarás instalar estas librerías utilizando la celda de código que aparece a continuación.\n",
    "\n",
    "<h4 style=\"color:red;\">Después de instalar las librerías a continuación, por favor REINICIA EL KERNEL y ejecuta todas las celdas.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "\n",
    "#!mamba install -y nltk\n",
    "#!pip install torchtext -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importación de las librerías requeridas\n",
    "\n",
    "_Se recomienda importar todas las bibliotecas necesarias en un solo lugar (aquí):_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from tqdm import tqdm  # Barra de progreso para bucles\n",
    "\n",
    "warnings.simplefilter('ignore')  # Ignora todas las advertencias\n",
    "import time\n",
    "from collections import OrderedDict  # Diccionario que mantiene el orden de inserción\n",
    "\n",
    "import re  # Expresiones regulares\n",
    "\n",
    "import numpy as np  # Operaciones numéricas eficientes\n",
    "import matplotlib.pyplot as plt  # Visualización de datos\n",
    "import pandas as pd  # Manipulación de datos en forma de tablas\n",
    "\n",
    "# Descarga de recursos de tokenización para NLTK\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Importación de PyTorch y módulos relevantes\n",
    "import torch\n",
    "import torch.nn as nn  # Módulo de redes neuronales\n",
    "import torch.nn.functional as F  # Funciones de activación, pérdidas, etc.\n",
    "import torch.optim as optim  # Optimizadores\n",
    "import string\n",
    "import time\n",
    "\n",
    "# Importación adicional para visualización de reducción de dimensión\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE  # t-SNE para visualización de vectores de alta dimensión\n",
    "\n",
    "# También puedes usar esta sección para suprimir advertencias generadas por tu código:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición de funciones auxiliares\n",
    "\n",
    "Elimina todos los caracteres que no sean parte de palabras (todo excepto números y letras)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_string(s):\n",
    "    # Eliminar todos los caracteres que no sean parte de palabras (todo excepto números y letras)\n",
    "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
    "    # Reemplazar todas las secuencias de espacios en blanco sin dejar espacio\n",
    "    s = re.sub(r\"\\s+\", '', s)\n",
    "    # Reemplazar dígitos sin dejar espacio\n",
    "    s = re.sub(r\"\\d\", '', s)\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Modelado de lenguaje**\n",
    "\n",
    "El modelado de lenguaje es un concepto fundamental dentro del campo del procesamiento de lenguaje natural (NLP) y la inteligencia artificial. Consiste en predecir la probabilidad de una secuencia de palabras dentro de un idioma dado. Este método tiene una naturaleza estadística y busca capturar los patrones, estructuras y relaciones que existen entre las palabras en un corpus de texto determinado.\n",
    "\n",
    "En esencia, un modelo de lenguaje busca comprender las probabilidades asociadas con secuencias de palabras. Esta comprensión puede aprovecharse en una multitud de tareas de NLP, incluyendo, pero no limitándose a, la generación de texto, traducción automática, reconocimiento de voz, análisis de sentimientos, entre otras.\n",
    "\n",
    "Consideremos las siguientes letras de una canción para ver si podemos generar una salida similar a partir de una palabra dada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song= \"\"\"We are no strangers to love\n",
    "You know the rules and so do I\n",
    "A full commitments what Im thinking of\n",
    "You wouldnt get this from any other guy\n",
    "I just wanna tell you how Im feeling\n",
    "Gotta make you understand\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Weve known each other for so long\n",
    "Your hearts been aching but youre too shy to say it\n",
    "Inside we both know whats been going on\n",
    "We know the game and were gonna play it\n",
    "And if you ask me how Im feeling\n",
    "Dont tell me youre too blind to see\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Weve known each other for so long\n",
    "Your hearts been aching but youre too shy to say it\n",
    "Inside we both know whats been going on\n",
    "We know the game and were gonna play it\n",
    "I just wanna tell you how Im feeling\n",
    "Gotta make you understand\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Natural Language Toolkit (NLTK)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK es, en efecto, una librería de código abierto ampliamente utilizada en Python, diseñada específicamente para diversas tareas de procesamiento de lenguaje natural (NLP). Proporciona un conjunto completo de herramientas, recursos y algoritmos que facilitan el análisis y la manipulación de datos del lenguaje humano. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenización\n",
    "\n",
    "La tokenización, un concepto fundamental dentro del campo del procesamiento de lenguaje natural (NLP), implica el proceso detallado de dividir un cuerpo de texto en unidades discretas conocidas como *tokens*. Estos tokens pueden abarcar palabras, frases, oraciones o incluso caracteres individuales, dependiendo del nivel de granularidad deseado para el análisis. \n",
    "\n",
    "Para los fines de este proyecto, nos enfocaremos en la *tokenización de palabras*, una técnica ampliamente utilizada. Esta técnica trata cada palabra del texto como una entidad independiente. Las palabras, típicamente separadas por espacios o signos de puntuación, actúan como tokens en este enfoque. Es importante señalar que la tokenización de palabras presenta características versátiles, incluyendo el manejo de mayúsculas, símbolos y signos de puntuación.\n",
    "\n",
    "Para lograr este objetivo, utilizaremos la función ```word_tokenize```. Durante este proceso, eliminaremos los signos de puntuación, los símbolos y las letras mayúsculas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "def preprocess(words):\n",
    "    tokens = word_tokenize(words)\n",
    "    tokens = [preprocess_string(w) for w in tokens]\n",
    "    return [w.lower() for w in tokens if len(w) != 0 or not (w in string.punctuation)]\n",
    "\n",
    "tokens = preprocess(song)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado es una colección de tokens, en la que cada elemento de la variable ```tokens``` corresponde a las letras de la canción, ordenados secuencialmente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La distribución de frecuencias de palabras en una oración representa cuántas veces aparece cada palabra en esa oración en particular. Proporciona un conteo de las apariciones de palabras individuales, lo que permite entender cuáles son las palabras más comunes o frecuentes dentro de la oración dada. Trabajemos con el siguiente ejemplo sencillo:\n",
    "\n",
    "```Texto```: **I like dogs and I kinda like cats**\n",
    "\n",
    "```Tokens```: **[I like, dogs, and, I, kinda, like, cats]**\n",
    "\n",
    "La función ```Count``` contabilizará las apariciones de las palabras en el texto de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Count(\\text{\"I\"})=2$\n",
    "\n",
    "$Count(\\text{\"like\"})= 2$\n",
    "\n",
    "$Count(\\text{\"dogs\"})=1$\n",
    "\n",
    "$Count(\\text{\"and\"})=1$\n",
    "\n",
    "$Count(\\text{\"kinda\"})=1$\n",
    "\n",
    "$Count(\\text{\"cats\"})=1$\n",
    "\n",
    "$\\text{Total de palabras} =8$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliza ```FreqDist``` de NLTK para transformar la distribución de frecuencias de palabras. El resultado es un diccionario de Python donde las claves corresponden a las palabras y los valores indican la frecuencia con la que aparece cada palabra. Consideremos el siguiente ejemplo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea una distribución de frecuencias de las palabras\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dibujamos las diez palabras con mayor frecuencia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(list(fdist.keys())[0:10], list(fdist.values())[0:10])\n",
    "plt.xlabel(\"Palabras\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Modelo unigrama**\n",
    "\n",
    "Un modelo *unigrama* (Unigram Model) es un tipo simple de modelo de lenguaje que considera cada palabra en una secuencia de forma independiente, sin tener en cuenta las palabras anteriores. En otras palabras, modela la probabilidad de que cada palabra ocurra en el texto, sin importar que palabra la precede. \n",
    "\n",
    "Los modelos unigrama pueden verse como un caso especial de los modelos *n-grama*, donde *n* es igual a 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos pensar que el texto sigue patrones y que las probabilidades se usan para medir qué tan probable es una secuencia de palabras. \n",
    "\n",
    "En un modelo unigrama, cada palabra se considera de forma independiente y no depende de las demás. Calculemos la probabilidad de **'I like tiramisu but I love cheesecake more'**.\n",
    "\n",
    "$  P(\\text{\"I\"}) = \\frac{\\text{Count}(\\text{\"I\"})}{\\text{Total de palabras}}=\\frac{2}{8} = 0.250  $\n",
    "\n",
    "$  P(\\text{\"like\"}) = \\frac{\\text{Count}(\\text{\"like\"})}{\\text{Total de palabras}}=\\frac{1}{8} = 0.125  $\n",
    "\n",
    "$  P(\\text{\"tiramisu\"}) = \\frac{\\text{Count}(\\text{\"tiramisu\"})}{\\text{Total de palabras}}=\\frac{1}{8} = 0.125  $\n",
    "\n",
    "$  P(\\text{\"but\"}) = \\frac{\\text{Count}(\\text{\"but\"})}{\\text{Total de palabras}}=\\frac{1}{8} = 0.125  $\n",
    "\n",
    "$  P(\\text{\"I\"}) = \\frac{\\text{Count}(\\text{\"I\"})}{\\text{Total de palabras}}=\\frac{2}{8} = 0.250  $\n",
    "\n",
    "$  P(\\text{\"love\"}) = \\frac{\\text{Count}(\\text{\"love\"})}{\\text{Total de palabras}}=\\frac{1}{8} = 0.125  $\n",
    "\n",
    "$  P(\\text{\"cheesecake\"}) = \\frac{\\text{Count}(\\text{\"cheesecake\"})}{\\text{Total de palabras}}=\\frac{1}{8} = 0.125  $\n",
    "\n",
    "$  P(\\text{\"more\"}) = \\frac{\\text{Count}(\\text{\"more\"})}{\\text{Total de palabras}}=\\frac{1}{8} = 0.125  $\n",
    "\n",
    "$P(\\text{\"I\"}, \\text{\"like\"}, \\text{\"tiramisu\"}, \\text{\"but\"}, \\text{\"I\"}, \\text{\"love\"}, \\text{\"cheesecake\"}, \\text{\"more\"}) = P(\\text{\"I\"}) \\cdot P(\\text{\"like\"}) \\cdot P(\\text{\"tiramisu\"}) \\cdot P(\\text{\"but\"}) \\cdot P(\\text{\"I\"}) \\cdot P(\\text{\"love\"}) \\cdot P(\\text{\"cheesecake\"}) \\cdot P(\\text{\"more\"}) = 0.250 \\times 0.125 \\times 0.125 \\times 0.125 \\times 0.250 \\times 0.125 \\times 0.125 \\times 0.125$\n",
    "\n",
    "En general, los modelos de lenguaje se reducen a predecir una secuencia de longitud $t$: $P(W_t, W_{t-1}, ..., W_0)$. En esta secuencia de ocho palabras, se tiene:\n",
    "\n",
    "$P(W_7=\\text{\"more\"}, W_6=\\text{\"cheesecake\"}, W_5=\\text{\"love\"}, W_4=\\text{\"I\"}, W_3=\\text{\"but\"}, W_2=\\text{\"tiramisu\"}, W_1=\\text{\"like\"}, W_0=\\text{\"I\"})$\n",
    "\n",
    "El subíndice sirve como un indicador posicional en la secuencia y no afecta la naturaleza de $P(\\bullet)$. Al expresar formalmente la secuencia, la última palabra se posiciona a la izquierda, descendiendo gradualmente conforme se avanza en la secuencia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando NLTK podemos normalizar los valores de frecuencia dividiéndolos por el conteo total de cada palabra para obtener una función de probabilidad. Ahora vamos a encontra la probabilidad de cada palabra.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteo total de cada palabra\n",
    "C = sum(fdist.values())\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hallamos la probabilidad de la palabra _strangers_, es decir, $P(strangers)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist['strangers'] / C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, obtemos cada palabra individual convirtiendo los tokens en un conjunto (set).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cómo el modelo unigrama predice la siguiente palabra probable\n",
    "\n",
    "Consideremos un escenario del ejemplo anterior **\"I like tiramisu but I love cheesecake more\"**, donde se le pide al modelo unigrama predecir la siguiente palabra después de la secuencia **\"I like\"**.\n",
    "\n",
    "Si la palabra con mayor probabilidad entre todas es **\"I\"**, con una probabilidad de 0.25, entonces, según el modelo, la palabra más probable después de **\"I like\"** sería **\"I\"**. Sin embargo, esta predicción no tiene sentido. Esto resalta una limitación importante del modelo unigrama: carece de contexto y sus predicciones dependen únicamente de la palabra con la probabilidad más alta, que en este caso es \"I\".\n",
    "\n",
    "Incluso si varias palabras tienen la misma probabilidad más alta, el modelo elegirá aleatoriamente una de todas las opciones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Modelo bigrama**\n",
    "\n",
    "Los bigramas representan pares de palabras consecutivas en una frase dada, es decir, $(w_{t-1}, w_t)$. Consideremos las siguientes palabras de tu ejemplo: \"I like dogs and I kinda like cats.\"\n",
    "\n",
    "La secuencia correcta de bigramas es:\n",
    "\n",
    "$(I, like)$\n",
    "\n",
    "$(like, dogs)$\n",
    "\n",
    "$(dogs, and)$\n",
    "\n",
    "$(and, I)$\n",
    "\n",
    "$(I, kinda)$\n",
    "\n",
    "$(kinda, like)$\n",
    "\n",
    "$(like, cats)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelos 2-grama**: Los modelos bigrama utilizan la probabilidad condicional. La probabilidad de una palabra depende únicamente de la palabra anterior, es decir, se usa la probabilidad condicional $(W_{t}, W_{t-1})$ para predecir la probabilidad de que la palabra $(W_t)$ siga a la palabra $W_{t-1}$ en una secuencia. \n",
    "\n",
    "Podemos calcular la probabilidad condicional para un modelo bigrama siguiendo los siguientes pasos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos el conteo bigrama para cada bigrama: $Count(W_{t-1}, W_{t})$\n",
    "\n",
    "$Count(\\text{I, like}) = 1$\n",
    "\n",
    "$Count(\\text{like, dogs}) = 1$\n",
    "\n",
    "$Count(\\text{dogs, and}) = 1$\n",
    "\n",
    "$Count(\\text{and, I}) = 1$\n",
    "\n",
    "$Count(\\text{I, kinda}) = 1$\n",
    "\n",
    "$Count(\\text{kinda, like}) = 1$\n",
    "\n",
    "$Count(\\text{like, cats}) = 1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, calculemos la probabilidad condicional para cada bigrama en la forma de $P(w_{t} | w_{t-1})$, donde $w_{t-1}$ es el **contexto**, y el tamaño del contexto es 1.\n",
    "\n",
    "$P(\\text{\"like\"} | \\text{\"I\"}) = \\frac{\\text{Count}(\\text{\"I, like\"})}{\\text{Total de ocurrencias de \"I\"}} = \\frac{1}{2} = 0.5$\n",
    "\n",
    "$P(\\text{\"dogs\"} | \\text{\"like\"}) = \\frac{\\text{Count}(\\text{\"like, dogs\"})}{\\text{Total de ocurrencias de \"like\"}} = \\frac{1}{2} = 0.5$\n",
    "\n",
    "$:$\n",
    "\n",
    "$P(\\text{\"like\"} | \\text{\"kinda\"}) = \\frac{\\text{Count}(\\text{\"kinda, like\"})}{\\text{Total de ocurrencias de \"kinda\"}} = \\frac{1}{1} = 1$\n",
    "\n",
    "$P(\\text{\"cats\"} | \\text{\"like\"}) = \\frac{\\text{Count}(\\text{\"like, cats\"})}{\\text{Total de ocurrencias de \"like\"}} = \\frac{1}{2} = 0.5$\n",
    "\n",
    "Estas probabilidades representan la probabilidad de encontrar la segunda palabra en un bigrama, dada la presencia de la primera.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este enfoque es, de hecho, una aproximación utilizada para determinar la palabra más probable $W_t$, dadas las palabras $W_{t-1}, W_{t-2}, \\ldots, W_1$ en la secuencia.\n",
    "\n",
    "$P(W_t | W_{t-1}, W_{t-2}, \\ldots, W_1) \\approx P(W_t | W_{t-1})$\n",
    "\n",
    "La probabilidad condicional $P(W_t | W_{t-1})$ denota la probabilidad de encontrar la palabra $W_t$, basándose en el contexto proporcionado por la palabra precedente $W_{t-1}$. Al utilizar esta aproximación, se simplifica el proceso de modelado asumiendo que la ocurrencia de la palabra actual está principalmente influenciada por la palabra inmediatamente anterior en la secuencia. \n",
    "\n",
    "De forma general, se puede identificar la palabra más probable como:\n",
    "\n",
    "$\\hat{W_t} = \\arg\\max_{W_t} \\left( P(W_t | W_{t-1}) \\right)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función ```bigrams``` es una función proporcionada por la biblioteca NLTK en Python. Esta función toma una secuencia de tokens como entrada y devuelve un iterador sobre pares consecutivos de tokens, formando bigramas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = nltk.bigrams(tokens)\n",
    "bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se convierte el generador en una lista, donde cada elemento de la lista es un bigrama.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_bigrams = list(nltk.bigrams(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver los primeros 10 bigramas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_bigrams[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos la distribución de frecuencias del bigrama $C(w_{t},w_{t-1})$ utilizando la función ```bigrams``` de NLTK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_bigrams = nltk.FreqDist(nltk.bigrams(tokens))\n",
    "freq_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado es similar a un diccionario, donde la clave es una tupla que contiene el bigrama.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_bigrams[('we', 'are')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible mostrar los primeros 10 valores de la distribución de frecuencias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mi_bigram in mi_bigrams[0:10]:\n",
    "    print(mi_bigram)\n",
    "    print(freq_bigrams[mi_bigram])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí, podemos generar la distribución condicional normalizando la distribución de frecuencias de los unigrama. En este caso, lo haremos para la palabra 'strangers' y luego ordenamos los resultados:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"strangers\"\n",
    "vocab_probabilities = {}\n",
    "for next_word in vocabulary:\n",
    "    vocab_probabilities[next_word] = freq_bigrams[(word, next_word)] / fdist[word]\n",
    "\n",
    "vocab_probabilities = sorted(vocab_probabilities.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se imprime las palabras que son más probables de ocurrir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_probabilities[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea una función para calcular la probabilidad condicional de $W_t$ dado $W_{t-1}$, ordena los resultados y devuélvelos como una lista.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(mi_words, freq_grams, normlize=1, vocabulary=vocabulary):\n",
    "    \"\"\"\n",
    "    Genera predicciones para la probabilidad condicional de la siguiente palabra dada una secuencia.\n",
    "\n",
    "    Args:\n",
    "        mi_words (list): Una lista de palabras en la secuencia de entrada.\n",
    "        freq_grams (dict): Un diccionario que contiene las frecuencias de los n-gramas.\n",
    "        normlize (int): Un factor de normalización para calcular las probabilidades.\n",
    "        vocabulary (list): Una lista de palabras en el vocabulario.\n",
    "    \n",
    "    Returns:\n",
    "        list: Una lista de las palabras predichas junto con sus probabilidades, ordenadas de forma descendente.\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_probabilities = {}  # Inicializa un diccionario para almacenar las probabilidades de las palabras predichas\n",
    "\n",
    "    context_size = len(list(freq_grams.keys())[0])  # Determina el tamaño del contexto a partir de las claves de los n-gramas\n",
    "\n",
    "    # Preprocesa las palabras de entrada y tomar sólo las palabras de contexto relevantes\n",
    "    mi_tokens = preprocess(mi_words)[0:context_size - 1]\n",
    "\n",
    "    # Calcula las probabilidades para cada palabra del vocabulario dado el contexto\n",
    "    for next_word in vocabulary:\n",
    "        temp = mi_tokens.copy()\n",
    "        temp.append(next_word)  # Añade la siguiente palabra al contexto\n",
    "\n",
    "        # Calcula la probabilidad condicional utilizando la información de frecuencia\n",
    "        if normlize != 0:\n",
    "            vocab_probabilities[next_word] = freq_grams[tuple(temp)] / normlize\n",
    "        else:\n",
    "            vocab_probabilities[next_word] = freq_grams[tuple(temp)]\n",
    "    # Ordena las palabras predichas basándose en sus probabilidades de forma descendente\n",
    "    vocab_probabilities = sorted(vocab_probabilities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return vocab_probabilities  # Devuelve la lista ordenada de palabras predichas y sus probabilidades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establece $W_{t-1}$ a 'are' y luego calcula todos los valores de $P(W_t | W_{t-1}=are)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_words = \"are\"\n",
    "\n",
    "vocab_probabilities = make_predictions(mi_words, freq_bigrams, normlize=fdist['i'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_probabilities[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La palabra con la mayor probabilidad, denotada como $\\hat{W}_t$, es la que aparece en el primer elemento de la lista; esto se puede usar como una función de autocompletar simple:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_probabilities[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos una secuencia utilizando el modelo bigrama, aprovechando la palabra previa _(t-1)_ para predecir y generar la siguiente palabra en la secuencia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_song = \"\"\n",
    "for w in tokens[0:100]:\n",
    "    mi_word = make_predictions(w, freq_bigrams)[0][0]\n",
    "    mi_song += \" \" + mi_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_song"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una secuencia con un modelo de n‑gramas, comenzando por la primera palabra y generando una salida inicial. A continuación, empleamos esa salida para predecir la siguiente palabra de la secuencia: introducimos una palabra en el modelo, usamos su salida para predecir la siguiente y repetimos el proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_song = \"i\"\n",
    "\n",
    "for i in range(100):\n",
    "    mi_word = make_predictions(mi_word, freq_bigrams)[0][0]\n",
    "    mi_song += \" \" + mi_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_song"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este método puede no ofrecer resultados óptimos. Consideremos lo siguiente:\n",
    "\n",
    "$\\hat{W_1}=\\arg\\max_{W_1} \\left( P(W_1 | W_{0}=\\text{like})\\right)$.\n",
    "\n",
    "Al evaluarlo, se observa que el resultado para $\\hat{W}_1$ incluye tanto \"dogs\" como \"cats\" con igual probabilidad.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  **Modelo trigrama**\n",
    "\n",
    "Para la oración del ejemplo: 'I like dogs and I kinda like cats'\n",
    "\n",
    "$ (I, like, dogs) $\n",
    "\n",
    "$(like, dogs, and) $\n",
    "\n",
    "$(dogs, and, I)$\n",
    "\n",
    "$(and, I, kinda)$\n",
    "\n",
    "$(I, kinda, like)$\n",
    "\n",
    "$(kinda, like, cats)$\n",
    "\n",
    "Los modelos trigrama también incorporan la probabilidad condicional. La probabilidad de una palabra depende de las dos palabras precedentes. Se usa la probabilidad condicional $P(W_t | W_{t-2}, W_{t-1})$ para predecir la probabilidad de que la palabra $W_t$ siga a las dos palabras previas en una secuencia. El contexto es $W_{t-2}, W_{t-1}$ y su tamaño es 2. Calculemos la probabilidad condicional para cada trigrama:\n",
    "\n",
    "Calculamos las frecuencias de cada trigrama: $Count(W_{t-2}, W_{t-1}, W_t)$\n",
    "\n",
    "### Conteo de frecuencias de trigramas\n",
    "\n",
    "$ \\text{Count(I, like, dogs)} = 1 $\n",
    "\n",
    "$ \\text{Count(like, dogs, and)} = 1 $\n",
    "\n",
    "$\\text{Count(dogs, and, I)} = 1$\n",
    "\n",
    "$ \\text{Count(and, I, kinda)} = 1$\n",
    "\n",
    "$ \\text{Count(I, kinda, like)} = 1 $\n",
    "\n",
    "$ \\text{Count(kinda, like, cats)} = 1 $\n",
    "\n",
    "La probabilidad condicional $ P(w_{t} | w_{t-1}, w_{t-2})$, donde $w_{t-1}$ y $w_{t-2}$ forman el contexto (de tamaño 2).\n",
    "\n",
    "Para entender mejor cómo esto supera al modelo bigrama, calculemos las probabilidades condicionales con el contexto \"I like\":\n",
    "\n",
    "$\\hat{W_2}=\\arg\\max_{W_2} \\left( P(W_2 | W_{1}=like,W_{0}=I)\\right)$\n",
    "\n",
    "y para las palabras \"dogs\" y \"cats\":\n",
    "\n",
    "$ P(\\text{\"dogs\"} |\\text{ \"like\", \"I\"}) = \\frac{Count(\\text{I, like, dogs})}{\\text{Total de  ocurrencias de \"I\", \"like\"}} = \\frac{1}{1} = 1 $\n",
    "\n",
    "$ P(\\text{\"cats\"} | \\text{\"like\", \"I\"}) = \\frac{Count(\\text{I, like, cats})}{\\text{Total \\ de \\ ocurrencias \\ de \\ \"I\", \"like\"}} = 0$\n",
    "\n",
    "Estas probabilidades indican la probabilidad de encontrar la tercera palabra en un trigrama. Es notable que el resultado $\\hat{W_2}$ es \"dogs\", lo cual parece concordar mejor con la secuencia.\n",
    "\n",
    "La función ```trigrams``` es proporcionada por la biblioteca NLTK de Python. Esta función toma una secuencia de tokens como entrada, devuelve un iterador sobre tripletes consecutivos de tokens (trigramas) y los convierte en una distribución de frecuencias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_trigrams = nltk.FreqDist(nltk.trigrams(tokens))\n",
    "freq_trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos la probabilidad para cada una de las siguientes palabras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_predictions(\"so do\", freq_trigrams, normlize=freq_bigrams[('do','i')])[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos la probabilidad para cada una de las siguientes palabras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_song = \"\"\n",
    "\n",
    "w1 = tokens[0]\n",
    "for w2 in tokens[0:100]:\n",
    "    gram = w1 + ' ' + w2\n",
    "    mi_word = make_predictions(gram, freq_trigrams)[0][0]\n",
    "    mi_song += \" \" + mi_word\n",
    "    w1 = w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_song"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen diversos desafíos asociados con los métodos basados en histogramas. Por ejemplo, si consideramos que existen **N** palabras en el vocabulario, un modelo unigrama tendría $N$ compartimientos, mientras que un modelo bigrama tendría $N^2$ compartimientos y así sucesivamente.\n",
    "\n",
    "Los modelos n-grama también tienen limitaciones en su capacidad para captar el contexto y las relaciones intrincadas entre palabras. \n",
    "\n",
    "Por ejemplo, consideremos las frases `I hate dogs`, `I don’t like dogs` y el hecho de que **don’t like** significa **dislike**. En este contexto, un enfoque basado en histogramas fallaría en entender la importancia semántica de la frase **don’t like** que equivale a **dislike**, perdiendo así la relación semántica esencial que implica.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Redes neuronales feedforward (FNN) para modelos de lenguaje**\n",
    "\n",
    "FNNs, o perceptrones multicapa, constituyen los componentes básicos para comprender las redes neuronales en NLP. En tareas de NLP, las FNNs procesan datos textuales transformándolos en vectores numéricos llamados **embeddings**. \n",
    "\n",
    "Estos embeddings se introducen en la red para predecir aspectos del lenguaje, como la siguiente palabra de una oración o el sentimiento de un texto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenización para FNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función de PyTorch se usa para obtener un tokenizador de texto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokens = tokenizer(song)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexación\n",
    "\n",
    "TorchText proporciona herramientas para tokenizar el texto en palabras individuales (tokens) y construir un vocabulario, el cual asigna a cada token un índice entero único. Esto es crucial para preparar los datos textuales para modelos de aprendizaje automático que requieren entrada numérica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Creamos un vocabulario a partir de los tokens del texto\n",
    "\n",
    "# Tokenizamos el texto 'song' usando el tokenizador proporcionado.\n",
    "# La función map aplica el tokenizador a cada palabra del texto al separarlo.\n",
    "# El resultado es una lista de tokens que representan las palabras del 'song'.\n",
    "tokenized_song = map(tokenizer, song.split())\n",
    "\n",
    "# Paso 2:  Se realiza la construcción del vocabulario\n",
    "# La función build_vocab_from_iterator construye un vocabulario a partir del texto tokenizado.\n",
    "# En este caso, se añade un token especial \"<unk>\" (token desconocido) para manejar palabras fuera del vocabulario.\n",
    "vocab = build_vocab_from_iterator(tokenized_song, specials=[\"<unk>\"])\n",
    "\n",
    "# Paso 3: Se establece el índice por defecto\n",
    "# Se asigna el índice por defecto del vocabulario al índice correspondiente al token \"<unk>\".\n",
    "# Esto asegura que cualquier token desconocido en el futuro se mapee a este índice.\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se convierte los tokens a índices aplicando la función, como se muestra a continuación:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab(tokens[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se escribe una función de texto que convierta el texto a índices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "text_pipeline(song)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se busca la palabra correspondiente a un índice utilizando el método ```get_itos()```. El resultado es una lista en la que el índice de la lista corresponde a una palabra.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_token = vocab.get_itos()\n",
    "index_to_token[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Capa de embeddings**\n",
    "\n",
    "Una capa de embeddings es un componente crucial en el procesamiento del lenguaje natural (NLP) y en las redes neuronales diseñadas para datos secuenciales. Su función es convertir variables categóricas, como palabras o índices discretos que representan tokens, en vectores continuos. Esta transformación facilita el entrenamiento y permite que la red aprenda relaciones significativas entre palabras.\n",
    "\n",
    "Consideramos un ejemplo simple con un vocabulario de palabras\n",
    "- **Vocabulario**: {apple, banana, orange, pear}\n",
    "\n",
    "Cada palabra en el vocabulario tiene asignado un índice único:\n",
    "- **Índices**: {0, 1, 2, 3}\n",
    "\n",
    "Cuando se usa una capa de embeddings, se inicializan vectores continuos de forma aleatoria para cada índice. Por ejemplo, los vectores de embeddings podrían ser:\n",
    "\n",
    "- Vector para el índice 0 (apple): `[0.2, 0.8]`\n",
    "- Vector para el índice 1 (banana): `[0.6, -0.5]`\n",
    "- Vector para el índice 2 (orange): `[-0.3, 0.7]`\n",
    "- Vector para el índice 3 (pear): `[0.1, 0.4]`\n",
    "\n",
    "En PyTorch, podemos crear una capa de embeddings de la siguiente manera:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 20\n",
    "vocab_size = len(vocab)\n",
    "embeddings = nn.Embedding(vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embeddings**: Obtemos el embedding para la primera palabra con índice 0 o 1. Es necesario convertir la entrada a un tensor. Los embeddings se inicializan aleatoriamente, pero a medida que el modelo se entrena, las palabras con significados similares tenderán a agruparse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(2): \n",
    "    embedding = embeddings(torch.tensor(n))\n",
    "    print(\"Palabra\", index_to_token[n])\n",
    "    print(\"Índice\", n)\n",
    "    print(\"Embedding\", embedding)\n",
    "    print(\"Forma del embedding\", embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos vectores servirán como entrada para la siguiente capa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generación de pares contexto-objetivo (n-gramas)\n",
    "\n",
    "Organiza las palabras dentro de un contexto de tamaño variable utilizando el siguiente enfoque: Cada palabra se denota por 'i'. \n",
    "Para establecer el contexto, resta 'j'. El tamaño del contexto viene determinado por el valor de ``CONTEXT_SIZE``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "\n",
    "ngrams = [\n",
    "    (\n",
    "        [tokens[i - j - 1] for j in range(CONTEXT_SIZE)],\n",
    "        tokens[i]\n",
    "    )\n",
    "    for i in range(CONTEXT_SIZE, len(tokens))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostremos el primer elemento, que resulta en una tupla. El primer elemento representa el contexto y el segundo la palabra objetivo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context, target = ngrams[0]\n",
    "print(\"Contexto\", context, \"Objetivo\", target)\n",
    "print(\"Indice del contexto\", vocab(context), \"Indice del objetivo\", vocab([target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este contexto, hay varias palabras.  Agrega los embeddings de cada una de estas palabras y ajusta el tamaño de entrada para la siguiente capa en consecuencia. Luego, crea la siguiente capa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(embedding_dim * CONTEXT_SIZE, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos dos embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_embeddings = embeddings(torch.tensor(vocab(context)))\n",
    "mi_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambiamos la forma de los embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_embeddings = mi_embeddings.reshape(1, -1)\n",
    "mi_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos usar esto como entrada en la siguiente capa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear(mi_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función batch\n",
    "\n",
    "Creamos una función batch para interactuar con el DataLoader. Se requieren varios ajustes para manejar palabras que forman parte de un contexto en un batch y que son la palabra objetivo en el siguiente batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "CONTEXT_SIZE = 3\n",
    "BATCH_SIZE = 10\n",
    "EMBEDDING_DIM = 10\n",
    "\n",
    "def collate_batch(batch):\n",
    "    batch_size = len(batch)\n",
    "    context, target = [], []\n",
    "    for i in range(CONTEXT_SIZE, batch_size):\n",
    "        target.append(vocab([batch[i]]))\n",
    "        context.append(vocab([batch[i - j - 1] for j in range(CONTEXT_SIZE)]))\n",
    "\n",
    "    return torch.tensor(context).to(device), torch.tensor(target).to(device).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del mismo modo, es importante resaltar que el tamaño del último batch podría ser diferente al de los anteriores. Para solucionar esto, se ajusta el último batch para que cumpla con el tamaño especificado, asegurando que sea un múltiplo del tamaño predeterminado. Cuando sea necesario, se utilizarán técnicas de padding (relleno) para lograr esta homogeneidad. Una de las estrategias utilizadas es añadir el comienzo de la canción al final del batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Padding = BATCH_SIZE - len(tokens) % BATCH_SIZE\n",
    "tokens_pad = tokens + tokens[0:Padding]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el `DataLoader`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "     tokens_pad, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Redes neuronales multiclase**\n",
    "\n",
    "Hemos desarrollado una clase en PyTorch para una red neuronal multiclase. La salida de la red es la probabilidad de la siguiente palabra dentro de un contexto dado. Por ello, el número de clases corresponde al número de palabras distintas. La capa inicial consiste en embeddings y, además de la capa final, se incorpora una capa oculta adicional.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.context_size = context_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        embeds = torch.reshape(embeds, (-1, self.context_size * self.embedding_dim))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuperamos las del objeto DataLoader y las incorporamos en la red neuronal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context, target = next(iter(dataloader))\n",
    "out = modelo(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque el modelo aún no ha sido entrenado, analizar la salida nos permite entender mejor cómo funciona. En la salida, la primera dimensión corresponde al tamaño del batch, mientras que la segunda dimensión representa la probabilidad asociada a cada clase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontramos el índice con la mayor probabilidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_index = torch.argmax(out, 1)\n",
    "predicted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buscamos el token correspondiente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[index_to_token[i.item()] for i in predicted_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una función que realice la misma tarea para los tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_song(modelo, number_of_words=100):\n",
    "    mi_song = \"\"\n",
    "    for i in range(number_of_words):\n",
    "        with torch.no_grad():\n",
    "            context = torch.tensor(vocab([tokens[i - j - 1] for j in range(CONTEXT_SIZE)])).to(device)\n",
    "            word_inx = torch.argmax(modelo(context))\n",
    "            mi_song += \" \" + index_to_token[word_inx.detach().item()]\n",
    "\n",
    "    return mi_song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_song(modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento\n",
    "\n",
    "Entrenar un modelo de lenguaje involucra un proceso de múltiples pasos en el que se utilizan datos de entrenamiento y prueba para optimizar el rendimiento del modelo. En el ámbito del procesamiento de lenguaje natural (NLP), este proceso a menudo utiliza diversas métricas para evaluar la precisión del modelo, como la perplexidad o la exactitud en datos no vistos. Sin embargo, en el contexto de esta exploración, emprenderemos un camino ligeramente diferente. En lugar de depender únicamente de las métricas convencionales de NLP, el enfoque se desplaza a la inspección manual de los resultados.\n",
    "\n",
    "Contamos con la pérdida de entropía cruzada (`cross entropy loss`) entre los logits de entrada y el objetivo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos desarrollado una función dedicada a entrenar el modelo usando el DataLoader proporcionado. Además de entrenar el modelo, la función muestra predicciones para cada época, generando contexto para las siguientes 100 palabras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, modelo, number_of_epochs=100, show=10):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dataloader (DataLoader): DataLoader que contiene los datos de entrenamiento.\n",
    "        modelo (nn.Module): Modelo de red neuronal a entrenar.\n",
    "        number_of_epochs (int, opcional): Número de épocas para el entrenamiento. Por defecto es 100.\n",
    "        show (int, opcional): Intervalo para mostrar el progreso. Por defecto es 10.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista que contiene los valores de pérdida para cada época.\n",
    "    \"\"\"\n",
    "\n",
    "    MI_LOSS = []  # Lista para almacenar los valores de pérdida para cada época\n",
    "\n",
    "    # Itera sobre el número de épocas especificado\n",
    "    for epoch in tqdm(range(number_of_epochs)):\n",
    "        total_loss = 0  # Inicializa la pérdida total para la época actual\n",
    "        mi_song = \"\"    # Inicializa una cadena para almacenar la canción generada\n",
    "\n",
    "        # Itera sobre cada batch en el dataloader\n",
    "        for context, target in dataloader:\n",
    "            modelo.zero_grad()          # Pone a cero los gradientes para evitar acumulación\n",
    "            predicted = modelo(context)  # Paso forward a través del modelo para obtener predicciones\n",
    "            loss = criterion(predicted, target.reshape(-1))  # Calcula la pérdida\n",
    "            total_loss += loss.item()   # Acumula la pérdida\n",
    "\n",
    "            loss.backward()    # Backpropagation para calcular los gradientes\n",
    "            optimizer.step()   # Actualiza los parámetros del modelo usando el optimizador\n",
    "\n",
    "        # Muestra el progreso y generar una canción cada cierto intervalo\n",
    "        if epoch % show == 0:\n",
    "            mi_song += write_song(modelo)  # Genera la canción usando el modelo\n",
    "\n",
    "            print(\"Canción generada:\")\n",
    "            print(\"\\n\")\n",
    "            print(mi_song)\n",
    "\n",
    "        MI_LOSS.append(total_loss / len(dataloader))  # Agrega la pérdida total promedio de la época a la lista MY_LOSS\n",
    "\n",
    "    return MI_LOSS  # Devuelve la lista de valores promedio de pérdida por época"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente lista se utilizará para almacenar la pérdida de cada modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_loss_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este segmento de código inicializa un modelo de lenguaje n-grama con un tamaño de contexto de 2. El modelo, denominado `model_2`, se configura en base al tamaño del vocabulario, la dimensión del embedding y el tamaño del contexto proporcionados. \n",
    "\n",
    "Se utiliza el optimizador stochastic gradient descent (SGD) con una tasa de aprendizaje de 0.01 para gestionar la actualización de parámetros del modelo. Además, se configura un programador de tasa de aprendizaje (learning rate scheduler) usando un método escalonado con un factor de reducción de 0.1 por época, el cual se encarga de ajustar la tasa de aprendizaje durante el proceso de entrenamiento. \n",
    "\n",
    "Estas configuraciones establecen el marco para entrenar el modelo n-grama con optimizaciones y ajustes en la tasa de aprendizaje.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define el tamaño del contexto para el modelo n-grama\n",
    "CONTEXT_SIZE = 2\n",
    "\n",
    "# Crea una instancia de la clase NGramLanguageModeler con los parámetros especificados\n",
    "model_2 = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE).to(device)\n",
    "\n",
    "# Define el optimizador para entrenar el modelo, usando descenso de gradiente estocástico (SGD)\n",
    "optimizer = optim.SGD(model_2.parameters(), lr=0.01)\n",
    "\n",
    "# Configura un programador de tasa de aprendizaje usando StepLR para ajustar la tasa de aprendizaje durante el entrenamiento\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1.0, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, entrenamos el modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_loss = train(dataloader, model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guarda el modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '2gram.pth'\n",
    "torch.save(model_2.state_dict(), save_path)\n",
    "mi_loss_list.append(mi_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código muestra los embeddings de palabras del modelo creado, reduce su dimensionalidad a 2D usando t-SNE y luego los grafica en un diagrama de dispersión. Además, se anotan los primeros 20 puntos en la visualización con las palabras correspondientes. Esto se utiliza para visualizar cómo las palabras similares se agrupan en un espacio de menor dimensión, revelando la estructura de los embeddings. \n",
    "\n",
    "Los embeddings permiten al modelo representar las palabras en un espacio vectorial continuo, capturando relaciones y similitudes semánticas entre ellas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model_2.embeddings.weight.cpu().detach().numpy()\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_2d = tsne.fit_transform(X)\n",
    "\n",
    "labels = []\n",
    "\n",
    "for j in range(len(X_2d)):\n",
    "    if j < 20:\n",
    "        plt.scatter(X_2d[j, 0], X_2d[j, 1], label=index_to_token[j])\n",
    "        labels.append(index_to_token[j])\n",
    "        # Añade la palabra como anotación\n",
    "        plt.annotate(index_to_token[j],\n",
    "                     (X_2d[j, 0], X_2d[j, 1]),\n",
    "                     textcoords=\"offset points\",\n",
    "                     xytext=(0, 10),\n",
    "                     ha='center')\n",
    "    else:\n",
    "        plt.scatter(X_2d[j, 0], X_2d[j, 1])\n",
    "\n",
    "plt.legend(labels, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repetimos el proceso para un contexto de cuatro.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 4\n",
    "model_4 = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE).to(device)\n",
    "optimizer = optim.SGD(model_4.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "mi_loss = train(dataloader, model_4 )\n",
    "\n",
    "save_path = '4gram.pth'\n",
    "torch.save(model_4.state_dict(), save_path)\n",
    "\n",
    "mi_loss_list.append(mi_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código que se muestra a continuación presenta los embeddings de palabras del modelo creado, reduce su dimensionalidad a 2D usando t-SNE y luego los grafica en un diagrama de dispersión. Además, se anotan los primeros 20 puntos en la visualización con las palabras correspondientes. Esto se usa para visualizar cómo las palabras similares se agrupan en un espacio de menor dimensión, revelando la estructura de los embeddings. \n",
    "\n",
    "Los embeddings permiten que el modelo represente las palabras en un espacio vectorial continuo, capturando relaciones y similitudes semánticas entre ellas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model_4.embeddings.weight.cpu().detach().numpy()\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_2d = tsne.fit_transform(X)\n",
    "\n",
    "labels = []\n",
    "\n",
    "for j in range(len(X_2d)):\n",
    "    if j < 20:\n",
    "        plt.scatter(X_2d[j, 0], X_2d[j, 1], label=index_to_token[j])\n",
    "        labels.append(index_to_token[j])\n",
    "        # Añade la palabra como anotación\n",
    "        plt.annotate(index_to_token[j],\n",
    "                     (X_2d[j, 0], X_2d[j, 1]),\n",
    "                     textcoords=\"offset points\",\n",
    "                     xytext=(0, 10),\n",
    "                     ha='center')\n",
    "    else:\n",
    "        plt.scatter(X_2d[j, 0], X_2d[j, 1])\n",
    "\n",
    "plt.legend(labels, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, para un contexto de ocho.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 8\n",
    "model_8 = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE).to(device)\n",
    "optimizer = optim.SGD(model_8.parameters(), lr=0.01)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "mi_loss = train(dataloader, model_8)\n",
    "\n",
    "save_path = '8gram.pth'\n",
    "torch.save(model_8.state_dict(), save_path)\n",
    "\n",
    "mi_loss_list.append(mi_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código muestra los embeddings de palabras del modelo creado, reduce su dimensionalidad a 2D usando t-SNE y luego los grafica en un diagrama de dispersión. Además, se anotan los primeros 20 puntos de la visualización con las palabras correspondientes. Esto se usa para visualizar cómo se agrupan las palabras similares en un espacio de menor dimensión, revelando la estructura de los embeddings. \n",
    "\n",
    "Los embeddings permiten que el modelo represente las palabras en un espacio vectorial continuo, capturando relaciones y similitudes semánticas entre ellas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model_8.embeddings.weight.cpu().detach().numpy()\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_2d = tsne.fit_transform(X)\n",
    "\n",
    "labels = []\n",
    "\n",
    "for j in range(len(X_2d)):\n",
    "    if j < 20:\n",
    "        plt.scatter(X_2d[j, 0], X_2d[j, 1], label=index_to_token[j])\n",
    "        labels.append(index_to_token[j])\n",
    "        # Añade la palabra como anotación\n",
    "        plt.annotate(index_to_token[j],\n",
    "                     (X_2d[j, 0], X_2d[j, 1]),\n",
    "                     textcoords=\"offset points\",\n",
    "                     xytext=(0, 10),\n",
    "                     ha='center')\n",
    "    else:\n",
    "        plt.scatter(X_2d[j, 0], X_2d[j, 1])\n",
    "\n",
    "plt.legend(labels, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al observar la pérdida graficada para cada modelo, se evidencia que a mayor tamaño del contexto, menor es la pérdida. Aunque este enfoque no incluye validación del modelo ni utiliza métricas convencionales de NLP, la evidencia visual respalda un mejor rendimiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (mi_loss, model_name) in zip(mi_loss_list, [\"2-grama\", \"4-grama\", \"8-grama\"]):\n",
    "    plt.plot(mi_loss, label=\"Pérdida de entropía cruzada - {}\".format(model_name))\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Perplexidad**\n",
    "\n",
    "La perplexidad es una medida utilizada para evaluar la efectividad de modelos de lenguaje o modelos probabilísticos. Proporciona una indicación de qué tan bien un modelo predice una muestra de datos o la probabilidad de un evento no visto. La perplexidad es comúnmente usada en tareas de procesamiento de lenguaje natural, como la traducción automática, el reconocimiento de voz y la generación de lenguaje.\n",
    "\n",
    "La perplexidad se deriva del concepto de pérdida de entropía cruzada, que mide la disimilitud entre las probabilidades predichas y las reales.\n",
    "\n",
    "$$\\text{Pérdida de entropía cruzada} = -\\sum_{i=1}^{N} y_i \\ln(p_i)$$\n",
    "\n",
    "La pérdida de entropía cruzada se calcula tomando la suma negativa del producto de las etiquetas verdaderas $y_i$ y el logaritmo de las probabilidades predichas $p_i$ sobre $N$ clases.\n",
    "\n",
    "El cálculo del exponencial del promedio de la pérdida de entropía cruzada nos da el valor de la perplexidad.\n",
    "\n",
    "$$\\text{Perplexidad} = e^{\\frac{1}{N} \\text{Pérdida de entropía cruzada}}$$\n",
    "\n",
    "Un valor menor de perplexidad indica que el modelo es más confiado y preciso al predecir los datos. Por el contrario, una perplexidad alta sugiere que el modelo es menos certero en sus predicciones.\n",
    "\n",
    "La perplexidad puede verse como una estimación del número promedio de opciones que el modelo tiene para la siguiente palabra o evento en una secuencia. Una perplexidad menor significa que el modelo está más seguro acerca de la siguiente palabra, mientras que una perplexidad mayor implica que existen más opciones posibles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (mi_loss, model_name) in zip(mi_loss_list, [\"2-grama\", \"4-grama\", \"8-grama\"]):\n",
    "    # Calcula la perplexidad usando la pérdida\n",
    "    perplexity = np.exp(mi_loss)\n",
    "    plt.plot(perplexity, label=\"Perplexidad - {}\".format(model_name))\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "1. **Análisis de frecuencias y smoothing**  \n",
    "   - A partir de un corpus pequeño (p. ej. versos de canciones), calcula manualmente distribuciones unigramas y bigramas.  \n",
    "   - Aplica al menos dos técnicas de suavizado (Laplace, Good–Turing) y compara cómo cambian las probabilidades de eventos raros.\n",
    "\n",
    "2. **Evaluación de modelos n‑grama**  \n",
    "   - Diseña un experimento para medir la *perplexidad* de modelos de orden 1, 2 y 3 sobre un conjunto de validación.  \n",
    "   - Interpreta qué tamaño de n‑grama equilibra mejor capacidad predictiva y complejidad.\n",
    "\n",
    "3. **Selección de contexto óptimo**  \n",
    "   - Plantea una estrategia para elegir el tamaño de ventana (context size) en redes feedforward de lenguaje.  \n",
    "   - ¿Cómo influye la longitud del contexto en la calidad de las predicciones y la eficiencia del entrenamiento?\n",
    "\n",
    "4. **Comparación de arquitecturas**  \n",
    "   - Propón una prueba comparativa entre un modelo n‑grama puro y una red neuronal simple (FNN) en la tarea de completar frases.  \n",
    "   - Define métricas cuantitativas (precisión, cobertura de vocabulario) y cualitativas (fluidez, coherencia).\n",
    "\n",
    "5. **Visualización de embeddings**  \n",
    "   - Elabora un protocolo para proyectar vectores de palabras a 2D (t‑SNE o PCA).  \n",
    "   - Describe cómo interpretar agrupamientos semánticos y detectar outliers en el espacio de embedding.\n",
    "\n",
    "6. **Detección de colisiones semánticas**  \n",
    "   - Identifica pares de palabras muy frecuentes que generen ambigüedad (\"bank\", \"lead2, etc.) y diseña un análisis de contexto para desambiguarlas.  \n",
    "   - Explica cómo un modelo de mayor orden mejoraría la desambiguación.\n",
    "\n",
    "7. **Generación controlada de texto**  \n",
    "   - Plantea un método para \"sembrar\" (seed) la generación de texto con una palabra o frase inicial y evaluar la diversidad de salidas.  \n",
    "   - ¿Qué ocurre si restringes el vocabulario del modelo durante la generación?\n",
    "\n",
    "8. **Impacto del preprocesamiento**  \n",
    "   - Compara distintas estrategias de limpieza (eliminación de stop‑words, lematización, minúsculas vs. mayúsculas) y analiza su efecto sobre la cobertura del vocabulario y la entropía del modelo.\n",
    "\n",
    "9. **Análisis de errores**  \n",
    "   - Reúne ejemplos de predicciones incorrectas de tu modelo n‑grama y clasifícalas según la razón del fallo (datos escasos, ambigüedad, etc.).  \n",
    "   - Sugiere mejoras de modelado o ampliación de corpus para cada caso.\n",
    "\n",
    "10. **Proyecto integrador**  \n",
    "    - Diseña un pequeño pipeline que vaya desde el preprocesamiento hasta la evaluación final de un modelo de lenguaje (incluyendo generación, métricas y visualizaciones).  \n",
    "    - Detalla los componentes, flujos de datos y criterios de éxito para una entrega académica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "prev_pub_hash": "2f3a760070e26b6682d94eed9766f4247e3c53a584ce883caaf62ec4b5e8b61d"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
