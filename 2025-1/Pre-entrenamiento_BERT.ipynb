{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preentrenamiento de modelos BERT**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este cuaderno práctico, aprenderá a construir un modelo BERT desde cero usando PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Configuración**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Instalación de librerías requeridas**\n",
    "\n",
    "Las siguientes librerías requeridas **no** están preinstaladas. **Deberás ejecutar la siguiente celda** para instalarlas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install 'pandas==2.2.1'\n",
    "#!pip install 'portalocker>=2.0.0'\n",
    "#!pip install 'torchtext==0.16.0'\n",
    "#!pip install 'pandas==2.2.1'\n",
    "#!pip install transformers\n",
    "#!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Importación de librerías requeridas**\n",
    "\n",
    "*Se recomienda importar todas las librerías requeridas en un solo lugar (aquí):*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import Tensor\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "from transformers import BertTokenizer\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torchtext.vocab import Vocab,build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.datasets import IMDB\n",
    "import random\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "import csv\n",
    "import json\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# También puede usar esta sección para suprimir advertencias generadas por su código:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Antecedentes**\n",
    "\n",
    "#### **Introducción al preentrenamiento**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El preentrenamiento implica entrenar un modelo en un gran corpus de texto no etiquetado para capturar patrones generales del lenguaje y relaciones semánticas. Los modelos preentrenados pueden luego ajustarse finamente en tareas específicas de NLP, como análisis de sentimientos, preguntas y respuestas o traducción automática.\n",
    "\n",
    "La motivación detrás del preentrenamiento de transformers es abordar las limitaciones de los enfoques tradicionales que requieren cantidades significativas de datos etiquetados para cada tarea específica. El preentrenamiento aprovecha la abundancia de datos de texto no etiquetados disponibles en Internet y facilita el aprendizaje por transferencia, donde el conocimiento aprendido en una tarea puede transferirse para ayudar a resolver otras tareas relacionadas.\n",
    "\n",
    "Los objetivos de preentrenamiento juegan un papel crucial en el entrenamiento de transformers. Por ejemplo, el modelado de lenguaje enmascarado (MLM) implica enmascarar aleatoriamente algunas palabras en una oración y entrenar al modelo para predecir las palabras enmascaradas en función del contexto circundante. \n",
    "\n",
    "Este objetivo ayuda al modelo a aprender comprensión contextual y a completar la información faltante. Otro objetivo llamado predicción de la siguiente oración (NSP) implica predecir si dos oraciones son consecutivas o seleccionadas al azar del corpus, lo que permite al modelo aprender relaciones a nivel de oración.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Objetivos de preentrenamiento**\n",
    "\n",
    "Los objetivos de preentrenamiento son componentes cruciales del proceso de preentrenamiento para transformers. Estos objetivos definen las tareas en las que se entrena el modelo durante la fase de preentrenamiento, lo que le permite aprender representaciones contextuales significativas del lenguaje. \n",
    "\n",
    "Dos objetivos de preentrenamiento comúnmente usados son el modelado de lenguaje enmascarado (MLM) y la predicción de la siguiente oración (NSP).\n",
    "\n",
    "1. **Modelado de lenguaje enmascarado (MLM):**\n",
    "   El modelado de lenguaje enmascarado implica enmascarar aleatoriamente algunas palabras en una oración y entrenar al modelo para predecir las palabras enmascaradas en función del contexto proporcionado por las palabras circundantes (es decir, palabras que aparecen antes o después de la palabra enmascarada).\n",
    "\n",
    "   El objetivo es permitir que el modelo aprenda comprensión contextual y complete la información faltante.\n",
    "   Así es como funciona el MLM:\n",
    "\n",
    "   * Dada una oración de entrada, se selecciona aleatoriamente un porcentaje de las palabras y se reemplazan con un token especial `[MASK]`.\n",
    "   * La tarea del modelo es predecir las palabras originales que fueron enmascaradas, dadas las palabras circundantes.\n",
    "   * Durante el entrenamiento, el modelo aprende a comprender la relación entre las palabras enmascaradas y el resto de la oración, capturando efectivamente la información contextual.\n",
    "\n",
    "3. **Predicción de la siguiente oración (NSP):**\n",
    "   La predicción de la siguiente oración implica entrenar al modelo para predecir si dos oraciones son consecutivas en el texto original o si se seleccionaron al azar del corpus. Este objetivo ayuda al modelo a aprender relaciones a nivel de oración y a entender la coherencia entre oraciones.\n",
    "   Así es como funciona el NSP:\n",
    "\n",
    "   * Dado un par de oraciones, el modelo se entrena para predecir si la segunda oración sigue a la primera en el texto original o si fue seleccionada al azar del corpus.\n",
    "   * El modelo aprende a capturar las relaciones entre oraciones y a entender el flujo de información en el texto.\n",
    "\n",
    "   El NSP es particularmente útil para tareas que implican comprender la relación entre múltiples oraciones, como preguntas y respuestas o clasificación de documentos. Al entrenar al modelo para predecir la coherencia de pares de oraciones, aprende a capturar las conexiones semánticas entre ellas.\n",
    "\n",
    "*Nota: diferentes modelos preentrenados pueden utilizar variaciones o combinaciones de estos objetivos, dependiendo de la arquitectura y la configuración de entrenamiento.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preentrenamiento de un modelo BERT**\n",
    "\n",
    "El preentrenamiento de un modelo BERT (Bidirectional Encoder Representations from Transformers) es un proceso complejo y que consume mucho tiempo, que requiere un gran corpus de datos de texto no etiquetados y recursos computacionales significativos. Sin embargo, se presenta a continuación un ejercicio simplificado para demostrar los pasos involucrados en el preentrenamiento de un modelo BERT utilizando los objetivos de modelado de lenguaje enmascarado (MLM) y predicción de la siguiente oración (NSP).\n",
    "\n",
    "Se indicará:\n",
    "\n",
    "* Crear cargadores de datos de entrenamiento y prueba a partir del conjunto de datos\n",
    "* Preentrenar BERT usando una tarea de MLM\n",
    "* Preentrenar BERT usando una tarea de NSP\n",
    "* Evaluar el modelo entrenado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cargando datos**\n",
    "\n",
    "Vamos a cargar los archivos CSV creados en el cuaderno de preparación de datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# Elimina instancia anterior\n",
    "if os.path.isdir(\"bert_dataset\"):\n",
    "    shutil.rmtree(\"bert_dataset\")\n",
    "\n",
    "# Descarga\n",
    "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/bZaoQD52DcMpE7-kxwAG8A.zip\"\n",
    "zip_path = \"BERT_dataset.zip\"\n",
    "urllib.request.urlretrieve(url, zip_path)\n",
    "\n",
    "# Descomprime\n",
    "with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "    z.extractall(\"bert_dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, puedes crear un Dataset de torch usando el archivo CSV que acaba de crear:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTCSVDataset(Dataset):\n",
    "    def __init__(self, filename):\n",
    "        self.data = pd.read_csv(filename)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        try:\n",
    "            \n",
    "            bert_input = torch.tensor(json.loads(row['BERT Input']), dtype=torch.long)\n",
    "            bert_label = torch.tensor(json.loads(row['BERT Label']), dtype=torch.long)\n",
    "            segment_label = torch.tensor([int(x) for x in row['Segment Label'].split(',')], dtype=torch.long)\n",
    "            is_next = torch.tensor(row['Is Next'], dtype=torch.long)\n",
    "            original_text = row['Original Text']  # Si desea usarlo\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error al decodificar JSON en la fila {idx}: {e}\")\n",
    "            print(\"BERT Input:\", row['BERT Input'])\n",
    "            print(\"BERT Label:\", row['BERT Label'])\n",
    "            # Maneja el error, p. ej., omitiendo esta fila o usando valores predeterminados\n",
    "            return None  # o algunos valores predeterminados\n",
    "        \n",
    "        return bert_input, bert_label, segment_label, is_next  # Incluye original_text si es necesario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, crea una función `collate` que aplique transformaciones a lotes del iterador de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = 0\n",
    "def collate_batch(batch):\n",
    "    bert_inputs_batch, bert_labels_batch, segment_labels_batch, is_nexts_batch = [], [], [], []\n",
    "\n",
    "    for bert_input, bert_label, segment_label, is_next in batch:\n",
    "        # Convierte cada secuencia en un tensor y agregarla a la lista respectiva\n",
    "        bert_inputs_batch.append(torch.tensor(bert_input, dtype=torch.long))\n",
    "        bert_labels_batch.append(torch.tensor(bert_label, dtype=torch.long))\n",
    "        segment_labels_batch.append(torch.tensor(segment_label, dtype=torch.long))\n",
    "        is_nexts_batch.append(is_next)\n",
    "\n",
    "    # Rellena las secuencias en el lote\n",
    "    bert_inputs_final = pad_sequence(bert_inputs_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    bert_labels_final = pad_sequence(bert_labels_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    segment_labels_final = pad_sequence(segment_labels_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    is_nexts_batch = torch.tensor(is_nexts_batch, dtype=torch.long)\n",
    "\n",
    "    return bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando un tamaño de lote arbitrario, puede crear dataloaders de entrenamiento y prueba:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for root, dirs, files in os.walk(\"bert_dataset\"):\n",
    "    print(root, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "\n",
    "train_dataset_path = './bert_dataset/bert_dataset/bert_train_data.csv'\n",
    "test_dataset_path = './bert_dataset/bert_dataset/bert_test_data.csv'\n",
    "\n",
    "train_dataset = BERTCSVDataset(train_dataset_path)\n",
    "test_dataset = BERTCSVDataset(test_dataset_path)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creación del modelo**\n",
    "\n",
    "En BERT, el embedding posicional, el embedding de tokens y el embedding de segmentos son tres tipos de embeddings usadas para representar los tokens de entrada en el modelo.\n",
    "\n",
    "1. **Embeddings de tokens (token embedding):** el embedding de tokens es la representación inicial de cada token en un modelo BERT. Mapea cada token a un vector denso de tamaño fijo, típicamente llamado tamaño de embeddings. La capa de embeddings de tokens en BERT aprende las representaciones contextuales de los tokens de entrada. Estos embeddings capturan el significado semántico de los tokens y sus relaciones con otros tokens en el contexto.\n",
    "\n",
    "2. **Embedding posicional (positional embedding):** BERT es un modelo basado en transformers que procesa los tokens de entrada en paralelo. Sin embargo, dado que los transformers no capturan inherentemente el orden de los tokens, se usa el embedding posicional para inyectar información de posición en el modelo. Añade un vector de representación a cada token que codifica su posición en la secuencia de entrada. El embedding posicional permite a BERT entender el orden secuencial de los tokens y capturar sus posiciones relativas.\n",
    "\n",
    "3. **Embedding de segmentos (segment embedding):** BERT puede manejar pares de oraciones o secuencias con segmentos distintos. Para diferenciar entre diferentes segmentos, como oraciones o secciones de un documento, se usa el embedding de segmentos. Asigna una representación vectorial única a cada segmento o parte de la entrada. Los embeddings de segmentos ayudan a BERT a entender las relaciones entre diferentes segmentos y a capturar el contexto dentro y entre ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 10\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# Define la clase PositionalEncoding como un módulo de PyTorch para agregar información posicional a los embeddings  de tokens\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # Crea una matriz de codificación posicional según la fórmula del artículo del Transformer\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: torch.Tensor):\n",
    "        #  Aplica las codificaciones posicionales a los embeddings de tokens de entrada\n",
    "\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedding (nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_size ,dropout=0.1,train=True):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = TokenEmbedding( vocab_size,emb_size )\n",
    "        self.positional_encoding = PositionalEncoding(emb_size,dropout)\n",
    "        self.segment_embedding = nn.Embedding(3, emb_size)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, bert_inputs, segment_labels=False):\n",
    "        my_embeddings=self.token_embedding(bert_inputs)\n",
    "        if self.train:\n",
    "          x = self.dropout(my_embeddings + self.positional_encoding(my_embeddings) + self.segment_embedding(segment_labels))\n",
    "        else:\n",
    "          x = my_embeddings + self.positional_encoding(my_embeddings)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, define un modelo BERT completo con los siguientes componentes clave:\n",
    "\n",
    "1. **Inicialización:** La clase `BERT` se define como una subclase de `torch.nn.Module`. Inicializa el modelo BERT con parámetros como tamaño de vocabulario, dimensión del modelo, número de capas, número de cabezas de atención y tasa de abandono (dropout).\n",
    "\n",
    "2. **Capa de embeddings:** El modelo BERT incluye una capa de embedding que combina embeddings de tokens e embeddings de segmentos usando la clase `BERTEmbedding`.\n",
    "\n",
    "3. **Codificador Transformer:** Se usan capas de codificador Transformer para codificar los embeddings de entrada. El número de capas, cabeceras de atención, tasa de dropout y dimensión del modelo se especifican según los parámetros definidos.\n",
    "\n",
    "4. **Predicción de la siguiente oración (next sentence prediction):** El modelo tiene una capa lineal para *predicción de la siguiente oración*. Toma la salida del codificador Transformer y predice la relación entre dos oraciones consecutivas, clasificándolas en dos clases.\n",
    "\n",
    "5. **Modelado de lenguaje enmascarado (masked language modeling):** El modelo también incluye una capa lineal para modelado de lenguaje enmascarado. Predice los tokens enmascarados en la secuencia de entrada tomando la salida del codificador Transformer y realizando predicciones a lo largo del vocabulario.\n",
    "\n",
    "6. **Paso hacia adelante (forward pass):** El método `forward` define el paso hacia adelante del modelo BERT. Toma tokens de entrada (`bert_inputs`) y etiquetas de segmentos (`segment_labels`) y devuelve predicciones para las tareas de *predicción de la siguiente oración* y *modelado de lenguaje enmascarado*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando un ejemplo de entrada, analicemos el embeddings en sus tres componentes esenciales: embeddings de token, codificación posicional e embeddings de segmento para comprender el proceso:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE=147161\n",
    "batch = 2\n",
    "count = 0\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# carga lotes de muestra del dataloader\n",
    "for batch in train_dataloader:\n",
    "    bert_inputs, bert_labels, segment_labels, is_nexts = [b.to(device) for b in batch]\n",
    "    count += 1\n",
    "    if count == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecciona una entrada de muestra\n",
    "bert_inputs[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_labels[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancia el TokenEmbedding \n",
    "token_embedding = TokenEmbedding(VOCAB_SIZE, emb_size=EMBEDDING_DIM )\n",
    "\n",
    "#los embeddings de tokens para una entrada de muestra\n",
    "t_embeddings = token_embedding(bert_inputs)\n",
    "# Cada token se transforma en un tensor de tamaño emb_size\n",
    "print(f\"Dimension de embeddings de token: {t_embeddings.size()}\") # Esperado: (sequence_length, batch_size, EMBEDDING_DIM)\n",
    "# Verifica los vectores de embeddings para los primeros 3 tokens de la primera muestra del lote\n",
    "# se obtienen embeddings[i,0,:] donde i se refiere al i-ésimo token de la primera muestra en el lote (b=0)\n",
    "for i in range(3):\n",
    "    print(f\"Embeddings de tokens para {i}-ésimo token de la primera muestra: {t_embeddings[i,0,:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positional_encoding = PositionalEncoding(emb_size=EMBEDDING_DIM,dropout=0)\n",
    "\n",
    "# Aplica codificación posicional a los embeddings de tokens\n",
    "p_embedding = positional_encoding(t_embeddings)\n",
    "\n",
    "print(f\"Dimension de los tokens codificados posicionalmente: {p_embedding.size()}\")# Esperado: (sequence_length, batch_size, EMBEDDING_DIM)\n",
    "# Verifica los vectores codificados posicionalmente para los primeros 3 tokens de la primera muestra del lote\n",
    "# se obtienen encoded_tokens[i,0,:] donde i se refiere al i-ésimo token de la primera muestra (b=0) en el lote\n",
    "for i in range(3):\n",
    "    print(f\"Embedding posicional para el {i}-ésimo token de la primera muestra: {p_embedding[i,0,:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_embedding = nn.Embedding(3, EMBEDDING_DIM)\n",
    "s_embedding = segment_embedding(segment_labels)\n",
    "print(f\"Dimension de embeddings de segmento: {s_embedding.size()}\")# Esperado: (sequence_length, batch_size, EMBEDDING_DIM)\n",
    "# Verifica los vectores de embeddings de segmento para los primeros 3 tokens de la primera muestra del lote\n",
    "# se obtienen segment_embedded[i,0,:] donde i se refiere al i-ésimo token de la primera muestra (b=0) en el lote\n",
    "for i in range(3):\n",
    "    print(f\"Embedding de segmento para el {i}-ésimo token de la primera muestra: {s_embedding[i,0,:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crea los vectores de embeddings combinados\n",
    "bert_embeddings = t_embeddings + p_embedding + s_embedding\n",
    "print(f\"Dimension de token + posicion + token de segmento codificado : {bert_embeddings.size()}\")\n",
    "# Verifica los vectores de embeddings BERT para los primeros 3 tokens de la primera muestra del lote\n",
    "# se obtienen bert_embeddings[i,0,:] donde i se refiere al i-ésimo token de la primera muestra (b=0) en el lote\n",
    "for i in range(3):\n",
    "    print(f\"BERT_Embedding para el {i}-ésimo token: {bert_embeddings[i,0,:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model=768, n_layers=12, heads=12, dropout=0.1):\n",
    "        \"\"\"\n",
    "        vocab_size: Tamaño del vocabulario.\n",
    "        d_model: Tamaño de los embeddings (hidden size).\n",
    "        n_layers: Número de capas del Transformer.\n",
    "        heads: Número de cabeceras de atención en cada capa del Transformer.\n",
    "        dropout: Tasa de dropout aplicada a los embeddings y a las capas del Transformer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.heads = heads\n",
    "\n",
    "        # Capa de embeddings que combina embeddings de tokens e embeddings de segmentos\n",
    "        self.bert_embedding = BERTEmbedding(vocab_size, d_model, dropout)\n",
    "\n",
    "        # Capas del codificador Transformer\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=heads, dropout=dropout, batch_first=False\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.encoder_layer, num_layers=n_layers\n",
    "        )\n",
    "\n",
    "        # Capa lineal para NSP (next sentence prediction)\n",
    "        self.nextsentenceprediction = nn.Linear(d_model, 2)\n",
    "\n",
    "        # Capa lineal para MLM (masked language modeling)\n",
    "        self.masked_language = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, bert_inputs, segment_labels):\n",
    "        \"\"\"\n",
    "        bert_inputs: Tokens de entrada.\n",
    "        segment_labels: IDs de segmento para distinguir los distintos segmentos en la entrada.\n",
    "        mask: Máscara de atención para evitar que el modelo atienda a tokens de padding.\n",
    "\n",
    "        return: Predicciones para la tarea de Next Sentence Prediction y para la tarea de Masked Language Modeling.\n",
    "        \"\"\"\n",
    "        padding_mask = (bert_inputs == PAD_IDX).transpose(0, 1)\n",
    "        # Genera embeddings a partir de tokens de entrada y etiquetas de segmento\n",
    "        my_bert_embedding = self.bert_embedding(bert_inputs, segment_labels)\n",
    "\n",
    "        # Pasa embeddings por el codificador Transformer\n",
    "        transformer_encoder_output = self.transformer_encoder(\n",
    "            my_bert_embedding, src_key_padding_mask=padding_mask\n",
    "        )\n",
    "\n",
    "        next_sentence_prediction = self.nextsentenceprediction(\n",
    "            transformer_encoder_output[0, :]\n",
    "        )\n",
    "        \n",
    "        # Modelado de lenguaje enmascarado: predecir todos los tokens en la secuencia\n",
    "        masked_language = self.masked_language(transformer_encoder_output)\n",
    "\n",
    "        return next_sentence_prediction, masked_language\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una instancia del modelo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 10\n",
    "\n",
    "# Define parámetros\n",
    "vocab_size = 147161  # Reemplaza VOCAB_SIZE con el tamaño de tu vocabulario\n",
    "d_model = EMBEDDING_DIM  # Reemplaza EMBEDDING_DIM con la dimensión de tu embeddings\n",
    "n_layers = 2  # Número de capas del Transformer\n",
    "initial_heads = 12  # Número inicial de cabeceras de atención\n",
    "initial_heads = 2\n",
    "# Asegura de que el número de cabeceras sea un factor de la dimensión del embedding\n",
    "heads = initial_heads - d_model % initial_heads\n",
    "\n",
    "dropout = 0.1  # Tasa de dropout\n",
    "\n",
    "# Crea una instancia del modelo BERT\n",
    "modelo = BERT(vocab_size, d_model, n_layers, heads, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_mask = (bert_inputs == PAD_IDX).transpose(0, 1)\n",
    "padding_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=heads, dropout=dropout,batch_first=False)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "# Pasa los embeddings a través del codificador Transformer\n",
    "transformer_encoder_output = transformer_encoder(bert_embeddings,src_key_padding_mask=padding_mask)\n",
    "transformer_encoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nextsentenceprediction = nn.Linear(d_model, 2)\n",
    "nsp = nextsentenceprediction(transformer_encoder_output[ 0,:])\n",
    "#logits para la tarea NSP\n",
    "print(f\"Forma de la salida NSP: {nsp.shape}\")  # Forma esperada: (batch_size, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_language = nn.Linear(d_model, vocab_size)\n",
    "# Modelado de lenguaje enmascarado: Predecir todos los tokens en la secuencia\n",
    "mlm = masked_language(transformer_encoder_output)\n",
    "#logits para tareas MLM\n",
    "print(f\"Forma de salida MLM: {mlm.shape}\")  # Forma esperada: (seq_length, batch_size, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluación**\n",
    "\n",
    "Después de crear el modelo BERT, el siguiente paso es entrenarlo y evaluar su desempeño. Para facilitar esto, se define una función `evaluate` con los siguientes pasos:\n",
    "\n",
    "1. **Función de pérdida**: Se define la función `CrossEntropyLoss` para calcular la pérdida entre los valores predichos y reales.\n",
    "2. **Argumentos de la función**: La función recibe argumentos incluyendo el dataloader, el modelo, la función de pérdida y el dispositivo.\n",
    "3. **Modo de evaluación**: El modelo BERT se pone en modo evaluación usando `modelo.eval()`, deshabilitando dropout y comportamientos específicos de entrenamiento. Se inicializan variables para rastrear la pérdida total, la pérdida total de siguiente oración, la pérdida total de máscara y el número total de lotes.\n",
    "4. **Bucle de evaluación**: La función itera sobre los lotes proporcionados por el dataloader.\n",
    "5. **Paso hacia adelante**: Se realiza un forward pass con el modelo BERT para obtener predicciones para las tareas de siguiente oración y modelado de lenguaje enmascarado.\n",
    "6. **Cálculo de la pérdida**: Se calculan las pérdidas para las tareas de siguiente oración y lenguaje enmascarado, y luego se suman para obtener la pérdida total del lote.\n",
    "7. **Cálculo de la pérdida promedio**: Se calcula la pérdida promedio, la pérdida promedio de siguiente oración y la pérdida promedio de máscara dividiendo las pérdidas totales por el número total de lotes.\n",
    "\n",
    "La función `evaluate` se usa no solo para evaluar el desempeño del modelo BERT, sino también durante la fase de entrenamiento para valorar el progreso del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX=0\n",
    "## La función de pérdida debe ignorar los tokens PAD y calcular la pérdida solo para los tokens enmascarados\n",
    "loss_fn_mlm = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "loss_fn_nsp = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "modelo.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader=test_dataloader, modelo=modelo, loss_fn_mlm=loss_fn_mlm, loss_fn_nsp=loss_fn_nsp, device=device):\n",
    "    modelo.eval()  # Desactiva dropout y otros comportamientos específicos de entrenamiento\n",
    "\n",
    "    total_loss = 0\n",
    "    total_next_sentence_loss = 0\n",
    "    total_mask_loss = 0\n",
    "    total_batches = 0\n",
    "    with torch.no_grad():  # Desactiva gradientes para la validación, ahorra memoria y cómputo\n",
    "        for batch in dataloader:\n",
    "            bert_inputs, bert_labels, segment_labels, is_nexts = [b.to(device) for b in batch]\n",
    "\n",
    "            # Paso hacia adelante\n",
    "            next_sentence_prediction, masked_language = modelo(bert_inputs, segment_labels)\n",
    "\n",
    "            # Calcula la pérdida para la predicción de la siguiente oración\n",
    "            # Asegura de que is_nexts tenga la forma correcta para CrossEntropyLoss\n",
    "            next_loss = loss_fn_nsp(next_sentence_prediction, is_nexts.view(-1))\n",
    "\n",
    "            # Calcula la pérdida para predecir los tokens enmascarados\n",
    "            # Aplana tanto las predicciones masked_language como bert_labels para cumplir con los requisitos de entrada de CrossEntropyLoss\n",
    "            mask_loss = loss_fn_mlm(masked_language.view(-1, masked_language.size(-1)), bert_labels.view(-1))\n",
    "\n",
    "            # Suma las dos pérdidas\n",
    "            loss = next_loss + mask_loss\n",
    "            if torch.isnan(loss):\n",
    "                continue\n",
    "            else:\n",
    "                total_loss += loss.item()\n",
    "                total_next_sentence_loss += next_loss.item()\n",
    "                total_mask_loss += mask_loss.item()\n",
    "                total_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / (total_batches + 1)\n",
    "    avg_next_sentence_loss = total_next_sentence_loss / (total_batches + 1)\n",
    "    avg_mask_loss = total_mask_loss / (total_batches + 1)\n",
    "\n",
    "    print(f\"Pérdida promedio: {avg_loss:.4f}, Pérdida promedio de la siguiente oración: {avg_next_sentence_loss:.4f}, Pérdida promedio de la máscara: {avg_mask_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Entrenamiento**\n",
    "\n",
    "El proceso de entrenamiento del modelo BERT implica los siguientes pasos:\n",
    "\n",
    "1. **Definición del optimizador**: Antes de comenzar el entrenamiento, se define un optimizador para entrenar el modelo BERT. En este caso, se utiliza el optimizador Adam.\n",
    "2. **Bucle de entrenamiento**: Dentro de cada época, se iteran los datos de entrenamiento en lotes.\n",
    "3. **Paso hacia adelante**: Para cada lote, se realiza un forward pass donde el modelo BERT predice la tarea de siguiente oración y la de lenguaje enmascarado.\n",
    "4. **Cálculo de la pérdida y actualización de parámetros**: Se calcula la pérdida en función de los valores predichos y reales. Luego, los parámetros del modelo se actualizan mediante backpropagation y recorte de gradientes.\n",
    "5. **Evaluación por época**: Después de cada época, se imprime la pérdida de entrenamiento promedio. Se evalúa el desempeño del modelo en el conjunto de prueba. Además, el modelo se guarda después de cada época.\n",
    "\n",
    "Estos pasos se repiten durante múltiples épocas para entrenar el modelo BERT y monitorear su progreso a lo largo del tiempo.\n",
    "\n",
    "\n",
    "**NOTA: El DataLoader actual es bastante grande y tomará varias horas entrenar el modelo con un conjunto de datos tan grande. Por lo tanto, a continuación está el conjunto de datos muestreado aleatoriamente (que es relativamente pequeño y aún tarda de 1 a 2 horas en ejecutarse) de IMDB para acelerar el proceso. Si deseas entrenar el modelo con el conjunto de datos completo, omite la siguiente celda y ejecuta directamente la celda de entrenamiento.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 3\n",
    "\n",
    "train_dataset_path = './bert_dataset/bert_dataset/bert_train_data_sampled.csv'\n",
    "test_dataset_path = './bert_dataset/bert_dataset/bert_test_data_sampled.csv'\n",
    "\n",
    "train_dataset = BERTCSVDataset(train_dataset_path)\n",
    "test_dataset = BERTCSVDataset(test_dataset_path)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define el optimizador\n",
    "optimizer = Adam(modelo.parameters(), lr=1e-4, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "\n",
    "# Configuración del bucle de entrenamiento\n",
    "num_epochs = 1\n",
    "total_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "# Define el número de pasos de calentamiento (warmup), p. ej., el 10 % del total\n",
    "warmup_steps = int(total_steps * 0.1)\n",
    "\n",
    "# Crea el programador de tasa de aprendizaje (learning rate scheduler)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=warmup_steps,\n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "# Listas para almacenar las pérdidas para graficar\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Épocas de entrenamiento\"):\n",
    "    modelo.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoca {epoch + 1}\")):\n",
    "        bert_inputs, bert_labels, segment_labels, is_nexts = [b.to(device) for b in batch]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        next_sentence_prediction, masked_language = modelo(bert_inputs, segment_labels)\n",
    "\n",
    "        next_loss = loss_fn_nsp(next_sentence_prediction, is_nexts)\n",
    "        mask_loss = loss_fn_mlm(masked_language.view(-1, masked_language.size(-1)), bert_labels.view(-1))\n",
    "\n",
    "        loss = next_loss + mask_loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(modelo.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Actualiza la tasa de aprendizaje\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            continue\n",
    "        else:\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader) + 1\n",
    "    train_losses.append(avg_train_loss)\n",
    "    print(f\"Epoca {epoch+1} - Pérdida de entrenamiento promedio: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Evaluación tras cada época\n",
    "    eval_loss = evaluate(test_dataloader, modelo, loss_fn_nsp, loss_fn_mlm, device)\n",
    "    eval_losses.append(eval_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A continuación se muestra un gráfico de pérdida versus época; ejecuta el código anterior para más de una época para obtener un gráfico (actualmente, num_epochs está configurado en 1).**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafica los valores de pérdida por época\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Pérdida de Entrenamiento')\n",
    "plt.plot(range(1, num_epochs + 1), eval_losses, label='Pérdida de Evaluación')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.title('Pérdida de entrenamiento y evaluación')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Inferencia**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForPreTraining \n",
    "# 1) Carga el tokenizer y ajusta vocab_size dinámicamente\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "vocab_size = tokenizer.vocab_size  # 30522\n",
    "\n",
    "# 2) Instancia el modelo con el vocab_size correcto\n",
    "modelo = BertForPreTraining.from_pretrained('bert-base-uncased') \n",
    "\n",
    "# 3) Mueve a dispositivo y pásalo a modo evaluación\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "modelo.to(device)\n",
    "modelo.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evaluar el rendimiento de un modelo BERT preentrenado en la tarea de predecir si una segunda oración sigue a la primera (Next Sentence Prediction, NSP), se define la función `predict_nsp`. La función opera de la siguiente manera:\n",
    "\n",
    "1. **Tokenización**: Las oraciones de entrada se tokenizan usando `tokenizer.encode_plus`, que devuelve un diccionario con las entradas tokenizadas. Estas entradas se convierten en tensores y se envían al dispositivo adecuado para el procesamiento.\n",
    "2. **Predicción**: Se utiliza el modelo BERT para hacer predicciones pasando los tensores de tokens y de segmentos como entrada.\n",
    "3. **Manipulación de logits**: Se selecciona el primer elemento del tensor de logits y se añade una dimensión extra, obteniendo una forma `[1, 2]`.\n",
    "4. **Probabilidad y predicción**: Los logits se pasan por una función softmax para obtener probabilidades, y la predicción se obtiene con `argmax`.\n",
    "5. **Interpretación del resultado**: La predicción se interpreta y se devuelve como una cadena, indicando si la segunda oración sigue o no a la primera.\n",
    "6. **Ejemplo de uso**: Se muestra cómo pasar dos oraciones de ejemplo a la función `predict_nsp` junto con el modelo y el tokenizador. El resultado se imprime, indicando si la segunda oración sigue a la primera según la predicción del modelo.\n",
    "\n",
    "Al utilizar `predict_nsp`, puedes evaluar el desempeño del modelo BERT preentrenado en determinar la relación entre dos oraciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nsp(sentence1, sentence2, modelo, tokenizer):\n",
    "    tokens = tokenizer.encode_plus(sentence1, sentence2, return_tensors=\"pt\")\n",
    "    ids = tokens[\"input_ids\"].to(device)\n",
    "    types = tokens[\"token_type_ids\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = modelo(input_ids=ids, token_type_ids=types)\n",
    "        # outputs.seq_relationship_logits es el tensor NSP [batch_size, 2]\n",
    "        logits = torch.softmax(outputs.seq_relationship_logits, dim=-1)\n",
    "        pred = torch.argmax(logits, dim=-1).item()\n",
    "    return \"La segunda oración sigue a la primera\" if pred == 1 else \"La segunda oración no sigue a la primera\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso\n",
    "sentence1 = \"The cat sat on the mat.\"\n",
    "sentence2 = \"It was a sunny day\"\n",
    "\n",
    "print(predict_nsp(sentence1, sentence2, modelo, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se define una función para realizar *Masked Language Modeling* (MLM) usando un modelo BERT preentrenado. La función opera de la siguiente manera:\n",
    "\n",
    "1. **Tokenización**: La oración de entrada se tokeniza con el tokenizador y se convierte en IDs de tokens, incluyendo los tokens especiales. La oración tokenizada se guarda en `tokens_tensor`.\n",
    "2. **Etiquetas de segmentos**: Se crean etiquetas de segmento dummy llenas de ceros y se guardan en `segment_labels`.\n",
    "3. **Predicción**: Se pasa `tokens_tensor` y `segment_labels` por el modelo BERT para extraer los logits de MLM como `predictions`.\n",
    "4. **Índice del token enmascarado**: Se identifica la posición del token `[MASK]` usando `nonzero` y se guarda en `mask_token_index`.\n",
    "5. **Índice predicho**: Se obtiene el índice predicho para el token `[MASK]` haciendo `argmax` sobre los logits de MLM en la posición correspondiente.\n",
    "6. **Conversión a token**: El índice predicho se convierte de nuevo a un token con `convert_ids_to_tokens`.\n",
    "7. **Oración reemplazada**: Se reemplaza el token `[MASK]` en la oración original con el token predicho, resultando en la oración final predicha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mlm(sentence, modelo, tokenizer):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    ids = inputs.input_ids.to(device)\n",
    "    mask_token_index = (ids == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
    "    with torch.no_grad():\n",
    "        outputs = modelo(input_ids=ids)\n",
    "        # outputs.prediction_logits es el tensor MLM [batch_size, seq_len, vocab_size]\n",
    "        logits = outputs.prediction_logits\n",
    "        mask_logits = logits[0, mask_token_index, :]\n",
    "        pred_id = torch.argmax(mask_logits, dim=-1).item()\n",
    "        pred_token = tokenizer.convert_ids_to_tokens(pred_id)\n",
    "    return sentence.replace(tokenizer.mask_token, pred_token, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplos\n",
    "print(predict_nsp(\"The cat sat on the mat.\", \"It was a sunny day\", modelo, tokenizer))\n",
    "print(predict_mlm(\"The cat sat on the [MASK].\", modelo, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicios**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejercicio 1: Predicción de la próxima oración (NSP) con BERT**\n",
    "\n",
    "1. **Cargar el modelo preentrenado de BERT**: Importa `BertForPreTraining` y `BertTokenizer` desde `transformers` y cargar el modelo preentrenado y el tokenizador `bert-base-uncased`.\n",
    "2. **Preparar la entrada de texto**: Codifica un par de oraciones utilizando el tokenizador cargado.\n",
    "3. **Ejecutar NSP**: Pasa la entrada codificada a través del modelo e interpretar `seq_relationship_logits` para determinar si el modelo predice las oraciones como consecutivas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForPreTraining, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Escribe tu codigo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejercicio 2: Modelado de lenguaje enmascarado (MLM) con BERT**\n",
    "\n",
    "1. **Inicializa el modelo y el tokenizador**:\n",
    "\n",
    "Carga `BertForPreTraining` y `BertTokenizer` desde la librería `transformers` usando el modelo `bert-base-uncased`.\n",
    "\n",
    "2. **Prepara la oración enmascarada**:\n",
    "\n",
    "Escribe una oración y reemplazar una palabra por `[MASK]`. Por ejemplo, \"The capital of France is [MASK].\"\n",
    "\n",
    "3. **Tokenizar y predecir**:\n",
    "\n",
    "Tokeniza la oración enmascarada con `BertTokenizer`. Luego, introducirla en `BertForPreTraining` y usar `prediction_logits` para encontrar el token más probable que se ajuste a la máscara.\n",
    "\n",
    "4. **Mostrar la predicción**:\n",
    "\n",
    "Convierte el ID del token predicho a una cadena de tokens e imprimir la palabra predicha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForPreTraining, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Escribe tu codigo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "prev_pub_hash": "f81e7c06d2b733d2f221088b8dc36a4290af8f068916988a507edf37176e5278"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
