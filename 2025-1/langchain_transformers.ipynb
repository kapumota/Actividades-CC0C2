{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Introducción a LangChain**\n",
    "\n",
    "LangChain es una librería de alto nivel diseñada para facilitar la construcción de aplicaciones impulsadas por modelos de lenguaje (LLM), proporcionando abstracciones sobre prompts, cadenas (Chains), agentes y herramientas. \n",
    "\n",
    "Su objetivo es ofrecer componentes reutilizables que orquesten flujos de llamada a LLM, gestión de contexto, almacenamiento de estados conversacionales y encadenamiento de tareas complejas sin tener que lidiar con detalles de bajo nivel de la API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hCcdzj9WQCOc",
    "outputId": "31c7e834-9438-4509-91c9-e18a9e145923"
   },
   "outputs": [],
   "source": [
    "!pip install transformers langchain accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Agentes LangChain  y herramientas**\n",
    "\n",
    "Los **Agents** en LangChain combinan LLMs con \"herramientas\" externas (por ejemplo, buscadores Web, bases de datos, funciones personalizadas). \n",
    "\n",
    "Funcionan así: el agente decide, a partir de un prompt y su memoria, qué herramienta invocar y con qué argumentos; tras ejecutar la herramienta, incorpora la respuesta al contexto y continúa el flujo. \n",
    "\n",
    "Esto permite construir asistentes que responden de manera dinámica, consultan APIs o ejecutan código.\n",
    "\n",
    "El patrón general para una aplicación con LangChain incluye:\n",
    "\n",
    "1. **PromptTemplate**: definiciones parametrizadas de prompts.\n",
    "2. **LLMChain**: encadena el prompt con la llamada al modelo y procesa la respuesta.\n",
    "3. **Memory** (opcional): guarda fragmentos de la conversación o metadatos.\n",
    "4. **Agents & Tools** (opcional): permite reactividad a eventos o consultas externas.\n",
    "\n",
    "**Creación de un resumidor de artículos de noticias**\n",
    "\n",
    "Para un resumen de noticias con LangChain:\n",
    "\n",
    "- `TextSplitter`: dividir artículos largos en segmentos manejables.\n",
    "- `PromptTemplate`: crear plantillas como \"Resume este fragmento en 2-3 oraciones.\"\n",
    "- `LLMChain:` iterar sobre cada fragmento y agregar los subtítulos.\n",
    "- `OutputParser:` combinar cada subtotal en un resumen coherente, filtrando duplicados y conectores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejemplo**\n",
    "\n",
    "Invocamos el pipeline de generación de texto y, dado que distintas versiones de pipelines de Transformers pueden devolver formatos diferentes (una lista de cadenas, una lista de diccionarios, un único diccionario o incluso una cadena suelta), incluye una pequeña rutina de detección para extraer siempre el texto generado de forma segura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = generate_text(\n",
    "    \"En este cuaderno, discutiremos los primeros pasos con IA generativa en Python.\",\n",
    "    max_new_tokens=50,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "# Determina dónde está el texto generado\n",
    "if isinstance(out, list):\n",
    "    # Si es lista de strings\n",
    "    if isinstance(out[0], str):\n",
    "        texto_generado = out[0]\n",
    "    # Si fuera lista de dicts (otras pipelines)\n",
    "    elif isinstance(out[0], dict):\n",
    "        texto_generado = out[0].get(\"generated_text\") or out[0].get(\"generated_sequence\")\n",
    "    else:\n",
    "        texto_generado = str(out[0])\n",
    "else:\n",
    "    # Si devuelve directamente un string o dict\n",
    "    if isinstance(out, dict):\n",
    "        texto_generado = out.get(\"generated_text\") or out.get(\"generated_sequence\") or str(out)\n",
    "    else:\n",
    "        texto_generado = str(out)\n",
    "\n",
    "print(texto_generado)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYAn7lQdYpa9"
   },
   "source": [
    "**Ejemplo 1: Generación de una historia interactiva**\n",
    "\n",
    "Este ejemplo genera una historia interactiva donde el lector puede elegir diferentes opciones y el modelo continuará la historia según la elección del lector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bfqyn3fOYvUF",
    "outputId": "17048a3d-8c0d-4302-bce9-2294d44fd6ba"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# 1. Detectae el dispositivo y tipo de dato óptimo\n",
    "if torch.cuda.is_available():\n",
    "    dispositivo = 0                     # usa GPU 0\n",
    "    dtype = torch.bfloat16             # bfloat16 en GPU para velocidad y menor uso de memoria\n",
    "else:\n",
    "    dispositivo = -1                   # usa CPU\n",
    "    dtype = torch.float32              # float32 en CPU para evitar conversiones lentas\n",
    "\n",
    "# 2. Crea el pipeline de generación sólo una vez\n",
    "generate_text = pipeline(\n",
    "    \"text-generation\",                 \n",
    "    model=\"aisquared/dlite-v1-355m\",   \n",
    "    torch_dtype=dtype,                 \n",
    "    trust_remote_code=True,            \n",
    "    device=dispositivo,                \n",
    ")\n",
    "\n",
    "# 3. Warm-up: primera llamada rápida para cache interno\n",
    "_ = generate_text(\"Hola\")\n",
    "\n",
    "# 4. Función optimizada para continuar la historia\n",
    "def continue_story(prompt: str, choice: str) -> str:\n",
    "    \"\"\"\n",
    "    Genera la siguiente parte de la historia combinando el prompt y la elección del usuario.\n",
    "    - max_new_tokens: límite bajo para tokens nuevos (reduce tiempo de generación)\n",
    "    - do_sample=False: decodificación greedy (más rápida y determinista)\n",
    "    \"\"\"\n",
    "    salida = generate_text(\n",
    "        f\"{prompt} {choice}\",\n",
    "        max_new_tokens=60,\n",
    "        do_sample=False\n",
    "    )\n",
    "    # Algunos pipelines devuelven lista de strings\n",
    "    if isinstance(salida, list) and isinstance(salida[0], str):\n",
    "        return salida[0]\n",
    "    # Otros devuelven lista de dicts con key \"generated_text\" o \"generated_sequence\"\n",
    "    if isinstance(salida, list) and isinstance(salida[0], dict):\n",
    "        return salida[0].get(\"generated_text\") or salida[0].get(\"generated_sequence\", \"\")\n",
    "    # Fallback: cast a str\n",
    "    return str(salida)\n",
    "\n",
    "# 5. Flujo interactivo con comentarios en español\n",
    "story_prompt = (\n",
    "    \"Te encuentras en un bosque oscuro. Delante de ti hay dos caminos. \"\n",
    "    \"¿Tomas el camino izquierdo o el camino derecho?\"\n",
    ")\n",
    "print(story_prompt)\n",
    "\n",
    "choice1 = input(\"Ingresa tu elección (izquierdo/derecho): \").strip().lower()\n",
    "story1 = continue_story(story_prompt, choice1)\n",
    "print(story1)\n",
    "\n",
    "story_prompt2 = (\n",
    "    story1 +\n",
    "    \"\\nTe topas con una figura misteriosa. ¿Te acercas a la figura o te escondes detrás de un árbol?\"\n",
    ")\n",
    "choice2 = input(\"Ingresa tu elección (acercarse/esconderse): \").strip().lower()\n",
    "story2 = continue_story(story1, choice2)\n",
    "print(story2)\n",
    "\n",
    "story_prompt3 = (\n",
    "    story2 +\n",
    "    \"\\nLa figura te ofrece un objeto mágico. ¿Lo aceptas o lo rechazas?\"\n",
    ")\n",
    "choice3 = input(\"Ingresa tu elección (aceptar/rechazar): \").strip().lower()\n",
    "story3 = continue_story(story2, choice3)\n",
    "print(story3)\n",
    "\n",
    "story_prompt4 = (\n",
    "    story3 +\n",
    "    \"\\nCon el objeto mágico en tu mano, sientes un poder. ¿Lo usas ahora o lo guardas para más tarde?\"\n",
    ")\n",
    "choice4 = input(\"Ingresa tu elección (usar/guardar): \").strip().lower()\n",
    "story4 = continue_story(story3, choice4)\n",
    "print(story4)\n",
    "\n",
    "story_prompt5 = (\n",
    "    story4 +\n",
    "    \"\\nTe encuentras con un dragón feroz. ¿Luchas contra él o intentas comunicarte?\"\n",
    ")\n",
    "choice5 = input(\"Ingresa tu elección (luchar/comunicar): \").strip().lower()\n",
    "story5 = continue_story(story4, choice5)\n",
    "print(story5)\n",
    "\n",
    "print(\"\\nLa historia termina aquí. ¡Gracias por jugar!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FuoWUFRPtPzX"
   },
   "source": [
    "**Ejemplo 2: Generación de un documento técnico**\n",
    "\n",
    "Este ejemplo utiliza el modelo para generar un documento técnico sobre inteligencia artificial, dividiendo el contenido en secciones específicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 966
    },
    "id": "7Q7t_a67YzbV",
    "outputId": "19caa089-bad5-440a-ce3b-be0fe8fc127c"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# 1. Detecta el dispositivo y tipo de dato óptimo\n",
    "if torch.cuda.is_available():\n",
    "    dispositivo = 0                     # usa GPU 0\n",
    "    dtype = torch.bfloat16             # bfloat16 en GPU para velocidad y menor uso de memoria\n",
    "else:\n",
    "    dispositivo = -1                   # usa CPU\n",
    "    dtype = torch.float32              # float32 en CPU para evitar conversiones lentas\n",
    "\n",
    "# 2. Crea el pipeline de generación de texto una sola vez\n",
    "generate_text = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"aisquared/dlite-v1-355m\",\n",
    "    torch_dtype=dtype,\n",
    "    trust_remote_code=True,\n",
    "    device=dispositivo,\n",
    ")\n",
    "\n",
    "# 3. Warm-up: llama brevemente para inicializar caches internas\n",
    "_ = generate_text(\"Hola\")\n",
    "\n",
    "def _extract_text(output) -> str:\n",
    "    \"\"\"\n",
    "    Función interna para extraer el texto generado\n",
    "    Soporta lista de strings, lista de dicts o un string directo.\n",
    "    \"\"\"\n",
    "    if isinstance(output, list):\n",
    "        first = output[0]\n",
    "        if isinstance(first, str):\n",
    "            return first\n",
    "        if isinstance(first, dict):\n",
    "            return first.get(\"generated_text\") or first.get(\"generated_sequence\", \"\")\n",
    "    if isinstance(output, dict):\n",
    "        return output.get(\"generated_text\") or output.get(\"generated_sequence\", \"\")\n",
    "    return str(output)\n",
    "\n",
    "# 4. Función optimizada para generar secciones del documento\n",
    "def generate_section(title: str) -> str:\n",
    "    \"\"\"\n",
    "    Genera una sección detallada sobre el título dado,\n",
    "    usando un prompt contextualizado en IA.\n",
    "    \"\"\"\n",
    "    prompt = f\"Escriba una sección detallada sobre {title} en el contexto de inteligencia artificial.\"\n",
    "    out = generate_text(\n",
    "        prompt,\n",
    "        max_new_tokens=100,   # límite de tokens a generar por sección\n",
    "        do_sample=False        # decodificación greedy (más rápida y determinista)\n",
    "    )\n",
    "    return _extract_text(out)\n",
    "\n",
    "# 5. Generación del documento con comentarios en español\n",
    "title = \"Una guía completa sobre la inteligencia artificial\"\n",
    "print(title)\n",
    "print(\"=\" * len(title))\n",
    "\n",
    "for section_title in [\n",
    "    \"Introduction\",\n",
    "    \"History of Artificial Intelligence\",\n",
    "    \"Machine Learning\",\n",
    "    \"Deep Learning\",\n",
    "    \"Applications of AI\",\n",
    "    \"Future of AI\"\n",
    "]:\n",
    "    print(f\"\\n{section_title}\")\n",
    "    print(\"-\" * len(section_title))\n",
    "    texto = generate_section(section_title)\n",
    "    print(texto)\n",
    "\n",
    "# 6. Sección de conclusión\n",
    "print(\"\\nConclusion\")\n",
    "print(\"-\" * len(\"Conclusion\"))\n",
    "out_conclusion = generate_text(\n",
    "    \"Escriba una conclusión para una guía completa sobre inteligencia artificial.\",\n",
    "    max_new_tokens=80,\n",
    "    do_sample=False\n",
    ")\n",
    "print(_extract_text(out_conclusion))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_mf8qny4te_"
   },
   "source": [
    "**Ejemplo 3: Explicaciones y ejemplos de funciones matemáticas**\n",
    "\n",
    "Este ejemplo proporcionará una explicación de varios conceptos matemáticos y ejemplos prácticos para cada uno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "4sKf0gGKtlsH",
    "outputId": "3b5ae448-5f72-4b04-89bd-f0c6ebb89d95"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# 1. Detecta dispositivo y tipo de dato óptimo\n",
    "if torch.cuda.is_available():\n",
    "    dispositivo = 0                   # usa GPU 0\n",
    "    dtype = torch.bfloat16           # bfloat16 en GPU para velocidad y menor uso de memoria\n",
    "else:\n",
    "    dispositivo = -1                 # usa CPU\n",
    "    dtype = torch.float32            # float32 en CPU para evitar conversiones lentas\n",
    "\n",
    "# 2. Crea el pipeline de generación de texto una sola vez\n",
    "generate_text = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"aisquared/dlite-v1-355m\",\n",
    "    torch_dtype=dtype,\n",
    "    trust_remote_code=True,\n",
    "    device=dispositivo,\n",
    ")\n",
    "\n",
    "# 3. Warm-up: inicializar caches internas\n",
    "_ = generate_text(\"Hola\")\n",
    "\n",
    "def _extract_text(output) -> str:\n",
    "    \"\"\"\n",
    "    Extrae el texto generado de distintos formatos de salida:\n",
    "    - lista de strings\n",
    "    - lista de dicts con 'generated_text' o 'generated_sequence'\n",
    "    - dict único\n",
    "    - string directo\n",
    "    \"\"\"\n",
    "    if isinstance(output, list):\n",
    "        first = output[0]\n",
    "        if isinstance(first, str):\n",
    "            return first\n",
    "        if isinstance(first, dict):\n",
    "            return first.get(\"generated_text\") or first.get(\"generated_sequence\", \"\")\n",
    "    if isinstance(output, dict):\n",
    "        return output.get(\"generated_text\") or output.get(\"generated_sequence\", \"\")\n",
    "    return str(output)\n",
    "\n",
    "# 4. Función optimizada para explicar conceptos matemáticos\n",
    "def explain_math_concept(concept: str) -> str:\n",
    "    \"\"\"\n",
    "    Genera una explicación detallada del concepto matemático con ejemplos.\n",
    "    \"\"\"\n",
    "    prompt = f\"Explica el {concept} con ejemplos.\"\n",
    "    out = generate_text(\n",
    "        prompt,\n",
    "        max_new_tokens=100,    # límite de tokens para la explicación\n",
    "        do_sample=False        # decodificación greedy (más rápida)\n",
    "    )\n",
    "    return _extract_text(out)\n",
    "\n",
    "# 5. Función optimizada para generar ejemplos prácticos\n",
    "def generate_math_examples(concept: str) -> str:\n",
    "    \"\"\"\n",
    "    Proporciona ejemplos prácticos y soluciones paso a paso para el concepto dado.\n",
    "    \"\"\"\n",
    "    prompt = f\"Proporciona ejemplo y soluciones paso a paso para el {concept}.\"\n",
    "    out = generate_text(\n",
    "        prompt,\n",
    "        max_new_tokens=150,    # límite de tokens para los ejemplos\n",
    "        do_sample=False\n",
    "    )\n",
    "    return _extract_text(out)\n",
    "\n",
    "# 6. Lista de conceptos y generación\n",
    "math_concepts = [\"derivatives\", \"integrals\", \"linear algebra\", \"probability\", \"statistics\"]\n",
    "\n",
    "for concept in math_concepts:\n",
    "    print(f\"\\nConcepto: {concept.capitalize()}\")\n",
    "    print(\"=\" * (len(concept) + 10))\n",
    "\n",
    "    # Explicación del concepto\n",
    "    explanation = explain_math_concept(concept)\n",
    "    print(\"\\nExplicación:\")\n",
    "    print(explanation)\n",
    "\n",
    "    # Ejemplos prácticos\n",
    "    examples = generate_math_examples(concept)\n",
    "    print(\"\\nEjemplos prácticos:\")\n",
    "    print(examples)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# 7. Ejemplo adicional: ecuación cuadrática\n",
    "quadratic_prompt = (\n",
    "    \"Resuelve la ecuación cuadrática 3x^2 - 4x - 5 = 0.\"\n",
    "    \"Proporciona una solución y una explicación paso a paso.\"\n",
    ")\n",
    "quadratic_out = generate_text(\n",
    "    quadratic_prompt,\n",
    "    max_new_tokens=120,\n",
    "    do_sample=False\n",
    ")\n",
    "print(\"\\nEjemplo de ecuación cuadrática:\")\n",
    "print(\"-\" * 30)\n",
    "print(_extract_text(quadratic_out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRHqEzHB7HKh"
   },
   "source": [
    "#### **Ejercicios**\n",
    "\n",
    "1. Diseña una historia interactiva con al menos tres niveles de decisiones. Escribe un diagrama de flujo que muestre todas las posibles rutas y resultados de la historia.\n",
    "2. Lee varias historias generadas por el modelo basadas en diferentes elecciones. Evalúa la continuidad y coherencia de las historias. Identifica y discute cualquier inconsistencia que encuentres.\n",
    "\n",
    "3. Escribe manualmente al menos dos nuevas escenas para cada posible elección en el segundo nivel de decisiones de la historia interactiva proporcionada. Asegúrate de que las escenas sean coherentes y se integren bien en la narrativa existente.\n",
    "\n",
    "4. Diseña un esquema detallado para un documento técnico sobre un tema de tu elección en inteligencia artificial. Incluye al menos seis secciones principales y describe brevemente el contenido de cada sección.\n",
    "\n",
    "5. Utiliza el modelo para generar una sección sobre un tema técnico de tu elección. Analiza el contenido generado en términos de precisión, relevancia y profundidad. ¿Cuáles son las fortalezas y debilidades del contenido generado?\n",
    "6. Encuentra un artículo técnico real sobre un tema similar al generado por el modelo. Compara ambos textos en términos de calidad, detalle y claridad. ¿Qué mejoras sugieres para el contenido generado por el modelo?\n",
    "\n",
    "7. Elige un concepto matemático avanzado (como transformadas de Fourier o series de Taylor). Escribe una explicación detallada del concepto, utilizando al menos dos ejemplos prácticos y resolviendo un problema paso a paso.\n",
    "8. Toma una solución generada por el modelo para un problema matemático (por ejemplo, la solución de una ecuación cuadrática). Revisa y valida cada paso de la solución. ¿Es correcta? Si encuentras errores, corrígelos y explica el proceso correcto.\n",
    "9. Escribe un conjunto de problemas matemáticos que se puedan resolver utilizando los conceptos explicados por el modelo. Para cada problema, proporciona una solución detallada y discute cómo el modelo podría ayudar a generar soluciones similares.\n",
    "10. Investiga diferentes modelos de lenguaje (por ejemplo, GPT-3, BERT, T5). Compara sus arquitecturas, capacidades y aplicaciones. ¿Cuáles son las principales diferencias y en qué contextos se\n",
    "11. Discute las implicaciones éticas y sociales del uso de modelos de lenguaje para generar contenido. Considera aspectos como la generación de noticias falsas, la privacidad de los datos y el sesgo en los modelos. ¿Qué medidas se pueden tomar para mitigar estos riesgos?\n",
    "12. Diseña un proyecto que utilice un modelo de lenguaje para resolver un problema específico en una industria (por ejemplo, salud, educación, finanzas). Describe el problema, cómo el modelo ayudará a resolverlo y los pasos necesarios para implementar la solución.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZUShv2D5B6S"
   },
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prompting con LangChain**\n",
    "\n",
    "#### **1. Prompt Templates: separación de redacción y variables**\n",
    "\n",
    "Las **Prompt Templates** son plantillas parametrizables que distinguen claramente la **estructura fija** de un prompt (instrucción, formatos, estilo) del **contenido dinámico** (variables concretas).\n",
    "\n",
    "* **Desacoplamiento**\n",
    "  El texto base del prompt queda centralizado en una plantilla; las variables se inyectan al ejecutarlo. De este modo no mezclamos la lógica de generación con valores específicos, lo que facilita su lectura y modificación.\n",
    "\n",
    "* **Mantenibilidad**\n",
    "  Al cambiar el tono, corregir errores tipográficos o ajustar el estilo, basta con actualizar la plantilla; todas las invocaciones posteriores heredarán la mejora de forma automática.\n",
    "\n",
    "* **Reutilización**\n",
    "  Una misma plantilla puede servir para diferenciar tareas (resúmenes, explicaciones, preguntas) simplemente variando las variables de entrada. Esto reduce la duplicación de prompts en el código y promueve un diseño más limpio.\n",
    "\n",
    "\n",
    "#### **2. Few-Shot & ExampleSelectors: ejemplos manuales y automáticos**\n",
    "\n",
    "Los **Few-Shot Prompts** incluyen ejemplos concretos de pares *(entrada -> salida)* dentro del propio prompt, enseñando al modelo \"en vuelo\" el patrón deseado. Sin embargo, elegir manualmente qué ejemplos incluir puede resultar laborioso y no siempre óptimo.\n",
    "\n",
    "* **ExampleSelectors**\n",
    "  Automatizan la selección de los ejemplos más relevantes desde una librería amplia, basándose en:\n",
    "\n",
    "  * **Similitud semántica** (mide la cercanía con embeddings).\n",
    "  * **Coincidencia de palabras clave** o metadatos.\n",
    "\n",
    "* **Beneficios**\n",
    "\n",
    "  1. **Adaptabilidad**: el modelo recibe ejemplos alineados con la consulta actual, mejorando su entendimiento.\n",
    "  2. **Escalabilidad**: permite gestionar grandes bancos de ejemplos sin saturar el contexto.\n",
    "  3. **Calidad constante**: evita sesgos o errores de selección manual, pues el sistema elige según métricas objetivas.\n",
    "\n",
    "\n",
    "#### **3. Output Parsers: formatos rígidos y fiabilidad**\n",
    "\n",
    "Los **Output Parsers** convierten la respuesta \"libre\" de un LLM en estructuras de datos controladas (JSON, diccionarios tipados, objetos de validación).\n",
    "\n",
    "* **Validación temprana**\n",
    "  Si la salida no encaja en el esquema esperado (por ejemplo, falta un campo obligatorio o el tipo de dato es incorrecto), el parser detecta la anomalía y permite:\n",
    "\n",
    "  * Solicitar al modelo un reintento con instrucciones aclaratorias.\n",
    "  * Aplicar reglas de recuperación o fallback.\n",
    "\n",
    "* **Reducción de errores downstream**\n",
    "  Al garantizar que el formato de salida sea siempre uniforme, evitamos excepciones e inconsistencias en etapas posteriores (almacenamiento, visualización, integración con otras APIs).\n",
    "\n",
    "* **Trazabilidad y observabilidad**\n",
    "  Registrar cuántas veces fallan los parsers o qué validaciones son las más conflictivas ayuda a iterar y optimizar tanto los prompts como los esquemas de datos.\n",
    "\n",
    "\n",
    "#### **4. Aplicación en un resumidor de noticias**\n",
    "\n",
    "Al incorporar estas tres piezas en un **News Articles Summarizer**, elevamos la calidad y consistencia de los resúmenes:\n",
    "\n",
    "1. **Prompt Templates**\n",
    "\n",
    "   * Definen el estilo editorial (longitud, tono, nivel de detalle) en un único lugar.\n",
    "   * Permiten ajustar rápidamente el \"brief\" que se envía al LLM sin tocar la lógica de orquestación.\n",
    "\n",
    "2. **Few-Shot & ExampleSelectors**\n",
    "\n",
    "   * Recurre a resúmenes humanos previos de artículos similares para guiar el estilo y evitar alucinaciones.\n",
    "   * Selecciona los ejemplos más alineados semánticamente con el tema del artículo, garantizando coherencia temática.\n",
    "\n",
    "3. **Output Parsers**\n",
    "\n",
    "   * Fuerzan un formato JSON fijo:\n",
    "\n",
    "     ```json\n",
    "     {\n",
    "       \"title\": \"...\",\n",
    "       \"summary\": \"...\",\n",
    "       \"keywords\": [\"...\", \"...\"],\n",
    "       \"author\": \"...\",\n",
    "       \"date\": \"YYYY-MM-DD\"\n",
    "     }\n",
    "     ```\n",
    "   * Facilitan la integración con sistemas de publicación o bases de datos, minimizando la limpieza de datos manual.\n",
    "\n",
    "\n",
    "#### **5. Extensión a Knowledge Graphs**\n",
    "\n",
    "Para convertir los resúmenes en un **recurso navegable y semántico**, podemos construir grafos de conocimiento:\n",
    "\n",
    "1. **Extracción de entidades y relaciones**\n",
    "\n",
    "   * Mediante prompts especializados, pedimos al LLM que identifique nombres de personas, organizaciones, conceptos clave y sus interacciones.\n",
    "\n",
    "2. **Construcción del grafo**\n",
    "\n",
    "   * **Nodos**: las entidades detectadas.\n",
    "   * **Aristas**: relaciones etiquetadas (e.g., \"colaboró con\", \"es parte de\", \"causó\").\n",
    "\n",
    "3. **Consultas semánticas**\n",
    "\n",
    "   * Combinamos un *retriever* para localizar fragmentos relevantes en los resúmenes con prompts que interpretan rutas en el grafo:\n",
    "\n",
    "     > \"¿Cómo se relaciona el concepto X con la organización Y?\"\n",
    "\n",
    "4. **Beneficios**\n",
    "\n",
    "   * Descubrimos conexiones ocultas entre temas.\n",
    "   * Ofrecemos a los usuarios exploración interactiva: navegar de nodo en nodo, profundizar en relaciones, generar nuevos insights.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
