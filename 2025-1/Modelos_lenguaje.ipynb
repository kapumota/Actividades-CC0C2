{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44c7b838",
   "metadata": {},
   "source": [
    "###  Introducción a los modelos de lenguaje en NLP\n",
    "\n",
    "Los **modelos de lenguaje** son herramientas fundamentales en el campo del Procesamiento del Lenguaje Natural (NLP). Su principal tarea consiste en asignar una probabilidad a secuencias de palabras, ya sea para predecir la siguiente palabra en una oración o para evaluar la probabilidad de que una oración completa sea gramatical y tenga sentido.  \n",
    "  \n",
    "Por ejemplo, consideremos la secuencia:\n",
    "\n",
    "> **Please turn your homework ...**\n",
    "\n",
    "Un modelo de lenguaje intentará asignar probabilidades a cada palabra que pudiera continuar esta oración. Es muy probable que palabras como \"in\" o \"over\" tengan altas probabilidades, mientras que términos poco probables como \"refrigerador\" o \"the\" en ese contexto recibirán probabilidades bajas. Esta capacidad predictiva es utilizada en diversas aplicaciones:  \n",
    "\n",
    "- **Corrección gramatical y ortográfica:** Ayuda a identificar y corregir errores (por ejemplo, distinguir entre \"Their\" y \"There\").  \n",
    "- **Reconocimiento de voz:** Permite que un sistema decida entre alternativas similares en función de la probabilidad de ocurrencia.  \n",
    "- **Sistemas de comunicación aumentativa:** Facilita la selección de palabras en dispositivos para personas con dificultades para comunicarse.\n",
    "\n",
    "Los grandes modelos de lenguaje actuales se entrenan precisamente con la tarea de predecir palabras futuras, y con ello aprenden de manera implícita una gran cantidad de información sobre la estructura y el significado del lenguaje.\n",
    "\n",
    "\n",
    "Los modelos de lenguaje también pueden asignar una probabilidad a una oración completa. Por ejemplo, pueden predecir que la siguiente secuencia tiene una probabilidad mucho mayor de aparecer en un texto:\n",
    "\n",
    "**`all of a sudden I notice three guys standing on the sidewalk`** que esta misma secuencia de palabras en un orden diferente: **`on guys all I of notice sidewalk three a sudden standing the`**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01b68f4-223e-4d89-9bba-0c497ad36e7f",
   "metadata": {},
   "source": [
    "####  Modelos de lenguaje basados en n-gramas\n",
    "\n",
    "Una de las aproximaciones clásicas para construir un modelo de lenguaje es la utilización de **n-gramas**. Un n-grama es una secuencia contigua de _n_ palabras extraída de un corpus. Por ejemplo:  \n",
    "- **Unigrama (n = 1):** Considera palabras individuales.  \n",
    "- **Bigramas (n = 2):** Considera pares consecutivos de palabras.  \n",
    "- **Trigramas (n = 3):** Considera secuencias de tres palabras, y así sucesivamente.\n",
    "\n",
    "La idea central es utilizar la frecuencia con la que aparecen estas secuencias en un corpus para estimar la probabilidad de ocurrencia de una palabra dada una historia (contexto).\n",
    "\n",
    "Consideremos el ejemplo de predecir la siguiente palabra en la secuencia \"its water is so transparent that\". La probabilidad condicional de la palabra siguiente (por ejemplo, \"the\") se puede estimar mediante el cociente:\n",
    "\n",
    "$$\n",
    "P(\\text{the}|\\text{its water is so transparent that}) = \\frac{C(\\text{its water is so transparent that the})}{C(\\text{its water is so transparent that})}\n",
    "$$\n",
    "\n",
    "Donde $C(\\cdot)$ indica la frecuencia o conteo de aparición de la secuencia en un corpus lo suficientemente grande. Si bien este método de estimación basado en frecuencias relativas es intuitivo, el lenguaje es tan creativo y diverso que incluso un corpus enorme (como toda la web) puede no cubrir todas las secuencias posibles, generando problemas con datos escasos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53d9f3f-8aed-4787-976f-7756b25f0a3e",
   "metadata": {},
   "source": [
    "#### La regla de la cadena y la descomposición de probabilidades\n",
    "\n",
    "Si quisiéramos conocer la probabilidad conjunta de una secuencia completa de palabras como *\"its water is so transparent\"*, podríamos calcularla preguntándonos: *\"De todas las posibles secuencias de cinco palabras, ¿cuántas corresponden a 'its water is so transparent'?\"* Para ello, deberíamos contar la ocurrencia de *\"its water is so transparent\"* y dividirla entre la suma de los recuentos de todas las posibles secuencias de cinco palabras.  \n",
    "\n",
    "Dado que este enfoque es computacionalmente ineficiente, necesitamos introducir métodos más inteligentes para estimar la probabilidad de una palabra `w` dada una historia `h`, o la probabilidad de una secuencia completa de palabras `W`.  \n",
    "\n",
    "Para formalizar la notación, representaremos la probabilidad de que una variable aleatoria $X_i$ tome el valor *\"the\"*, es decir, $P(X_i = \\text{\"the\"})$, mediante la simplificación $P(\\text{the})$.  \n",
    "\n",
    "Denotaremos una secuencia de $n$ palabras como $w_1, w_2, \\dots, w_n$ o, de manera equivalente, $w_{1:n}$. La expresión $w_{1:n-1}$ representa la secuencia $w_1, w_2, \\dots, w_{n-1}$, y también usaremos la notación alternativa $w_{<n}$, que se puede leer como *\"todos los elementos de $w$ desde $w_1$ hasta $w_{n-1}$\"*.  \n",
    "\n",
    "Para expresar la probabilidad conjunta de una secuencia completa de palabras, en lugar de escribir:\n",
    "\n",
    "$$\n",
    "P(X_1 = w_1, X_2 = w_2, X_3 = w_3, \\dots, X_n = w_n)\n",
    "$$\n",
    "\n",
    "usaremos la notación más compacta:\n",
    "\n",
    "$$\n",
    "P(w_1, w_2, \\dots, w_n).\n",
    "$$\n",
    "  \n",
    "\n",
    "Para calcular la probabilidad de una secuencia completa de palabras $P(w_{1:n})$, aplicamos la **regla de la cadena** de la probabilidad, que nos permite descomponer la probabilidad conjunta en una serie de probabilidades condicionales:\n",
    "\n",
    "$$\n",
    "P(w_{1:n}) = P(w_1) \\cdot P(w_2 | w_1) \\cdot P(w_3 | w_{1:2}) \\cdots P(w_n | w_{1:n-1}) = \\prod_{k=1}^{n} P(w_k | w_{1:k-1}).\n",
    "$$\n",
    "\n",
    "Esta descomposición es fundamental, ya que en lugar de estimar directamente la probabilidad conjunta de una secuencia completa (lo cual sería inabordable en la práctica debido a la explosión combinatoria), podemos estimar la probabilidad condicional de cada palabra dado su contexto previo.  \n",
    "\n",
    "Sin embargo, en la mayoría de los casos, no conocemos una forma exacta de calcular la probabilidad de una palabra dada una larga secuencia de palabras precedentes, $P(w_n | w_{1:n-1})$, lo que nos lleva a explorar métodos aproximados y modelos de lenguaje que faciliten esta tarea.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046cc879",
   "metadata": {},
   "source": [
    "**Suposición de Markov en los modelos de lenguaje**\n",
    "\n",
    "En la práctica, calcular $ P(w_n|w_{1:n-1}) $ resulta difícil debido a la inmensidad y complejidad del contexto completo. Aquí es donde entra la **suposición de Markov**. Esta asume que la probabilidad de una palabra depende únicamente de las últimas $n-1$ palabras y no de toda la historia previa.  \n",
    " \n",
    "Por ejemplo, en el **modelo bigrama** (donde $ n = 2 $) se asume:\n",
    "\n",
    "$$\n",
    "P(w_n|w_{1:n-1}) \\approx P(w_n|w_{n-1})\n",
    "$$\n",
    "\n",
    "Es decir, en lugar de considerar toda la secuencia \"Walden Pond's water is so transparent that\", solo se utiliza la última palabra \"that\" para predecir \"the\". De forma similar, se pueden construir modelos trigramas (considerando las dos palabras anteriores) y, en general, modelos n-gramas:\n",
    "\n",
    "$$\n",
    "P(w_n|w_{1:n-1}) \\approx P(w_n|w_{n-N+1:n-1})\n",
    "$$\n",
    "\n",
    "Esta simplificación permite manejar la complejidad del modelo, pero introduce limitaciones, ya que se ignora la información de largo alcance que pudiera ser relevante para predecir la siguiente palabra.\n",
    "\n",
    "> Para los modelos probabilísticos, normalizar significa dividir por algún conteo total de manera que las probabilidades resultantes estén entre 0 y 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a100138e",
   "metadata": {},
   "source": [
    "#### Estimación de probabilidades mediante MLE\n",
    "\n",
    "La **estimación por máxima verosimilitud (MLE)** es un método directo para estimar las probabilidades de ocurrencia basándose en los conteos observados en el corpus de entrenamiento. La idea es que la probabilidad de que ocurra una secuencia de palabras se aproxima a la frecuencia con la que aparece dicha secuencia en el corpus.\n",
    "\n",
    "Por ejemplo, para un bigrama se tiene:\n",
    "\n",
    "$$\n",
    "P(w_n|w_{n-1}) = \\frac{C(w_{n-1},w_n)}{C(w_{n-1})}\n",
    "$$\n",
    "\n",
    "Aquí, $C(w_{n-1},w_n)$ es la cantidad de veces que aparece el par $(w_{n-1},w_n)$ y $C(w_{n-1})$ es la cantidad total de veces que aparece la palabra $w_{n-1}$ en la posición anterior en el corpus. La misma idea se extiende a n-gramas de orden superior:\n",
    "\n",
    "$$\n",
    "P(w_n|w_{n-N+1:n-1}) = \\frac{C(w_{n-N+1:n-1},w_n)}{C(w_{n-N+1:n-1})}\n",
    "$$\n",
    "\n",
    "Este método, aunque intuitivo, enfrenta dos problemas principales:\n",
    "\n",
    "- **Escasez de datos:** Un n-grama que nunca se haya observado en el corpus recibe una probabilidad cero, lo que puede ser problemático cuando se trabaja con secuencias nuevas.  \n",
    "- **Sobreestimación de n-gramas frecuentes:** Los n-gramas que aparecen muy a menudo pueden tener sus probabilidades sobreestimadas, sesgando el modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17db9385",
   "metadata": {},
   "source": [
    "##### **Ejemplo con un mini-corpus y cálculo de probabilidades de bigramas**  \n",
    "\n",
    "Trabajaremos con un ejemplo utilizando un mini-corpus de tres oraciones. Para ello, primero debemos modificar cada oración agregando un símbolo especial `<s>` al inicio, lo que nos proporcionará el contexto de bigrama para la primera palabra. También añadiremos un símbolo especial de fin de oración `</s>` para indicar el final de la secuencia.  \n",
    "\n",
    "> El símbolo de fin de oración es necesario para garantizar que la gramática de bigramas defina una distribución de probabilidad válida. Sin este símbolo, en lugar de que la suma de las probabilidades de todas las oraciones sea igual a uno, las probabilidades de las oraciones de una longitud específica sumarían uno. Esto implicaría un conjunto infinito de distribuciones de probabilidad, con una distribución distinta para cada longitud de oración.  \n",
    "\n",
    "Nuestro mini-corpus con los símbolos `<s>` y `</s>` quedaría de la siguiente forma:  \n",
    "\n",
    "```\n",
    "<s> I am Sam </s>  \n",
    "<s> Sam I am </s>  \n",
    "<s> I do not like green eggs and ham </s>  \n",
    "```\n",
    "\n",
    "A partir de este corpus, calculamos algunas probabilidades de bigramas:\n",
    "\n",
    "$$\n",
    "P(\\text{I} | \\texttt{<s>}) = \\frac{2}{3} = 0.67\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{Sam} | \\texttt{<s>}) = \\frac{1}{3} = 0.33\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{am} | \\text{I}) = \\frac{2}{3} = 0.67\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\texttt{</s>} | \\text{Sam}) = \\frac{1}{2} = 0.5\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{Sam} | \\text{am}) = \\frac{1}{2} = 0.5\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{do} | \\text{I}) = \\frac{1}{3} = 0.33\n",
    "$$\n",
    "\n",
    "\n",
    "En el caso general de estimación de parámetros para modelos de n-gramas mediante **máxima verosimilitud (MLE)**, la probabilidad condicional se calcula como:\n",
    "\n",
    "$$\n",
    "P(w_n | w_{n-N+1:n-1}) = \\frac{C(w_{n-N+1:n-1} w_n)}{C(w_{n-N+1:n-1})}\n",
    "$$\n",
    "\n",
    "Esta ecuación estima la probabilidad de un n-grama dividiendo la frecuencia observada de la secuencia completa ($w_{n-N+1:n-1}, w_n$) por la frecuencia del prefijo ($w_{n-N+1:n-1}$), es decir, el contexto previo sin la última palabra.  \n",
    "\n",
    "La idea detrás de esta ecuación es que la probabilidad de que una palabra ocurra en un contexto específico se puede aproximar observando con qué frecuencia aparece dicho contexto completo en el corpus de entrenamiento, en relación con la frecuencia del prefijo. Esto refleja cuán probable es que una palabra específica siga a un conjunto particular de palabras anteriores.  \n",
    "\n",
    "El uso de frecuencias relativas para estimar probabilidades es un ejemplo de **estimación de máxima verosimilitud (MLE)**. En **MLE**, el conjunto de parámetros encontrados maximiza la verosimilitud del conjunto de entrenamiento $T$ dado el modelo $M$, es decir, $P(T | M)$,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c47ec7-abc2-44d1-a281-4476910034ee",
   "metadata": {},
   "source": [
    "##### **Tokenización y preprocesamiento del corpus**\n",
    "\n",
    "Antes de construir las tablas de frecuencia, es fundamental procesar el corpus para convertir el texto en una lista de tokens (palabras) de manera que se pueda trabajar con ellos de forma estructurada.\n",
    "\n",
    "El siguiente fragmento de código muestra una función para tokenizar el texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ec1717-2fc1-450e-b0c5-5412e13269c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def tokenize(texto):\n",
    "    # Tokeniza el texto y remueve signos de puntuación\n",
    "    texto = re.sub(r'[^\\w\\s]', '', texto.lower())\n",
    "    return texto.split()\n",
    "\n",
    "# Ejemplo de tokenización\n",
    "corpus = \"\"\"\n",
    "    What restaurants serve Italian food in Berkeley?\n",
    "    Where can I find a cheap restaurant downtown?\n",
    "\"\"\"\n",
    "tokens = tokenize(corpus)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7d32d1-978f-4250-8f30-ce4359b097fe",
   "metadata": {},
   "source": [
    "##### **Construcción de tablas de frecuencia**\n",
    "\n",
    "Una vez tokenizado el texto, se procede a construir tablas de frecuencia para unigrama, bigramas y trigramas. Estas tablas serán esenciales para estimar las probabilidades según MLE.\n",
    "\n",
    "El código a continuación muestra cómo construir un contador de n-gramas:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fd7ac9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_ngram_counts(tokens, n):\n",
    "    ## Se generan n-gramas utilizando la técnica de zip para crear versiones desplazadas de la lista de tokens.\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return Counter(ngrams)\n",
    "\n",
    "# Conteo de unigramas, bigramas y trigramas\n",
    "unigrams = build_ngram_counts(tokens, 1)\n",
    "bigrams = build_ngram_counts(tokens, 2)\n",
    "trigrams = build_ngram_counts(tokens, 3)\n",
    "\n",
    "print(\"Unigramas:\", unigrams)\n",
    "print(\"Bigramas:\", bigrams)\n",
    "print(\"Trigramas:\", trigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f03f48",
   "metadata": {},
   "source": [
    "Línea clave: `ngrams = zip(*[tokens[i:] for i in range(n)])`\n",
    "\n",
    "1. `tokens[i:] for i in range(n)`: Crea una lista de sublistas (o iteradores) que son versiones desplazadas de la lista original de tokens. Por ejemplo, si tenemos los tokens `['a', 'b', 'c', 'd']` y `n=2` (bigramas), esto generará dos versiones:\n",
    "\n",
    "- `tokens[0:] --> ['a', 'b', 'c', 'd']` (sin desplazamiento)\n",
    "- `tokens[1:] → ['b', 'c', 'd']` (desplazado en una posición)\n",
    "\n",
    "2. `zip(*...)`: Combina las sublistas generadas en el paso anterior, de manera que produce tuplas que representan las secuencias de n-gramas. Siguiendo el ejemplo de bigramas, zip tomaría un elemento de cada lista desplazada y los combinaría en tuplas:\n",
    "\n",
    "- La primera tupla sería `('a', 'b')`\n",
    "- La segunda tupla sería `('b', 'c')`\n",
    "- La tercera tupla sería `('c', 'd')`\n",
    "\n",
    "Para trigramas, el proceso es similar pero con tres listas desplazadas.\n",
    "\n",
    "3. `Counter(ngrams)`: Después de crear las tuplas de n-gramas con zip, se las pasa a Counter, que cuenta cuántas veces aparece cada tupla (n-grama) en el texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca5f11f-440b-4535-b2a5-50929bad36e2",
   "metadata": {},
   "source": [
    "##### **Estimación de probabilidades con MLE**\n",
    "\n",
    "Una vez que se han obtenido las tablas de frecuencia, se pueden calcular las probabilidades de ocurrencia de n-gramas utilizando el método de máxima verosimilitud (MLE).\n",
    "\n",
    "Para un bigrama, la fórmula MLE es:\n",
    "\n",
    "\n",
    "$P(w_n | w_{n-1}) = \\frac{C(w_{n-1} w_n)}{C(w_{n-1})}$\n",
    "\n",
    "El código siguiente implementa esta fórmula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c0f191-4252-4cfc-b94e-f796d820bc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_prob(bigrams, unigrams, word1, word2):\n",
    "    return bigrams[(word1, word2)] / unigrams[(word1,)]\n",
    "\n",
    "# Ejemplo de probabilidad de bigrama\n",
    "word1, word2 = 'cheap', 'restaurant'\n",
    "print(f\"P({word2}|{word1}) =\", bigram_prob(bigrams, unigrams, word1, word2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf9d8ca-d437-4ca9-ba49-af6ab300222e",
   "metadata": {},
   "source": [
    "##### **Generalización a n-gramas**\n",
    "\n",
    "La misma idea se extiende para n-gramas de orden superior. La fórmula general es: \n",
    "\n",
    "$P(w_n|w_{n-N+1:n-1}) = \\frac{C(w_{n-N+1:n-1} w_n)}{C(w_{n-N+1:n-1)}}$\n",
    "\n",
    "El siguiente código implementa la función para calcular la probabilidad de un n-grama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9bb00f-e950-457b-87eb-0a73eaf0e66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_prob(ngram_counts, lower_order_counts, context, word):\n",
    "    return ngram_counts[context + (word,)] / lower_order_counts[context]\n",
    "\n",
    "# Ejemplo con trigramas\n",
    "context = ('i', 'find')\n",
    "word = 'a'\n",
    "print(f\"P({word}|{' '.join(context)}) =\", ngram_prob(trigrams, bigrams, context, word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40814c3-c528-4ca0-88f4-5c4604296260",
   "metadata": {},
   "source": [
    "##### **Suavizado de Laplace**\n",
    "\n",
    "Uno de los problemas de la estimación MLE es que si un n-grama nunca se ha visto en el corpus, su probabilidad se asigna como cero. Esto es problemático para cualquier modelo que deba generalizar a secuencias nuevas.  \n",
    "El **suavizado de Laplace** (o suavizado aditivo) añade 1 al conteo de cada n-grama para evitar probabilidades nulas:\n",
    "\n",
    "\n",
    "$P(w_n | w_{n-1}) = \\frac{C(w_{n-1} w_n) + 1}{C(w_{n-1}) + |V|}$\n",
    "\n",
    "donde $|V|$ es el tamaño del vocabulario (la cantidad de palabras únicas).\n",
    "\n",
    "El siguiente fragmento de código implementa esta técnica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a00ef0-4575-43e2-bd9a-467d9e0fcca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Función para aplicar suavizado de Laplace\n",
    "def laplace_smoothing(bigrams, unigrams, word1, word2, vocab_size):\n",
    "    \"\"\"\n",
    "    Calcula la probabilidad suavizada de un bigrama usando el suavizado de Laplace.\n",
    "    \n",
    "    \n",
    "    Parámetros:\n",
    "    - bigrams: un diccionario con conteos de bigramas.\n",
    "    - unigrams: un diccionario con conteos de unigramas.\n",
    "    - word1: la primera palabra del bigrama.\n",
    "    - word2: la segunda palabra del bigrama.\n",
    "    - vocab_size: el tamaño del vocabulario (número total de palabras únicas).\n",
    "    \n",
    "    Retorna:\n",
    "    - La probabilidad suavizada del bigrama.\n",
    "    \"\"\"\n",
    "    bigram_count = bigrams[(word1, word2)]\n",
    "    unigram_count = unigrams[(word1,)]\n",
    "    return (bigram_count + 1) / (unigram_count + vocab_size)\n",
    "\n",
    "# Ejemplo de uso\n",
    "unigrams = defaultdict(int)\n",
    "bigrams = defaultdict(int)\n",
    "\n",
    "# Ejemplo de corpus tokenizado\n",
    "tokens = ['i', 'find', 'a', 'cheap', 'restaurant', 'downtown']\n",
    "\n",
    "# Construcción de conteos de unigramas y bigramas\n",
    "for i in range(len(tokens) - 1):\n",
    "    unigrams[(tokens[i],)] += 1\n",
    "    bigrams[(tokens[i], tokens[i+1])] += 1\n",
    "# No olvides contar el último unigrama\n",
    "unigrams[(tokens[-1],)] += 1\n",
    "\n",
    "# Tamaño del vocabulario\n",
    "vocab_size = len(set(tokens))\n",
    "\n",
    "# Calculamos la probabilidad suavizada para el bigrama ('cheap', 'restaurant')\n",
    "word1, word2 = 'cheap', 'restaurant'\n",
    "prob = laplace_smoothing(bigrams, unigrams, word1, word2, vocab_size)\n",
    "print(f\"Probabilidad suavizada de ('{word1}', '{word2}'):\", prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ea2e3f-9ba2-43ae-91e6-10d2e40beb22",
   "metadata": {},
   "source": [
    "##### **Evaluación del modelo: perplejidad**\n",
    "\n",
    "Una vez construido el modelo, es importante evaluar su desempeño. Una métrica común es la **perplejidad**, que mide la capacidad del modelo para predecir una secuencia de palabras. La perplejidad se define en términos del logaritmo negativo de las probabilidades y, en esencia, mide la incertidumbre del modelo.\n",
    "\n",
    "El siguiente código calcula la perplejidad de un conjunto de prueba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92d507b-43e9-44ee-94bc-37d1de559189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_perplexity(test_data, bigrams, unigrams):\n",
    "    perplexity = 0\n",
    "    N = len(test_data)\n",
    "    for i in range(1, N):\n",
    "        word1, word2 = test_data[i-1], test_data[i]\n",
    "        prob = laplace_smoothing(bigrams, unigrams, word1, word2, vocab_size)\n",
    "        perplexity += -math.log(prob)\n",
    "    return math.exp(perplexity / N)\n",
    "\n",
    "test_tokens = tokenize(\"find a restaurant downtown\")\n",
    "print(\"Perplejidad:\", calculate_perplexity(test_tokens, bigrams, unigrams))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2988799a-5857-4216-b17e-f9c0974f8489",
   "metadata": {},
   "source": [
    "##### **Interpolación de n-gramas**\n",
    "\n",
    "El suavizado y la combinación de distintos órdenes de n-gramas pueden mejorar la estimación de probabilidades. La **interpolación** busca combinar las estimaciones de unigramas, bigramas y trigramas (u órdenes superiores) para obtener una probabilidad final más robusta. La fórmula de interpolación es:\n",
    "\n",
    "$$\n",
    "P_{interp}(w_n | w_{n-1}, w_{n-2}) = \\lambda_3 P(w_n | w_{n-1}, w_{n-2}) + \\lambda_2 P(w_n | w_{n-1}) + \\lambda_1 P(w_n)\n",
    "$$\n",
    "\n",
    "Donde $\\lambda_1$, $\\lambda_2$, $\\lambda_3$ son los pesos asignados a los unigramas, bigramas y trigramas, respectivamente que suman 1 y determinan la contribución de cada modelo de orden diferente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce25b473-7729-4c69-8f7e-8908433718b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Función para obtener la probabilidad interpolada\n",
    "def interpolated_prob(trigrams, bigrams, unigrams, word1, word2, word3, vocab_size, lambdas=(0.1, 0.3, 0.6)):\n",
    "    \"\"\"\n",
    "    Calcula la probabilidad interpolada usando unigramas, bigramas y trigramas.\n",
    "    P(w_n | w_{n-2}, w_{n-1}) = λ3 * P(w_n | w_{n-2}, w_{n-1}) + λ2 * P(w_n | w_{n-1}) + λ1 * P(w_n)\n",
    "\n",
    "    Parámetros:\n",
    "    - trigrams: diccionario con conteos de trigramas.\n",
    "    - bigrams: diccionario con conteos de bigramas.\n",
    "    - unigrams: diccionario con conteos de unigramas.\n",
    "    - word1: palabra n-2.\n",
    "    - word2: palabra n-1.\n",
    "    - word3: palabra n.\n",
    "    - vocab_size: tamaño del vocabulario.\n",
    "    - lambdas: pesos para la interpolación.\n",
    "\n",
    "    Retorna:\n",
    "    - La probabilidad interpolada.\n",
    "    \"\"\"\n",
    "    # Descomponemos los pesos lambda\n",
    "    lambda1, lambda2, lambda3 = lambdas\n",
    "\n",
    "    # Calcular probabilidades individuales (con suavizado Laplace)\n",
    "    P_w3 = (unigrams[(word3,)] + 1) / (sum(unigrams.values()) + vocab_size)\n",
    "    P_w3_given_w2 = (bigrams[(word2, word3)] + 1) / (unigrams[(word2,)] + vocab_size)\n",
    "    P_w3_given_w1_w2 = (trigrams[(word1, word2, word3)] + 1) / (bigrams[(word1, word2)] + vocab_size)\n",
    "\n",
    "    # Probabilidad interpolada\n",
    "    return lambda3 * P_w3_given_w1_w2 + lambda2 * P_w3_given_w2 + lambda1 * P_w3\n",
    "\n",
    "# Ejemplo de datos y uso\n",
    "tokens = ['i', 'find', 'a', 'cheap', 'restaurant', 'downtown']\n",
    "\n",
    "# Construimos unigramas, bigramas y trigramas\n",
    "unigrams = defaultdict(int)\n",
    "bigrams = defaultdict(int)\n",
    "trigrams = defaultdict(int)\n",
    "\n",
    "for i in range(len(tokens) - 2):\n",
    "    unigrams[(tokens[i],)] += 1\n",
    "    bigrams[(tokens[i], tokens[i+1])] += 1\n",
    "    trigrams[(tokens[i], tokens[i+1], tokens[i+2])] += 1\n",
    "unigrams[(tokens[-2],)] += 1\n",
    "bigrams[(tokens[-2], tokens[-1])] += 1\n",
    "unigrams[(tokens[-1],)] += 1\n",
    "\n",
    "vocab_size = len(set(tokens))\n",
    "\n",
    "# Calcular probabilidad interpolada para (\"cheap\", \"restaurant\", \"downtown\")\n",
    "word1, word2, word3 = 'cheap', 'restaurant', 'downtown'\n",
    "prob_interpolada = interpolated_prob(trigrams, bigrams, unigrams, word1, word2, word3, vocab_size)\n",
    "print(f\"Probabilidad interpolada de ('{word1}', '{word2}', '{word3}'):\", prob_interpolada)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47613a0-f268-41b4-98ff-1cd48579d7f6",
   "metadata": {},
   "source": [
    "#### El suavizado de **Kneser-Ney** \n",
    "\n",
    "El suavizado de Laplace es útil, pero en algunos casos se requieren técnicas más sofisticadas. El **suavizado de Kneser-Ney** es una de las técnicas avanzadas que no solo utiliza los conteos absolutos de los n-gramas, sino que también tiene en cuenta la diversidad de contextos en los que aparece una palabra. Esto es especialmente útil para manejar n-gramas raros cuyos componentes individuales puedan aparecer en muchos contextos diferentes.\n",
    "\n",
    "La fórmula para un bigrama con suavizado de Kneser-Ney es:\n",
    "\n",
    "$\n",
    "P_{KN}(w_n | w_{n-1}) = \\max(C(w_{n-1} w_n) - d, 0) / C(w_{n-1}) + \\lambda(w_{n-1}) P_{KN}(w_n)\n",
    "$\n",
    "\n",
    "Donde $d$ es un descuento y $\\lambda(w_{n-1})$ ajusta la probabilidad restante para asegurar que las probabilidades sumen `1`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b94b056-c5d7-431b-bc3a-c2c394fbd3bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def kneser_ney_prob(bigrams, unigrams, word1, word2, discount, vocab_size):\n",
    "    \"\"\"\n",
    "    Calcula la probabilidad suavizada de Kneser-Ney para bigramas.\n",
    "    \n",
    "    Parámetros:\n",
    "    - bigrams: diccionario con conteos de bigramas.\n",
    "    - unigrams: diccionario con conteos de unigramas.\n",
    "    - word1: primera palabra del bigrama.\n",
    "    - word2: segunda palabra del bigrama.\n",
    "    - discount: el factor de descuento.\n",
    "    - vocab_size: tamaño del vocabulario.\n",
    "    \n",
    "    Retorna:\n",
    "    - La probabilidad de Kneser-Ney para el bigrama.\n",
    "    \"\"\"\n",
    "    # Contar bigramas y unigramas\n",
    "    bigram_count = bigrams[(word1, word2)]\n",
    "    unigram_count = unigrams[(word1,)]\n",
    "\n",
    "    # Descuento aplicado\n",
    "    max_term = max(bigram_count - discount, 0) / unigram_count\n",
    "\n",
    "    # Conteo de bigramas distintos que empiezan con word1 (contexto)\n",
    "    unique_continuations = len([w for w in unigrams if (word1, w[0]) in bigrams])\n",
    "\n",
    "    # Probabilidad marginal de la palabra individual (unigrama)\n",
    "    marginal_prob = (unigrams[(word2,)] + 1) / (sum(unigrams.values()) + vocab_size)\n",
    "\n",
    "    # Probabilidad suavizada\n",
    "    return max_term + discount * unique_continuations * marginal_prob / unigram_count\n",
    "\n",
    "# Parámetros\n",
    "discount = 0.75  # Un descuento típico en Kneser-Ney\n",
    "\n",
    "# Calcular la probabilidad suavizada de Kneser-Ney para ('cheap', 'restaurant')\n",
    "word1, word2 = 'cheap', 'restaurant'\n",
    "prob_kneser_ney = kneser_ney_prob(bigrams, unigrams, word1, word2, discount, vocab_size)\n",
    "print(f\"Probabilidad de Kneser-Ney para ('{word1}', '{word2}'):\", prob_kneser_ney)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da25ac65-c050-45b0-accb-4f91572a0346",
   "metadata": {},
   "source": [
    "#### Ejercicios\n",
    "\n",
    "**Ejercicio 1: Carga y preprocesamiento del corpus**\n",
    "\n",
    "**Objetivo:** Cargar y preprocesar un corpus de texto.\n",
    "\n",
    "- **Instrucciones:**\n",
    "  1. Carga un archivo de texto que contenga diálogos sobre restaurantes (puede simular el corpus del **Berkeley Restaurant Project**).\n",
    "  2. Implementa una función que procese el texto eliminando signos de puntuación y convierta todo a minúsculas.\n",
    "  3. Tokeniza el texto en una lista de palabras.\n",
    "\n",
    "```python\n",
    "# Archivo corpus.txt\n",
    "\"\"\"\n",
    "What restaurants serve Italian food in Berkeley?\n",
    "Where can I find a cheap restaurant downtown?\n",
    "\"\"\"\n",
    "\n",
    "def load_and_preprocess_corpus(file_path):\n",
    "    \"\"\"\n",
    "    Carga y preprocesa el corpus desde un archivo de texto.\n",
    "    - Elimina signos de puntuación y convierte todo a minúsculas.\n",
    "    - Devuelve una lista de palabras (tokens).\n",
    "    \"\"\"\n",
    "    # Implementar la función de carga y preprocesamiento del corpus\n",
    "    pass\n",
    "\n",
    "# Tarea: Implementa y prueba la función usando un archivo de texto.\n",
    "```\n",
    "\n",
    "**Ejercicio 2: Cálculo decConteos de unigramas, bigramas y trigramas**\n",
    "\n",
    "**Objetivo:** Implementar un código para calcular los conteos de unigramas, bigramas y trigramas a partir del corpus tokenizado.\n",
    "\n",
    "- **Instrucciones:**\n",
    "  1. Escribe una función para contar los unigramas, bigramas y trigramas.\n",
    "  2. Devuelve los conteos como diccionarios.\n",
    "\n",
    "```python\n",
    "from collections import defaultdict\n",
    "\n",
    "def calculate_ngram_counts(tokens, n):\n",
    "    \"\"\"\n",
    "    Calcula los conteos de n-gramas a partir de una lista de tokens.\n",
    "    Retorna un diccionario con los conteos de n-gramas.\n",
    "    \"\"\"\n",
    "    # Implementar el cálculo de n-gramas\n",
    "    pass\n",
    "\n",
    "# Tarea: Implementa la función para unigramas, bigramas y trigramas\n",
    "# Ejemplo: tokens = ['what', 'restaurants', 'serve', 'italian', 'food']\n",
    "```\n",
    "\n",
    "**Ejercicio 3: Implementación de MLE (Maximum Likelihood Estimation)**\n",
    "\n",
    "**Objetivo:** Implementar la estimación de máxima verosimilitud (MLE) para bigramas y trigramas.\n",
    "\n",
    "- **Instrucciones:**\n",
    "  1. Implementa una función que calcule la probabilidad de un bigrama o trigram usando MLE.\n",
    "  2. Usa la fórmula:\n",
    "\n",
    "  $$\n",
    "  P(w_n | w_{n-1}) = \\frac{C(w_{n-1} w_n)}{C(w_{n-1})}\n",
    "  $$\n",
    "\n",
    "```python\n",
    "def mle_prob(bigrams, unigrams, word1, word2):\n",
    "    \"\"\"\n",
    "    Calcula la probabilidad MLE para un bigrama.\n",
    "    \"\"\"\n",
    "    # Implementar la probabilidad MLE\n",
    "    pass\n",
    "\n",
    "# Tarea: Calcula P('italian' | 'serve') usando los bigramas y unigramas de tu corpus.\n",
    "```\n",
    "\n",
    "**Ejercicio 4: Aplicación de suavizado de Laplace**\n",
    "\n",
    "**Objetivo:** Implementar el suavizado de Laplace para bigramas.\n",
    "\n",
    "- **Instrucciones:**\n",
    "  1. Modifica la función MLE para incluir el suavizado de Laplace, usando la fórmula:\n",
    "\n",
    "  $$\n",
    "  P(w_n | w_{n-1}) = \\frac{C(w_{n-1} w_n) + 1}{C(w_{n-1}) + |V|}\n",
    "  $$\n",
    "\n",
    "  2. Implementa la función y calcula la probabilidad suavizada para algunos bigramas del corpus.\n",
    "\n",
    "```python\n",
    "def laplace_smoothing(bigrams, unigrams, word1, word2, vocab_size):\n",
    "    \"\"\"\n",
    "    Aplica el suavizado de Laplace a un bigrama.\n",
    "    \"\"\"\n",
    "    # Implementar suavizado de Laplace\n",
    "    pass\n",
    "\n",
    "# Tarea: Calcula la probabilidad suavizada para varios bigramas como ('cheap', 'restaurant').\n",
    "```\n",
    "\n",
    "**Ejercicio 5: Implementación de la interpolación de N-gramas**\n",
    "\n",
    "**Objetivo:** Implementar la interpolación de unigramas, bigramas y trigramas para mejorar las probabilidades.\n",
    "\n",
    "- **Instrucciones:**\n",
    "  1. Usa la fórmula de interpolación:\n",
    "\n",
    "  $$\n",
    "  P_{interp}(w_n | w_{n-1}, w_{n-2}) = \\lambda_3 P(w_n | w_{n-1}, w_{n-2}) + \\lambda_2 P(w_n | w_{n-1}) + \\lambda_1 P(w_n)\n",
    "  $$\n",
    "\n",
    "  2. Implementa una función que combine unigramas, bigramas y trigramas con pesos $\\lambda_1$, $\\lambda_2$, $\\lambda_3$.\n",
    "\n",
    "```python\n",
    "def interpolated_prob(trigrams, bigrams, unigrams, word1, word2, word3, vocab_size, lambdas=(0.1, 0.3, 0.6)):\n",
    "    \"\"\"\n",
    "    Calcula la probabilidad interpolada usando unigramas, bigramas y trigramas.\n",
    "    \"\"\"\n",
    "    # Implementar interpolación\n",
    "    pass\n",
    "\n",
    "# Tarea: Prueba la interpolación para secuencias de tres palabras del corpus.\n",
    "```\n",
    "\n",
    "**Ejercicio 6: Suavizado de Kneser-Ney**\n",
    "\n",
    "**Objetivo:** Implementar el suavizado de Kneser-Ney para bigramas.\n",
    "\n",
    "- **Instrucciones:**\n",
    "  1. Implementa el suavizado de Kneser-Ney para calcular la probabilidad de un bigrama.\n",
    "  2. Usa la fórmula:\n",
    "\n",
    "  $$\n",
    "  P_{KN}(w_n | w_{n-1}) = \\frac{\\max(C(w_{n-1} w_n) - d, 0)}{C(w_{n-1})} + \\lambda(w_{n-1}) P_{KN}(w_n)\n",
    "  $$\n",
    "\n",
    "```python\n",
    "def kneser_ney_prob(bigrams, unigrams, word1, word2, discount, vocab_size):\n",
    "    \"\"\"\n",
    "    Calcula la probabilidad suavizada de Kneser-Ney para bigramas.\n",
    "    \"\"\"\n",
    "    # Implementar suavizado de Kneser-Ney\n",
    "    pass\n",
    "\n",
    "# Tarea: Aplica suavizado de Kneser-Ney para calcular la probabilidad de ('cheap', 'restaurant').\n",
    "```\n",
    "\n",
    "**Ejercicio 7: Evaluación del modelo con perplejidad**\n",
    "\n",
    "**Objetivo:** Implementar una función para evaluar el modelo de n-gramas con **perplejidad**.\n",
    "\n",
    "- **Instrucciones:**\n",
    "  1. Implementa una función que calcule la **perplejidad** de un modelo n-grama dado un conjunto de prueba.\n",
    "\n",
    "  $$\n",
    "  \\text{Perplejidad} = \\exp \\left( - \\frac{1}{N} \\sum_{i=1}^N \\log P(w_i | w_{i-n+1:i-1}) \\right)\n",
    "  $$\n",
    "\n",
    "```python\n",
    "import math\n",
    "\n",
    "def calculate_perplexity(test_data, ngram_model, unigrams, bigrams, trigrams, vocab_size):\n",
    "    \"\"\"\n",
    "    Calcula la perplejidad de un modelo n-grama dado un conjunto de prueba.\n",
    "    \"\"\"\n",
    "    # Implementar la función de perplejidad\n",
    "    pass\n",
    "\n",
    "# Tarea: Calcula la perplejidad del modelo con suavizado de Laplace en un conjunto de prueba.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2bb431-6a1a-4c56-8fdd-5267e6be84b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a126091",
   "metadata": {},
   "source": [
    "#### **Algunos problemas prácticos**  \n",
    "\n",
    "Hasta ahora, hemos descrito modelos de **bigramas** con fines pedagógicos. Sin embargo, en la práctica podemos utilizar **modelos de trigramas**, que se condicionan en las dos palabras anteriores, o incluso **modelos de 4-gramas o 5-gramas** cuando se dispone de suficientes datos de entrenamiento.  \n",
    "\n",
    "Para estos **n-gramas más largos**, es necesario asumir **contextos adicionales** tanto al inicio como al final de la oración.  \n",
    "\n",
    "Por ejemplo, para calcular probabilidades de trigramas al comienzo de una oración, utilizamos **dos pseudo-palabras `<s>`** que representan el inicio de la oración. Esto nos permite manejar casos en los que los primeros trigramas no cuentan con suficientes palabras previas.  \n",
    "\n",
    "\n",
    "##### **Ejemplo: Cálculo de Probabilidades de Trigramas**  \n",
    "\n",
    "Supongamos que queremos calcular la probabilidad del trigrama **\"I am happy\"** al comienzo de una oración. Específicamente, buscamos estimar:  \n",
    "\n",
    "$$\n",
    "P(\\text{happy} | \\text{I am})\n",
    "$$\n",
    "\n",
    "##### **Paso 1: Preparar la oración con pseudo-palabras**  \n",
    "\n",
    "Dado que \"I\" es la primera palabra de la oración, agregamos dos pseudo-palabras `<s>` al principio para calcular el primer trigrama. Así, la oración se modifica de la siguiente manera:  \n",
    "\n",
    "```\n",
    "<s> <s> I am happy\n",
    "```\n",
    "\n",
    "##### **Paso 2: Calcular la probabilidad del primer trigrama**  \n",
    "\n",
    "El primer trigrama es `(<s>, <s>, I)`, y su probabilidad se calcula como:  \n",
    "\n",
    "$$\n",
    "P(\\text{I} | <s> <s>)\n",
    "$$\n",
    "\n",
    "Esto representa la probabilidad de que \"I\" sea la primera palabra tras los dos marcadores de inicio de oración.\n",
    "\n",
    "##### **Paso 3: Calcular probabilidades sucesivas**  \n",
    "\n",
    "El siguiente trigrama es `(<s>, I, am)`, cuya probabilidad se calcula como:  \n",
    "\n",
    "$$\n",
    "P(\\text{am} | <s> I)\n",
    "$$\n",
    "\n",
    "Finalmente, para el trigrama **\"I am happy\"**, calculamos:  \n",
    "\n",
    "$$\n",
    "P(\\text{happy} | I\\ am)\n",
    "$$\n",
    "\n",
    "Este enfoque permite extender los modelos de n-gramas a secuencias más largas y mejorar su capacidad predictiva."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1941dc56",
   "metadata": {},
   "source": [
    "##### **Observación**  \n",
    "\n",
    "Siempre representamos y calculamos las probabilidades en modelos de lenguaje utilizando su **forma logarítmica**, conocidas como **probabilidades logarítmicas**.  \n",
    "\n",
    "Dado que las probabilidades son, por definición, **menores o iguales a 1**, al multiplicar varias de ellas, el producto se vuelve cada vez más pequeño. Multiplicar un número suficientemente grande de **n-gramas** puede llevar a **desbordamiento numérico** debido a la acumulación de valores extremadamente pequeños.  \n",
    "\n",
    "Para evitar este problema, en lugar de trabajar con probabilidades crudas, utilizamos **logaritmos**. Al hacerlo, obtenemos valores que son más manejables numéricamente y evitamos el riesgo de desbordamiento.  \n",
    "\n",
    "Además, la **suma en el espacio logarítmico** es equivalente a la **multiplicación en el espacio lineal**, lo que simplifica los cálculos. Es decir, en lugar de multiplicar probabilidades directamente, **sumamos sus logaritmos**:  \n",
    "\n",
    "$\n",
    "p_1 \\times p_2 \\times p_3 \\times p_4 = \\exp(\\log p_1 + \\log p_2 + \\log p_3 + \\log p_4)\n",
    "$\n",
    "\n",
    "En la práctica, usamos `log` para referirnos al **logaritmo natural ($\\ln$)** cuando la base no se especifica.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
