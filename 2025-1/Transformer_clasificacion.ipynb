{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Aplicando los Transformers para clasificación**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías requeridas para este cuaderno \n",
    "# Las que ya vienen preinstaladas están comentadas.\n",
    "# Si ejecutas el cuaderno en tu entorno local, descomenta las siguientes líneas:\n",
    "\n",
    "# !pip install -q pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1\n",
    "# - Para actualizar un paquete a la última versión disponible:\n",
    "# !pip install pmdarima -U\n",
    "# - Para fijar un paquete en una versión concreta:\n",
    "# !pip install --upgrade pmdarima==2.0.2\n",
    "\n",
    "# Nota: si tu entorno no soporta el comando \"!pip install\", deja estas líneas en comentarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install dash-core-components==2.0.0 \n",
    "#!pip install dash-table==5.0.0\n",
    "#!pip install dash==2.9.3\n",
    "#!pip install -Uqq dash-html-components==2.0.0\n",
    "#!pip install -Uqq portalocker>=2.0.0\n",
    "#!pip install -qq torchtext\n",
    "#!pip install -qq torchdata\n",
    "#!pip install -Uqq plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puedes suprimir aqui los warnings generados por tu codigo\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import accumulate\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from IPython.display import Markdown as md\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objs as go\n",
    "import pickle\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Funciones auxiliares**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(COST,ACC):\n",
    "    \n",
    "    fig, ax1 = plt.subplots()\n",
    "    color = 'tab:red'\n",
    "    ax1.plot(COST, color=color)\n",
    "    ax1.set_xlabel('epoch', color=color)\n",
    "    ax1.set_ylabel('total loss', color=color)\n",
    "    ax1.tick_params(axis='y', color=color)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('accuracy', color=color)  \n",
    "    ax2.plot(ACC, color=color)\n",
    "    ax2.tick_params(axis='y', color=color)\n",
    "    fig.tight_layout()  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embdings(my_embdings,name,vocab):\n",
    "  \n",
    "  fig = plt.figure()\n",
    "  ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "  ax.scatter(my_embdings[:,0], my_embdings[:,1], my_embdings[:,2])\n",
    "\n",
    "  for j, label in enumerate(name):\n",
    "      i=vocab.get_stoi()[label]\n",
    "      ax.text(my_embdings[j,0], my_embdings[j,1], my_embdings[j,2], label)\n",
    "\n",
    "  ax.set_xlabel('X Label')\n",
    "  ax.set_ylabel('Y Label')\n",
    "  ax.set_zlabel('Z Label')\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tras(words, modelo):\n",
    "    tokens = tokenizer(words)\n",
    "\n",
    "    d_model = 100\n",
    "\n",
    "    x = torch.tensor(text_pipeline(words)).unsqueeze(0).to(device)\n",
    "\n",
    "    x_ = modelo.emb(x) * math.sqrt(d_model)\n",
    "\n",
    "    x = modelo.pos_encoder(x_)\n",
    "\n",
    "    q_proj_weight = modelo.state_dict()['transformer_encoder.layers.0.self_attn.in_proj_weight'][0:embed_dim].t()\n",
    "    k_proj_weight = modelo.state_dict()['transformer_encoder.layers.0.self_attn.in_proj_weight'][embed_dim:2*embed_dim].t()\n",
    "    v_proj_weight = modelo.state_dict()['transformer_encoder.layers.0.self_attn.in_proj_weight'][2*embed_dim:3*embed_dim].t()\n",
    "\n",
    "    Q = (x @ q_proj_weight).squeeze(0)\n",
    "    K = (x @ k_proj_weight).squeeze(0)\n",
    "    V = (x @ v_proj_weight).squeeze(0)\n",
    "\n",
    "    scores = Q @ K.T\n",
    "\n",
    "    row_labels = tokens\n",
    "    col_labels = row_labels\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(scores.cpu().detach().numpy())\n",
    "    plt.yticks(range(len(row_labels)), row_labels)\n",
    "    plt.xticks(range(len(col_labels)), col_labels, rotation=90)\n",
    "    plt.title(\"Atención producto-punto\")\n",
    "    plt.show()\n",
    "\n",
    "    att = nn.Softmax(dim=1)(scores)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(att.cpu().detach().numpy())\n",
    "    plt.yticks(range(len(row_labels)), row_labels)\n",
    "    plt.xticks(range(len(col_labels)), col_labels, rotation=90)\n",
    "    plt.title(\"Atención producto-punto escalado\")\n",
    "    plt.show()\n",
    "\n",
    "    head = nn.Softmax(dim=1)(scores) @ V\n",
    "\n",
    "    tsne(x_, tokens, title=\"Embeddings\")\n",
    "    tsne(head, tokens, title=\"Cabeceras de atención\")\n",
    "\n",
    "\n",
    "def tsne(embeddings, tokens, title=\"Embeddings\"):\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    tsne_result = tsne.fit_transform(embeddings.squeeze(0).cpu().detach().numpy())\n",
    "    \n",
    "    plt.scatter(tsne_result[:, 0], tsne_result[:, 1])\n",
    "\n",
    "    plt.title(title)\n",
    "\n",
    "    for j, label in enumerate(tokens):\n",
    "        plt.text(tsne_result[j, 0], tsne_result[j, 1], label)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list_to_file(lst, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(lst, file)\n",
    "\n",
    "def load_list_from_file(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        loaded_list = pickle.load(file)\n",
    "    return loaded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    (1,\"Introduction to NLP\"),\n",
    "    (2,\"Basics of PyTorch\"),\n",
    "    (1,\"NLP Techniques for Text Classification\"),\n",
    "    (3,\"Named Entity Recognition with PyTorch\"),\n",
    "    (3,\"Sentiment Analysis using PyTorch\"),\n",
    "    (3,\"Machine Translation with PyTorch\"),\n",
    "    (1,\" NLP Named Entity,Sentiment Analysis,Machine Translation \"),\n",
    "    (1,\" Machine Translation with NLP \"),\n",
    "    (1,\" Named Entity vs Sentiment Analysis  NLP \"),\n",
    "    (3,\"he painted the car red\"),\n",
    "    (1,\"he painted the red car\")\n",
    "    ]\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for  _,text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(dataset), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_pipeline(x):\n",
    "    return vocab(tokenizer(x))\n",
    "\n",
    "def label_pipeline(x):\n",
    "    return int(x) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cero padding**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [torch.tensor([j for j in range(1,i)]) for i in range(2,10)]\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "print(padded_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Codificación posicional:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_tokens='he painted the car red he painted the red car'\n",
    "\n",
    "mi_index=text_pipeline(mi_tokens)\n",
    "mi_index\n",
    "\n",
    "embedding_dim=3\n",
    "\n",
    "vocab_size=len(vocab)\n",
    "print(vocab_size)\n",
    "\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_embdings=embedding(torch.tensor(mi_index)).detach().numpy()\n",
    "plot_embdings(mi_embdings,tokenizer(mi_tokens),vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = torch.arange(0, vocab_size, dtype=torch.float).unsqueeze(1)\n",
    "position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model=3\n",
    "pe = torch.zeros(vocab_size,d_model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe=torch.cat((position, position, position), 1)\n",
    "pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples,dim=mi_embdings.shape\n",
    "samples,dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embding=mi_embdings+pe[0:samples,:].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embdings(pos_embding,tokenizer(mi_tokens),vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embding[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embding[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe=torch.cat((0.1*position, -0.1*position, 0*position), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pe[:, 0].numpy(), label=\"Dimension 1\")\n",
    "plt.plot(pe[:, 1].numpy(), label=\"Dimension 2\")\n",
    "plt.plot(pe[:, 2].numpy(), label=\"Dimension 3\")\n",
    "\n",
    "plt.xlabel(\"Número de secuencia\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embding=mi_embdings+pe[0:samples,:].numpy()\n",
    "plot_embdings(pos_embding,tokenizer(mi_tokens),vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe=torch.cat((torch.sin(2*3.14*position/6), 0*position+1, 0*position+1), 1)\n",
    "pos_embding=mi_embdings+pe[0:samples,:].numpy()\n",
    "plot_embdings(pos_embding,tokenizer(mi_tokens),vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pe[:, 0].numpy(), label=\"Dimension 1\", linestyle='-')\n",
    "plt.plot(pe[:, 1].numpy(), label=\"Dimension 2\", linestyle='--')\n",
    "plt.plot(pe[:, 2].numpy(), label=\"Dimension 3\", linestyle=':')\n",
    "\n",
    "plt.ylim([-1, 1.1])\n",
    "\n",
    "plt.xlabel(\"Número de secuencia\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe=torch.cat((torch.cos(2*3.14*position/25), torch.sin(2*3.14*position/25),  torch.sin(2*3.14*position/5)), 1)\n",
    "pos_embding=mi_embdings+pe[0:samples,:].numpy()\n",
    "plot_embdings(pos_embding,tokenizer(mi_tokens),vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pe[:, 0].numpy(), label=\"Dimension 1\")\n",
    "plt.plot(pe[:, 1].numpy(), label=\"Dimension 2\")\n",
    "plt.plot(pe[:, 2].numpy(), label=\"Dimension 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, vocab_size=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(vocab_size, d_model)\n",
    "        position = torch.arange(0, vocab_size, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float()\n",
    "            * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_embdings=embedding(torch.tensor(mi_index))\n",
    "mi_embdings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_embdings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer=nn.TransformerEncoderLayer(\n",
    "            d_model=3,\n",
    "            nhead=1,\n",
    "            dim_feedforward=1,\n",
    "            dropout=0,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=encoder_layer(mi_embdings)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dict = encoder_layer.state_dict()\n",
    "for name, param in params_dict.items():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim=3\n",
    "q_proj_weight = encoder_layer.state_dict()['self_attn.in_proj_weight'][0:embed_dim].t()\n",
    "k_proj_weight = encoder_layer.state_dict()['self_attn.in_proj_weight'][embed_dim:2*embed_dim].t()\n",
    "v_proj_weight = encoder_layer.state_dict()['self_attn.in_proj_weight'][2*embed_dim:3*embed_dim].t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q=mi_embdings@q_proj_weight\n",
    "K=mi_embdings@k_proj_weight\n",
    "V=mi_embdings@v_proj_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=Q@K.T/np. sqrt(embed_dim)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head=nn.Softmax(dim=1)(scores)@V\n",
    "head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes mostrar la otra capa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dict = transformer_encoder.state_dict()\n",
    "for name, param in params_dict.items():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Clasificación de texto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter= AG_NEWS(split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y,text= next(iter(train_iter ))\n",
    "print(y,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_news_label = {1: \"World\", 2: \"Sports\", 3: \"Business\", 4: \"Sci/Tec\"}\n",
    "ag_news_label[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = len(set([label for (label, text) in train_iter ]))\n",
    "num_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab([\"age\",\"hello\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Conjunto de datos**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = AG_NEWS()\n",
    "\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "\n",
    "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cargador de datos**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        text_list.append(torch.tensor(text_pipeline(_text), dtype=torch.int64))\n",
    "\n",
    "\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = pad_sequence(text_list, batch_first=True)\n",
    "\n",
    "\n",
    "    return label_list.to(device), text_list.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label,seqence=next(iter(valid_dataloader ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Red neuronal**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        \n",
    "        self,\n",
    "        vocab_size,\n",
    "        num_class,\n",
    "        embedding_dim=100,\n",
    "        nhead=5,\n",
    "        dim_feedforward=2048,\n",
    "        num_layers=6,\n",
    "        dropout=0.1,\n",
    "        activation=\"relu\",\n",
    "        classifier_dropout=0.1):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb = nn.Embedding(vocab_size,embedding_dim)\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(\n",
    "            d_model=embedding_dim,\n",
    "            dropout=dropout,\n",
    "            vocab_size=vocab_size,\n",
    "        )\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.classifier = nn.Linear(embedding_dim, num_class)\n",
    "        self.d_model = embedding_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y,x=next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emsize=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creando el modelo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = Net(vocab_size=vocab_size,num_class=4).to(device)\n",
    "modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label=modelo(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.unsqueeze(torch.tensor(text_pipeline(text)),0).to(device)\n",
    "\n",
    "        output = modelo(text)\n",
    "        return ag_news_label[output.argmax(1).item() + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"I like sports and stuff\",text_pipeline )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model_eval):\n",
    "    model_eval.eval()\n",
    "    total_acc, total_count= 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text) in enumerate(dataloader):\n",
    "            predicted_label = model_eval(text.to(device))\n",
    "\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(test_dataloader, modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR=0.1\n",
    "\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(modelo.parameters(), lr=LR)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenando el modelo para 10 épocas.\n",
    "\n",
    ">Omite este paso si no tienes GPU. Recupera y usa el modelo entrenado para 100 épocas y guardado en el siguiente paso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCHS = 10\n",
    "# cum_loss_list=[]\n",
    "# acc_epoch=[]\n",
    "# acc_old=0\n",
    "\n",
    "# for epoch in tqdm(range(1, EPOCHS + 1)):\n",
    "#     modelo.train()\n",
    "#     cum_loss=0\n",
    "#     for idx, (label, text) in enumerate(train_dataloader):\n",
    "#         optimizer.zero_grad()\n",
    "#         label, text=label.to(device), text.to(device)\n",
    "\n",
    "\n",
    "#         predicted_label = modelo(text)\n",
    "#         loss = criterion(predicted_label, label)\n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(modelo.parameters(), 0.1)\n",
    "#         optimizer.step()\n",
    "#         cum_loss+=loss.item()\n",
    "#     print(\"Loss\",cum_loss)\n",
    "\n",
    "#     cum_loss_list.append(cum_loss)\n",
    "#     accu_val = evaluate(valid_dataloader)\n",
    "#     acc_epoch.append(accu_val)\n",
    "\n",
    "#     if accu_val > acc_old:\n",
    "#       acc_old= accu_val\n",
    "#       torch.save(modelo.state_dict(), 'mi_modelo.pth')\n",
    "\n",
    "# save_list_to_file(lst=cum_loss_list, filename=\"loss.pkl\")\n",
    "# save_list_to_file(lst=acc_epoch, filename=\"acc.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tienes la capacidad de subir el modelo entrenado junto con datos completos sobre la pérdida acumulada y la precisión promedio en cada época.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicios**\n",
    "\n",
    "0. **Integrar un modelo pre-entrenado (BERT) en el código existente**\n",
    "\n",
    "   1. Instala la librería Transformers:\n",
    "\n",
    "      ```bash\n",
    "      pip install transformers\n",
    "      ```\n",
    "   2. En el cuaderno, sustituye la construcción de embeddings y el `TransformerEncoder` de la clase `Net` por un backbone de BERT:\n",
    "\n",
    "      ```python\n",
    "      from transformers import BertModel, BertTokenizer\n",
    "\n",
    "      tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "      backbone  = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "      ```\n",
    "   3. Modifica la clase `Net` para que reciba `backbone` en su constructor y, en `forward`, haga:\n",
    "\n",
    "      ```python\n",
    "      tokens = tokenizer(text,\n",
    "                         padding=\"max_length\",\n",
    "                         truncation=True,\n",
    "                         max_length=128,\n",
    "                         return_tensors=\"pt\")\n",
    "      outputs = backbone(input_ids=tokens.input_ids.to(device),\n",
    "                         attention_mask=tokens.attention_mask.to(device))\n",
    "      pooled  = outputs.pooler_output           # [batch_size, hidden_size]\n",
    "      logits  = self.classifier(self.dropout(pooled))\n",
    "      ```\n",
    "   4. Ajusta el `collate_batch` para devolver `text` en bruto y pasa el texto al modelo en lugar de tensores de índices.\n",
    "   5. Entrena sobre AG\\_NEWS durante **5 epocas** con `Adam(lr=2e-5)` y `criterion = CrossEntropyLoss()`.\n",
    "   6. Al finalizar, imprime en pantalla:\n",
    "\n",
    "      * **Shape de salida**: `(batch_size, 4)`\n",
    "\n",
    "      * **Accuracy en test** (esperado): \\~ 92 %\n",
    "\n",
    "      * **Matriz de confusión** `4×4`\n",
    "\n",
    "      * **Reporte de clasificación** (Precision/Recall/F1 por clase)\n",
    "\n",
    "   > **Ejemplo de salida esperada**\n",
    "   >\n",
    "   > ```\n",
    "   > Shape logits: torch.Size([32, 4])\n",
    "   > Test accuracy: 0.91–0.93\n",
    "   > Confusion matrix:\n",
    "   > [[6121  120   50   30]\n",
    "   >  [  80 5850  200   20]\n",
    "   >  [  40  180 6000  100]\n",
    "   >  [  35   15  125 6325]]\n",
    "   > Classification report:\n",
    "   >               precision    recall  f1-score   support\n",
    "   >\n",
    "   >            0       0.98      0.97      0.98      6321\n",
    "   >            1       0.95      0.97      0.96      6050\n",
    "   >            2       0.94      0.95      0.95      6320\n",
    "   >            3       0.97      0.97      0.97      6500\n",
    "   >\n",
    "   >     accuracy                           0.92     25191\n",
    "   >    macro avg       0.96      0.96      0.96     25191\n",
    "   > weighted avg       0.96      0.92      0.96     25191\n",
    "   > ```\n",
    "\n",
    "1. **Habilitar el training loop y configurar optimizadores, pérdida y scheduler**\n",
    "\n",
    "   1. Descomenta el bloque que define el bucle de entrenamiento.\n",
    "   2. Configura al menos dos optimizadores distintos (por ejemplo, `SGD` y `Adam`) y compara sus resultados.\n",
    "   3. Define `criterion = nn.CrossEntropyLoss()`.\n",
    "   4. Añade un scheduler (por ejemplo, `StepLR` o `CosineAnnealingLR`) y muestra su estado en cada epoch.\n",
    "   5. Ejecuta el entrenamiento durante **10 epocas**, registrando pérdida y precisión de validación en listas.\n",
    "   6. Usa `plot(COST, ACC)` para graficar las curvas de entrenamiento y validación.\n",
    "\n",
    "2. **Guardado y recarga de checkpoints**\n",
    "\n",
    "   1. Al final de cada epoca, comprueba si la precisión en validación ha mejorado.\n",
    "   2. Si ha mejorado, guarda un checkpoint:\n",
    "\n",
    "      ```python\n",
    "      torch.save({\n",
    "          'epoch': epoch,\n",
    "          'model_state': modelo.state_dict(),\n",
    "          'optimizer_state': optimizer.state_dict(),\n",
    "          'best_acc': best_acc,\n",
    "      }, 'checkpoint_best.pth')\n",
    "      ```\n",
    "   3. Implementa `load_checkpoint(path, modelo, optimizer=None)` para recargar modelo y optimizador.\n",
    "   4. Detén el entrenamiento tras 5 epocas, recarga el checkpoint y continúa 5 epocas más, verificando que las métricas sean consistentes.\n",
    "\n",
    "\n",
    "3. **Métricas avanzadas y logging en TensorBoard**\n",
    "\n",
    "   1. Durante la evaluación, además de `accuracy`, calcula:\n",
    "\n",
    "      * Matriz de confusión (`sklearn.metrics.confusion_matrix`).\n",
    "      * Precision, Recall y F1-score por clase (`sklearn.metrics.classification_report`).\n",
    "   2. Inicializa un `SummaryWriter` y registra en cada epoch:\n",
    "\n",
    "      * Scalars: `loss_train`, `loss_val`, `acc_train`, `acc_val`.\n",
    "      * Matriz de confusión como imagen.\n",
    "      * Reporte de clasificación como texto o tabla.\n",
    "   3. Ejecuta `tensorboard --logdir=runs` y verifica los gráficos y métricas.\n",
    "\n",
    "\n",
    "4. **Más robustez en DataLoader y máscaras de padding**\n",
    "\n",
    "   1. Modifica `collate_batch` para devolver también `src_key_padding_mask` de forma `(batch_size, seq_len)`.\n",
    "   2. Pasa esta máscara al `TransformerEncoder`:\n",
    "\n",
    "      ```python\n",
    "      out = encoder(src_emb, src_key_padding_mask=padding_mask)\n",
    "      ```\n",
    "   3. Asegúrate de que los tokens de padding no participen en la atención.\n",
    "   4. Compara la estabilidad de la pérdida con y sin máscara para validar la corrección.\n",
    "\n",
    "\n",
    "5. **Parametrización de hiperparámetros**\n",
    "\n",
    "   1. Añade `argparse` para recibir:\n",
    "\n",
    "      * `--num-layers`, `--num-heads`, `--dim-feedforward`, `--dropout`\n",
    "      * `--batch-size`, `--lr`, `--epochs`\n",
    "   2. Alternativamente, carga un archivo JSON o YAML con los mismos parámetros.\n",
    "   3. Inicializa modelo, optimizador y dataLoaders a partir de estos valores.\n",
    "   4. Ejecuta el script con distintos parámetros y observa el impacto en el rendimiento.\n",
    "\n",
    "\n",
    "6. **Entrenamiento distribuido (DistributedDataParallel)**\n",
    "\n",
    "   1. Inicializa el entorno distribuido con `torch.distributed.init_process_group`.\n",
    "   2. Envuelve el modelo en `torch.nn.parallel.DistributedDataParallel`.\n",
    "   3. Usa `DistributedSampler` en los DataLoaders.\n",
    "   4. Entrena en 2 o más procesos y verifica que el rendimiento escala con el número de GPUs/processes.\n",
    "\n",
    "\n",
    "7. **Empaquetar el modelo como API de inferencia**\n",
    "\n",
    "   1. Elige **FastAPI** o **Flask**.\n",
    "   2. Carga el modelo desde el checkpoint.\n",
    "   3. Define un endpoint `/predict/` que acepte JSON `{ \"text\": \"...\" }`.\n",
    "   4. Aplica `text_pipeline`, construye el tensor, ejecuta `modelo.eval()` y devuelve `{ \"label\": int, \"score\": float }`.\n",
    "   5. Prueba la API con `curl` o un script en `requests`.\n",
    "\n",
    "\n",
    "8. **Reproducibilidad**\n",
    "\n",
    "   1. Al inicio del script, fija semillas:\n",
    "\n",
    "      ```python\n",
    "      seed = 42\n",
    "      torch.manual_seed(seed)\n",
    "      np.random.seed(seed)\n",
    "      random.seed(seed)\n",
    "      if torch.cuda.is_available():\n",
    "          torch.cuda.manual_seed_all(seed)\n",
    "      ```\n",
    "   2. Ejecuta dos entrenamientos idénticos y compara métricas para asegurar consistencia.\n",
    "\n",
    "9. **Documentación y pruebas unitarias con pytest**\n",
    "\n",
    "   1. Añade docstrings a:\n",
    "\n",
    "      * `text_pipeline()`, `label_pipeline()`, `PositionalEncoding`, `Net.forward()`.\n",
    "   2. Crea tests que verifiquen:\n",
    "\n",
    "      * `text_pipeline(\"foo bar\")` devuelve lista de enteros.\n",
    "      * `PositionalEncoding(d_model=4, max_len=10)` genera tensor `(1, 10, 4)`.\n",
    "      * `Net.forward()` con batch sintético devuelve logits `(batch_size, num_classes)`.\n",
    "   3. Asegura cobertura mínima del 80 %.\n",
    "\n",
    "\n",
    "10. **Uso de un modelo pre-entrenado de Hugging Face para clasificación**\n",
    "\n",
    "    1. Carga `bert-base-uncased` con `BertModel.from_pretrained(...)`.\n",
    "    2. Reemplaza embedding y encoder por BERT, congelando o descongelando últimas capas.\n",
    "    3. Entrena sobre AG\\_NEWS y compara precisión y tiempos con el Transformer desde cero.\n",
    "\n",
    "\n",
    "11. **Full fine-tuning vs. fine-tuning parcial**\n",
    "\n",
    "    1. Experimento 1: ajusta todos los parámetros de BERT.\n",
    "    2. Experimento 2: ajusta solo la capa de clasificación.\n",
    "    3. Compara rendimiento y coste computacional en ambos casos.\n",
    "\n",
    "\n",
    "12. **Implementación de LoRA en un Transformer de clasificación**\n",
    "\n",
    "    1. Aplica low-rank adaptation a las proyecciones Q y V de `TransformerEncoderLayer`.\n",
    "    2. Añade matrices de bajo rango aprendibles.\n",
    "    3. Evalúa impacto en precisión y número de parámetros.\n",
    "\n",
    "\n",
    "13. **Adaptación de QLoRA para texto largo**\n",
    "\n",
    "    1. Cuantiza tu modelo pre-entrenado a 4 bits usando QLoRA.\n",
    "    2. Entrena con batch pequeño y observa cambios en memoria y precisión.\n",
    "\n",
    "14. **Visualización de mapas de atención**\n",
    "\n",
    "    1. Extiende `plot_tras` para múltiples cabeceras y capas.\n",
    "    2. Para muestras de test, dibuja atención y describe patrones relevantes.\n",
    "\n",
    "15. **Evaluación con RLHF simplificado**\n",
    "\n",
    "    1. Genera predicciones en validación.\n",
    "    2. Simula una función de recompensa que penalice errores en clases críticas.\n",
    "    3. Ajusta el modelo con policy gradient (por ejemplo, REINFORCE).\n",
    "    4. Compara con el entrenamiento supervisado estándar.\n",
    "\n",
    "\n",
    "16. **Experimentación con objetivos de preentrenamiento**\n",
    "\n",
    "    1. Preentrena un encoder pequeño con MLM y CLM sobre un corpus reducido.\n",
    "    2. Transfiérelo a clasificación y compara con un modelo entrenado desde cero.\n",
    "\n",
    "17. **Benchmark de distilación y cuantización**\n",
    "\n",
    "    1. Aplica knowledge distillation: entrena un modelo student para imitar un teacher grande.\n",
    "    2. Posteriormente aplica post-training quantization al student.\n",
    "    3. Mide precisión vs. tamaño del modelo.\n",
    "\n",
    "18. **Exploración de emergent abilities al aumentar capas**\n",
    "\n",
    "    1. Crea instancias de `Net` con 2, 4, 8 y 12 capas.\n",
    "    2. Entrena brevemente cada una.\n",
    "    3. Grafica precisión vs. número de parámetros y observa posibles saltos de rendimiento inesperados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "prev_pub_hash": "a2b858645f1fcfb4f2d8b8e240d3758016f850dbb407f7fa587cddfb1598bb11"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
