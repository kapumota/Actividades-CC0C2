{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c86dec4c-9300-4bed-8c08-c972a8f1f9fe",
   "metadata": {},
   "source": [
    "## **Respuesta del Examen Parcial CC0C2**\n",
    "\n",
    "### **Respuesta 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce4ecf8-6a48-44d6-a963-260d5b507b9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import Iterator, List, Optional, Tuple, Dict\n",
    "\n",
    "\n",
    "def crear_data_loader(ruta: Optional[str], tamaño_batch: int) -> Iterator[List[str]]:\n",
    "    \"\"\"\n",
    "    Crea un iterador que entrega el corpus en lotes de tamaño dado.\n",
    "    Si ruta es None, genera un mini-corpus de ejemplo.\n",
    "    \"\"\"\n",
    "    if ruta:\n",
    "        with open(ruta, encoding='utf-8') as f:\n",
    "            lines = [line.strip() for line in f if line.strip()]\n",
    "    else:\n",
    "        # Mini-corpus por defecto\n",
    "        lines = [\n",
    "            \"The quick brown fox jumps over the lazy dog\",\n",
    "            \"Byte-Pair Encoding is a simple data compression technique\",\n",
    "            \"ChatGPT genera texto con un modelo de lenguaje\"\n",
    "        ]\n",
    "    # Generar lotes\n",
    "    def _batcher(data):\n",
    "        for i in range(0, len(data), tamaño_batch):\n",
    "            yield data[i:i + tamaño_batch]\n",
    "\n",
    "    return _batcher(lines)\n",
    "\n",
    "\n",
    "def entrenar_bpe_simple(\n",
    "    corpus: List[str],\n",
    "    num_merges: int\n",
    ") -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Entrena reglas BPE de forma simple:\n",
    "    - Tokeniza a nivel carácter con </w>\n",
    "    - En cada paso, encuentra el par más frecuente y lo fusiona\n",
    "    - Actualiza frecuencias sólo para palabras afectadas (actualización incremental)\n",
    "    \"\"\"\n",
    "    # Tokenizar cada palabra del corpus en lista de símbolos\n",
    "    words: List[List[str]] = []\n",
    "    for sentence in corpus:\n",
    "        for word in sentence.split():\n",
    "            words.append(list(word) + ['</w>'])\n",
    "\n",
    "    # Frecuencias iniciales de pares adyacentes\n",
    "    def get_pair_freq(words_list: List[List[str]]) -> Dict[Tuple[str,str], int]:\n",
    "        freqs: Dict[Tuple[str,str], int] = {}\n",
    "        for symbols in words_list:\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pair = (symbols[i], symbols[i+1])\n",
    "                freqs[pair] = freqs.get(pair, 0) + 1\n",
    "        return freqs\n",
    "\n",
    "    pair_freqs = get_pair_freq(words)\n",
    "    rules: List[Tuple[str, str]] = []\n",
    "    history: List[Tuple[int, Tuple[str, str], int, int]] = []\n",
    "\n",
    "    for i in range(1, num_merges + 1):\n",
    "        if not pair_freqs:\n",
    "            break\n",
    "        # Seleccionar el par más frecuente\n",
    "        best_pair, freq_before = max(pair_freqs.items(), key=lambda x: x[1])\n",
    "        a, b = best_pair\n",
    "        merged = a + b\n",
    "        # Aplicar fusión sólo en palabras que contienen el par\n",
    "        for symbols in words:\n",
    "            j = 0\n",
    "            while j < len(symbols) - 1:\n",
    "                if symbols[j] == a and symbols[j+1] == b:\n",
    "                    symbols[j] = merged\n",
    "                    symbols.pop(j+1)\n",
    "                else:\n",
    "                    j += 1\n",
    "        # Recalcular frecuencias en todo el corpus (puede optimizarse)\n",
    "        new_freqs = get_pair_freq(words)\n",
    "        freq_after = new_freqs.get(best_pair, 0)\n",
    "        history.append((i, best_pair, freq_before, freq_after))\n",
    "        rules.append(best_pair)\n",
    "        pair_freqs = new_freqs\n",
    "\n",
    "    # Imprimir primeras 5 reglas con sus frecuencias\n",
    "    print(\"Primeras 5 fusiones (iter, par, freq antes -> freq después):\")\n",
    "    for it, pair, bef, aft in history[:5]:\n",
    "        print(f\"{it}: {pair} -> {bef} -> {aft}\")\n",
    "    return rules\n",
    "\n",
    "\n",
    "def tokenizar_con_bpe(texto: str, reglas: List[Tuple[str,str]]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokeniza texto aplicando reglas BPE en orden.\n",
    "    \"\"\"\n",
    "    tokens: List[str] = []\n",
    "    for word in texto.split():\n",
    "        symbols = list(word) + ['</w>']\n",
    "        for a, b in reglas:\n",
    "            merged = a + b\n",
    "            j = 0\n",
    "            while j < len(symbols) - 1:\n",
    "                if symbols[j] == a and symbols[j+1] == b:\n",
    "                    symbols[j] = merged\n",
    "                    symbols.pop(j+1)\n",
    "                else:\n",
    "                    j += 1\n",
    "        # Cada símbolo (incluyendo los merges) conserva </w> en sus tokens\n",
    "        for sym in symbols:\n",
    "            tokens.append(sym)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def detokenizar_bpe(tokens: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Reconstruye texto original a partir de tokens BPE.\n",
    "    Maneja tokens que contienen el sufijo '</w>'.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for tok in tokens:\n",
    "        if tok.endswith('</w>'):\n",
    "            # Sufijo indica fin de palabra\n",
    "            word = tok[:-4]\n",
    "            result.append(word)\n",
    "            result.append(' ')\n",
    "        else:\n",
    "            result.append(tok)\n",
    "    text = ''.join(result).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Demo mínima\n",
    "    corpus = [\n",
    "        \"low low lower\",\n",
    "        \"lowest lower lowest\"\n",
    "    ]\n",
    "    num_merges = 20\n",
    "    reglas = entrenar_bpe_simple(corpus, num_merges)\n",
    "\n",
    "    # Vocabulario inicial y final\n",
    "    vocab_inicial = set()\n",
    "    for sent in corpus:\n",
    "        for w in sent.split():\n",
    "            vocab_inicial.update(list(w) + ['</w>'])\n",
    "    vocab_final = set(tokenizar_con_bpe(' '.join(corpus), reglas))\n",
    "\n",
    "    # Round-trip con asserts\n",
    "    frases = [\"low lower\", \"lowest low\"]\n",
    "    for original in frases:\n",
    "        tokens = tokenizar_con_bpe(original, reglas)\n",
    "        recovered = detokenizar_bpe(tokens)\n",
    "        assert recovered == original, f\"Round-trip falló: {original}\"\n",
    "        print(\"Round-trip OK para:\", original)\n",
    "\n",
    "    # Comprobaciones adicionales\n",
    "    assert len(vocab_inicial) > len(vocab_final), \"El vocabulario no se ha reducido\"\n",
    "    assert any(pair in reglas for pair in [(\"l\",\"o\"), (\"e\",\"s\")]), \"No se detectaron pares comunes\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe2d93e-5098-4086-a1b9-b9c6b9caa843",
   "metadata": {},
   "source": [
    "### **Respuesta 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f31c8b-1a66-45b9-b1fd-b3c2683f36a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "\n",
    "def crear_embeddings_semilla(vocab: List[str], dim: int = 50, seed: int = 42) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Genera embeddings reproducibles para cada palabra del vocabulario,\n",
    "    usando una distribución gaussiana estándar.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    embeddings = {word: rng.standard_normal(dim) for word in vocab}\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def similitud_coseno(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calcula la similitud coseno entre dos vectores.\n",
    "    \"\"\"\n",
    "    dot = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0.0\n",
    "    return dot / (norm1 * norm2)\n",
    "\n",
    "\n",
    "def encontrar_palabras_mas_similares(\n",
    "    palabra_objetivo: str,\n",
    "    embeddings: Dict[str, np.ndarray],\n",
    "    top_n: int = 5\n",
    ") -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Devuelve las top_n palabras más similares a la palabra_objetivo\n",
    "    según similitud coseno. Maneja out-of-vocabulary.\n",
    "    \"\"\"\n",
    "    if palabra_objetivo not in embeddings:\n",
    "        raise ValueError(f\"'{palabra_objetivo}' no está en el vocabulario.\")\n",
    "    vec_target = embeddings[palabra_objetivo]\n",
    "    scores = []\n",
    "    for w, vec in embeddings.items():\n",
    "        if w == palabra_objetivo:\n",
    "            continue\n",
    "        score = similitud_coseno(vec_target, vec)\n",
    "        scores.append((w, score))\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scores[:top_n]\n",
    "\n",
    "\n",
    "def resolver_analogia(\n",
    "    w1: str, w2: str, w3: str,\n",
    "    embeddings: Dict[str, np.ndarray]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Resuelve analogía w1:w2 :: w3:? usando vec(w2)-vec(w1)+vec(w3)\n",
    "    y devuelve la palabra más similar, excluyendo w1,w2,w3.\n",
    "    \"\"\"\n",
    "    for w in (w1, w2, w3):\n",
    "        if w not in embeddings:\n",
    "            raise ValueError(f\"'{w}' no está en el vocabulario.\")\n",
    "    vec = embeddings[w2] - embeddings[w1] + embeddings[w3]\n",
    "    best_word = None\n",
    "    best_score = -np.inf\n",
    "    for w, emb in embeddings.items():\n",
    "        if w in (w1, w2, w3):\n",
    "            continue\n",
    "        score = similitud_coseno(vec, emb)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_word = w\n",
    "    return best_word\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Definir un vocabulario pequeño con pares de género y profesiones\n",
    "    vocab = [\n",
    "        'hombre', 'mujer', 'rey', 'reina', 'doctor', 'doctora',\n",
    "        'maestro', 'maestra', 'ingeniero', 'ingeniera', 'hospital',\n",
    "        'escuela', 'ciudad', 'palacio', 'profesor', 'profesora',\n",
    "        'rey', 'reina', 'príncipe', 'princesa', 'actor', 'actriz',\n",
    "        'rey', 'reina', 'agua', 'fuego', 'tierra', 'aire', 'sol', 'luna'\n",
    "    ]\n",
    "\n",
    "    # 1) Crear embeddings sintéticos\n",
    "    embeddings = crear_embeddings_semilla(vocab, dim=50, seed=123)\n",
    "\n",
    "    # 2) Búsqueda de vecinos más similares para 3 palabras\n",
    "    prueba_palabras = ['doctor', 'mañana', 'ingeniero']\n",
    "    for palabra in ['doctor', 'maestro', 'ingeniero']:\n",
    "        vecinos = encontrar_palabras_mas_similares(palabra, embeddings, top_n=5)\n",
    "        print(f\"Vecinos de '{palabra}':\")\n",
    "        for w, score in vecinos:\n",
    "            print(f\"  {w}: {score:.4f}\")\n",
    "        print()\n",
    "\n",
    "    # 3) Resolver analogías\n",
    "    analogias = [\n",
    "        ('rey', 'hombre', 'mujer'),\n",
    "        ('doctor', 'hospital', 'maestra')\n",
    "    ]\n",
    "    for w1, w2, w3 in analogias:\n",
    "        respuesta = resolver_analogia(w1, w2, w3, embeddings)\n",
    "        print(f\"{w1} : {w2} :: {w3} : {respuesta}\")\n",
    "    print()\n",
    "\n",
    "    # 4) Experimento de sesgo\n",
    "    # Medir similitud de 'ingeniero' con 'hombre' y 'mujer'\n",
    "    sim_h = similitud_coseno(embeddings['ingeniero'], embeddings['hombre'])\n",
    "    sim_m = similitud_coseno(embeddings['ingeniero'], embeddings['mujer'])\n",
    "    print(\"Similitud(ingeniero, hombre):\", f\"{sim_h:.4f}\")\n",
    "    print(\"Similitud(ingeniero, mujer):\", f\"{sim_m:.4f}\")\n",
    "    if sim_h > sim_m:\n",
    "        print(\"Potencial sesgo: 'ingeniero' está más cerca de 'hombre'.\")\n",
    "    else:\n",
    "        print(\"Potencial sesgo: 'ingeniero' está más cerca de 'mujer'.\")\n",
    "    print()\n",
    "\n",
    "    print(\"Nota: con vectores aleatorios, estas correlaciones son fortuitas.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b1b9cc-a32b-4dec-bd6e-2fd98a13d9eb",
   "metadata": {},
   "source": [
    "### **Respuesta 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf08fd0a-77d4-41ec-8a2f-e291cdb74b3a",
   "metadata": {},
   "source": [
    "#### **Diferencias entre greedy decoding, beam Search y muestreo con temperatura**\n",
    "\n",
    "Estos son tres métodos para seleccionar el siguiente token en una secuencia durante la fase de generación (inferencia) de un modelo de lenguaje.\n",
    "\n",
    "* **Greedy decoding:**\n",
    "    * **Explicación:** En cada paso de tiempo, selecciona el token con la probabilidad más alta según la salida del modelo. Es el enfoque más simple y rápido.\n",
    "    * **Ventajas:** Muy rápido y computacionalmente barato.\n",
    "    * **Desventajas:** Puede llevar a secuencias subóptimas. Una elección localmente óptima (la palabra más probable ahora) puede cerrar la puerta a una secuencia global mucho mejor. A menudo genera texto repetitivo y predecible.\n",
    "\n",
    "* **Beam search:**\n",
    "    * **Explicación:** En lugar de mantener una única secuencia candidata (la mejor), mantiene `k` secuencias candidatas (el \"haz\" o *beam*). En cada paso, expande cada una de las `k` secuencias con todos los posibles siguientes tokens del vocabulario. De todas las nuevas secuencias generadas, se queda con las `k` mejores según su probabilidad acumulada (generalmente se usa la suma de log-probabilidades para evitar underflow numérico).\n",
    "    * **Ventajas:** Explora un espacio de búsqueda más amplio que el greedy decoding, lo que generalmente conduce a secuencias de mayor calidad y más probables.\n",
    "    * **Desventajas:** Es computacionalmente más costoso. Requiere `k` veces más memoria y cómputo que el greedy search. Puede seguir favoreciendo secuencias de alta probabilidad pero \"aburridas\" o repetitivas.\n",
    "\n",
    "* **Muestreo con temperatura:**\n",
    "    * **Explicación:** Introduce aleatoriedad en la selección de tokens. Antes de aplicar la función softmax a las salidas del modelo (logits), estos se dividen por un valor de **temperatura** ($T$).\n",
    "        * Si $T \\to 0$, el muestreo se vuelve determinista, similar al greedy decoding.\n",
    "        * Si $T = 1$, se muestrea de la distribución de probabilidad original del modelo.\n",
    "        * Si $T > 1$, la distribución de probabilidad se vuelve más uniforme, aumentando la probabilidad de que se seleccionen tokens menos probables. Esto introduce más diversidad y \"creatividad\" en el texto.\n",
    "    * **Ventajas:** Permite controlar el equilibrio entre aleatoriedad y determinismo. Útil para tareas creativas donde la diversidad es deseable.\n",
    "    * **Desventajas:** Una temperatura demasiado alta puede generar texto incoherente y sin sentido. El resultado no es reproducible a menos que se fije la semilla aleatoria.\n",
    "\n",
    "#### **Uso de *teacher forcing**\n",
    "\n",
    "El **teacher forcing** es una técnica de entrenamiento para modelos recurrentes (RNN, LSTM, etc.) en tareas de secuencia a secuencia.\n",
    "\n",
    "* **¿Cuándo y por qué emplearlo?:** Se emplea durante el **entrenamiento**. En lugar de alimentar al siguiente paso de tiempo la salida que el propio modelo predijo en el paso anterior, se le alimenta el **token correcto** (la verdad fundamental o *ground truth*) de la secuencia objetivo.\n",
    "* **Ventajas:**\n",
    "    1.  **Acelera la convergencia:** Al recibir siempre la entrada correcta, el modelo aprende más rápido y el entrenamiento es mucho más estable.\n",
    "    2.  **Paralelización:** Permite que los cálculos para cada paso de tiempo se realicen en paralelo, ya que la entrada en el tiempo $t$ no depende de la salida del modelo en $t-1$.\n",
    "* **Problema asociado (exposure bias):** Durante la inferencia, el modelo solo tiene acceso a sus propias predicciones, que pueden contener errores. Como nunca fue expuesto a sus propios errores durante el entrenamiento, puede cometer un error y desviarse significativamente, generando secuencias de baja calidad.\n",
    "\n",
    "\n",
    "#### **Limitación del vector de contexto y la atención de Bahdanau**\n",
    "\n",
    "* **Problema del vector de contexto único:** En la arquitectura Seq2Seq clásica, el *encoder* comprime toda la información de la oración de entrada en un único vector de tamaño fijo, llamado **vector de contexto**. Este vector es la única información que el *decoder* recibe sobre la entrada. Para oraciones largas, es extremadamente difícil (si no imposible) comprimir todos los matices y dependencias en este vector sin una pérdida significativa de información. Esto actúa como un **cuello de botella** (*bottleneck*), degradando el rendimiento en secuencias largas.\n",
    "\n",
    "* **Atención \"global\" de Bahdanau:** La atención de Bahdanau soluciona este problema permitiendo que el *decoder* \"mire\" a todas las salidas ocultas del *encoder* en cada paso de decodificación.\n",
    "    1.  En cada paso del decoder (al generar una palabra), el estado oculto actual del decoder se compara con **todos** los estados ocultos del encoder.\n",
    "    2.  Esta comparación genera una serie de **pesos de atención** (o *alignment scores*), que se normalizan con una función softmax. Estos pesos indican qué partes de la oración de entrada son más relevantes para generar la palabra actual.\n",
    "    3.  Se calcula un **vector de contexto dinámico** como una suma ponderada de los estados ocultos del encoder, usando los pesos de atención.\n",
    "    4.  Este vector de contexto dinámico, específico para el paso de tiempo actual, se concatena con el estado oculto del decoder y se utiliza para predecir la siguiente palabra.\n",
    "\n",
    "De esta forma, el modelo no depende de un único vector fijo, sino que crea un atajo focalizado a las partes relevantes de la entrada en cada paso, solucionando el problema del cuello de botella.\n",
    "\n",
    "\n",
    "#### **Función de la máscara de atención con teacher forcing**\n",
    "\n",
    "En arquitecturas como los Transformers, que no son inherentemente secuenciales, se puede procesar toda la secuencia a la vez. Cuando se usa *teacher forcing* en el *decoder* de un Transformer, se le alimenta la secuencia de salida completa.\n",
    "\n",
    "La **máscara de atención** (o *look-ahead mask*) es crucial aquí. Su función es asegurar que, al predecir el token en la posición $t$, el modelo solo pueda atender a los tokens en posiciones anteriores ($< t$) y no a los futuros ($\\ge t$). Impide que el modelo \"haga trampa\" mirando la respuesta correcta que viene después en la secuencia. Esto preserva la propiedad **autorregresiva** del modelo, forzándolo a aprender a predecir el siguiente token basándose únicamente en los tokens anteriores.\n",
    "\n",
    "\n",
    "#### **Gradient Clipping**\n",
    "\n",
    "El *gradient clipping* es una técnica para mitigar el problema de la **explosión de gradientes**, común en las RNNs. La explosión de gradientes ocurre cuando los gradientes crecen exponencialmente durante la retropropagación a través del tiempo, resultando en actualizaciones de pesos enormes que desestabilizan el entrenamiento (a menudo resultando en `NaN`).\n",
    "\n",
    "* **Gradient clipping por valor (clip by value):** Establece un umbral mínimo y máximo (ej., `[-c, c]`). Si un gradiente está fuera de este rango, se recorta a ese valor límite. `grad = max(min_val, min(max_val, grad))`.\n",
    "* **Gradient clipping por norma (clip by norm):** Calcula la norma L2 de todo el vector de gradientes de un parámetro (o de todos los parámetros juntos). Si esta norma excede un umbral `max_norm`, todo el vector de gradientes se reescala para que su norma sea igual a `max_norm`, manteniendo su dirección original. $$\\text{Si } \\|\\mathbf{g}\\| > \\text{max\\_norm}, \\text{ entonces } \\mathbf{g} \\leftarrow \\frac{\\text{max\\_norm}}{\\|\\mathbf{g}\\|} \\mathbf{g}$$\n",
    "\n",
    "**Efecto negativo de un umbral demasiado bajo:** Si el umbral de *clipping* es demasiado bajo, el modelo puede aprender muy lentamente. Se estarían \"frenando\" incluso las actualizaciones de gradientes legítimamente grandes que son necesarias para un aprendizaje rápido y efectivo, lo que podría impedir que el modelo converja a una buena solución.\n",
    "\n",
    "#### **Teacher-forcing ratio y estrategias de reducción**\n",
    "\n",
    "La ***teacher-forcing ratio*** es la probabilidad con la que se utilizará la técnica de *teacher forcing* en un paso de entrenamiento. Un ratio de 1.0 significa usar siempre *teacher forcing*, y un ratio de 0.0 significa no usarlo nunca (el modelo se alimenta de sus propias predicciones, como en la inferencia).\n",
    "\n",
    "Reducir este ratio a lo largo del entrenamiento ayuda a mitigar el *exposure bias*. Al exponer gradualmente al modelo a sus propias predicciones (y errores potenciales), se vuelve más robusto y su rendimiento en la inferencia mejora.\n",
    "\n",
    "**Dos escenarios para reducirla:**\n",
    "\n",
    "1.  **Decaimiento lineal:**\n",
    "    * **Propuesta:** Empezar con un ratio de 1.0 y disminuirlo linealmente en cada época (o cada N iteraciones) hasta que llegue a 0.0 (o a un valor pequeño como 0.1) hacia el final del entrenamiento.\n",
    "    * **Justificación:** Es una estrategia simple y efectiva. Al principio, cuando el modelo es inestable, se beneficia de la guía constante del *teacher forcing*. A medida que aprende, se le \"quita\" gradualmente, forzándolo a aprender a corregir sus propios errores y a ser más robusto ante las condiciones de la inferencia.\n",
    "\n",
    "2.  **Muestreo programado (scheduled sampling):**\n",
    "    * **Propuesta:** En lugar de decaer el ratio para todo el batch, se puede decidir estocásticamente para cada instancia o paso de tiempo si usar *teacher forcing* o no, basándose en un ratio que decae. Una versión más avanzada es hacer que la probabilidad de usar la predicción del modelo en lugar del *ground truth* sea una función del número de época (ej., `k / (k + exp(epoch / k))` para alguna constante `k`).\n",
    "    * **Justificación:** Introduce una transición más suave y estocástica entre el entrenamiento y la inferencia. Al forzar al modelo a enfrentarse a sus errores de forma aleatoria, se puede lograr una mayor generalización y robustez en comparación con un decaimiento determinista.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b167102-75dc-4f98-adab-daa4d1b89a19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import time\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# 0. Datos \n",
    "sentences = [\n",
    "    \"I love this place!\", \"Worst service ever.\", \"The food was amazing.\",\n",
    "    \"I will never come back.\", \"Absolutely fantastic experience!\", \"Terrible, simply terrible.\",\n",
    "    \"Not bad at all.\", \"It was okay, nothing special.\", \"I’m delighted with the result.\",\n",
    "    \"This is disappointing.\", \"Great job, team!\", \"I hate waiting so long.\",\n",
    "    \"Superb quality and taste.\", \"The product broke instantly.\", \"Highly recommend it.\",\n",
    "    \"Save your money, skip this.\", \"Totally worth the price.\", \"Service was rude and slow.\",\n",
    "    \"Exceeded my expectations.\", \"I regret buying this.\"\n",
    "]\n",
    "labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
    "\n",
    "# 1. Preprocesamiento\n",
    "\n",
    "# a. Tokenización (simple split)\n",
    "tokenized_sentences = [s.lower().split() for s in sentences]\n",
    "\n",
    "# b. Creación del vocabulario ordenado por frecuencia, añadiendo tokens especiales\n",
    "word_counts = Counter(word for s in tokenized_sentences for word in s)\n",
    "vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "word_to_idx = {word: i+2 for i, word in enumerate(vocab)}\n",
    "word_to_idx['<PAD>'] = 0\n",
    "word_to_idx['<UNK>'] = 1\n",
    "idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
    "vocab_size = len(word_to_idx)\n",
    "\n",
    "# c. Conversión a índices y d. Padding\n",
    "indexed = [\n",
    "    [word_to_idx.get(w, word_to_idx['<UNK>']) for w in s]\n",
    "    for s in tokenized_sentences\n",
    "]\n",
    "max_len = max(len(seq) for seq in indexed)\n",
    "padded = np.array([\n",
    "    seq + [word_to_idx['<PAD>']] * (max_len - len(seq))\n",
    "    for seq in indexed\n",
    "])\n",
    "\n",
    "print(f\"Tamaño del vocabulario: {vocab_size}\")\n",
    "print(f\"Longitud máxima de secuencia: {max_len}\\n\")\n",
    "\n",
    "# Datos PyTorch\n",
    "features = torch.from_numpy(padded).long()\n",
    "targets  = torch.tensor(labels).float().unsqueeze(1)\n",
    "\n",
    "dataset = TensorDataset(features, targets)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size  = len(dataset) - train_size\n",
    "train_ds, test_ds = random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=4)\n",
    "\n",
    "# 2. Construcción del modelo\n",
    "\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim,\n",
    "                 rnn_type='RNN', n_layers=1, dropout=0.2):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        # 2.1 Embedding con padding_idx=0\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "\n",
    "        # 2.2 Selección de RNN/GRU/LSTM y ajuste de dropout solo si n_layers>1\n",
    "        dp = dropout if n_layers > 1 else 0.0\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
    "                               dropout=dp, batch_first=True)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(embedding_dim, hidden_dim, n_layers,\n",
    "                              dropout=dp, batch_first=True)\n",
    "        else:\n",
    "            self.rnn = nn.RNN(embedding_dim, hidden_dim, n_layers,\n",
    "                              dropout=dp, batch_first=True)\n",
    "\n",
    "        # 2.3 Dropout y capa final\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc      = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len]\n",
    "        embedded = self.embedding(x)           # [batch, seq_len, emb_dim]\n",
    "        rnn_out, hidden = self.rnn(embedded)\n",
    "        # hidden:\n",
    "        #  - RNN/GRU -> tensor [n_layers, batch, hidden_dim]\n",
    "        #  - LSTM    -> tuple (h_n, c_n), cada uno [n_layers, batch, hidden_dim]\n",
    "        if isinstance(hidden, tuple):\n",
    "            # LSTM\n",
    "            h_n, c_n    = hidden\n",
    "            last_hidden = h_n[-1]             # [batch, hidden_dim]\n",
    "        else:\n",
    "            # RNN o GRU\n",
    "            last_hidden = hidden[-1]          # [batch, hidden_dim]\n",
    "\n",
    "        out = self.dropout(last_hidden)       # [batch, hidden_dim]\n",
    "        out = self.fc(out)                    # [batch, 1]\n",
    "        return out\n",
    "\n",
    "#3. Entrenamiento y 4. Evaluación \n",
    "\n",
    "def train_and_evaluate(modelo, model_name):\n",
    "    print(f\"-{model_name}\")\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(modelo.parameters(), lr=0.005)\n",
    "    epochs    = 25\n",
    "\n",
    "    start = time.time()\n",
    "    modelo.train()\n",
    "    for _ in range(epochs):\n",
    "        for xb, yb in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            logits = modelo(xb)\n",
    "            loss   = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    end = time.time()\n",
    "    print(f\"Tiempo de entrenamiento: {end-start:.3f}s\")\n",
    "\n",
    "    # Evaluación\n",
    "    modelo.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            out = modelo(xb)\n",
    "            preds = torch.round(torch.sigmoid(out))\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(yb.cpu().numpy())\n",
    "\n",
    "    all_preds  = np.array(all_preds).flatten()\n",
    "    all_labels = np.array(all_labels).flatten()\n",
    "    p = precision_score(all_labels, all_preds, zero_division=0)\n",
    "    r = recall_score(all_labels, all_preds, zero_division=0)\n",
    "    f = f1_score(all_labels, all_preds, zero_division=0)\n",
    "    print(f\"Precisión: {p:.4f}, Recall: {r:.4f}, F1: {f:.4f}\\n\")\n",
    "\n",
    "# 5. Comparación de modelos \n",
    "\n",
    "embedding_dim = 32\n",
    "hidden_dim    = 32\n",
    "n_layers      = 1\n",
    "\n",
    "# 5.1 RNN simple\n",
    "model_rnn = SentimentClassifier(vocab_size, embedding_dim, hidden_dim,\n",
    "                                rnn_type='RNN', n_layers=n_layers, dropout=0.2)\n",
    "train_and_evaluate(model_rnn, \"RNN Simple\")\n",
    "\n",
    "# 5.2 LSTM\n",
    "model_lstm = SentimentClassifier(vocab_size, embedding_dim, hidden_dim,\n",
    "                                 rnn_type='LSTM', n_layers=n_layers, dropout=0.2)\n",
    "train_and_evaluate(model_lstm, \"LSTM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b0bd44-0d0d-4e92-ae8b-308ce31e2483",
   "metadata": {},
   "source": [
    "**Simulador de decoder Seq2Seq con beam search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e7bbc8-8b46-4d63-8ecf-e97d4990d894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Simulación del entorno ---\n",
    "\n",
    "# Vocabulario pequeño\n",
    "vocab = {\n",
    "    '<SOS>': 0, 'A': 1, 'B': 2, 'C': 3, '<EOS>': 4, 'D': 5\n",
    "}\n",
    "idx_to_word = {i: w for w, i in vocab.items()}\n",
    "\n",
    "def funcion_paso_modelo(secuencia_parcial_indices):\n",
    "    \"\"\"\n",
    "    Función simulada que devuelve una distribución de probabilidad\n",
    "    sobre el siguiente token. Tiene una lógica simple y predecible.\n",
    "    \"\"\"\n",
    "    last_token_idx = secuencia_parcial_indices[-1]\n",
    "    \n",
    "    # Probabilidades por defecto (distribución uniforme)\n",
    "    probs = np.full(len(vocab), 0.1)\n",
    "\n",
    "    if last_token_idx == vocab['<SOS>']:\n",
    "        # Al inicio, favorecer 'A'\n",
    "        probs[vocab['A']] = 0.8\n",
    "    elif last_token_idx == vocab['A']:\n",
    "        # Después de 'A', favorecer 'B' o 'C'\n",
    "        probs[vocab['B']] = 0.6\n",
    "        probs[vocab['C']] = 0.3\n",
    "    elif last_token_idx == vocab['B']:\n",
    "        # Después de 'B', favorecer 'C' o '<EOS>'\n",
    "        probs[vocab['C']] = 0.5\n",
    "        probs[vocab['<EOS>']] = 0.4\n",
    "    elif last_token_idx == vocab['C']:\n",
    "        # Después de 'C', casi siempre terminar\n",
    "        probs[vocab['<EOS>']] = 0.9\n",
    "    \n",
    "    # Normalizar para que sumen 1\n",
    "    probs /= np.sum(probs)\n",
    "    \n",
    "    return {idx: p for idx, p in enumerate(probs)}\n",
    "\n",
    "# 2. Implementación de beam search\n",
    "\n",
    "def beam_search_decoder(k_beam, max_longitud_secuencia, funcion_paso_modelo):\n",
    "    \"\"\"\n",
    "    Implementa la decodificación con beam search.\n",
    "    \"\"\"\n",
    "    # Inicializar con el token <SOS>\n",
    "    # Cada 'haz' es una tupla: (secuencia, log_prob_acumulada)\n",
    "    k_beams = [([vocab['<SOS>']], 0.0)]\n",
    "    \n",
    "    # Lista para guardar las secuencias completas que terminan en <EOS>\n",
    "    completed_sequences = []\n",
    "\n",
    "    for _ in range(max_longitud_secuencia):\n",
    "        all_candidates = []\n",
    "        \n",
    "        # 1. Expandir cada beam\n",
    "        for seq, score in k_beams:\n",
    "            # Si el último token es <EOS>, la secuencia está completa\n",
    "            if seq[-1] == vocab['<EOS>']:\n",
    "                completed_sequences.append((seq, score))\n",
    "                continue\n",
    "\n",
    "            # Obtener las probabilidades del siguiente token del 'modelo'\n",
    "            next_token_probs = funcion_paso_modelo(seq)\n",
    "            \n",
    "            # Crear nuevos candidatos a partir del beam actual\n",
    "            for token_idx, prob in next_token_probs.items():\n",
    "                if prob == 0: continue # Evitar log(0)\n",
    "                \n",
    "                new_seq = seq + [token_idx]\n",
    "                # Usamos log-probabilidades para estabilidad numérica y porque sumar es más rápido\n",
    "                new_score = score + np.log(prob)\n",
    "                all_candidates.append((new_seq, new_score))\n",
    "\n",
    "        # Si no hay candidatos para expandir y ya hay secuencias completas, terminamos\n",
    "        if not all_candidates:\n",
    "            break\n",
    "            \n",
    "        # 2. Ordenar todos los candidatos y podar (pruning)\n",
    "        # Se ordena por puntuación (mayor es mejor)\n",
    "        ordered_candidates = sorted(all_candidates, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 3. Seleccionar los k_beam mejores para el siguiente paso\n",
    "        # Se toman 'k_beam' menos el número de secuencias que ya hemos completado\n",
    "        k_beams = ordered_candidates[:k_beam - len(completed_sequences)]\n",
    "\n",
    "        # Si todos los beams activos han generado secuencias completas, podemos parar antes\n",
    "        if all(s[-1] == vocab['<EOS>'] for s, _ in k_beams) and len(k_beams) > 0:\n",
    "            completed_sequences.extend(k_beams)\n",
    "            break\n",
    "\n",
    "    # Si el bucle termina por max_longitud, agregar los haces actuales a las completadas\n",
    "    completed_sequences.extend(k_beams)\n",
    "    \n",
    "    # Ordenar las secuencias completadas finales por su puntuación\n",
    "    # Se puede normalizar por longitud para no penalizar secuencias largas: score / len(seq)\n",
    "    final_sorted_sequences = sorted(completed_sequences, key=lambda x: x[1] / len(x[0]), reverse=True)\n",
    "    \n",
    "    return final_sorted_sequences\n",
    "\n",
    "# 3. Simulación y comparación\n",
    "\n",
    "def run_simulation(k):\n",
    "    print(f\"Ejecutando beam search con k_beam = {k}\")\n",
    "    resultados = beam_search_decoder(\n",
    "        k_beam=k,\n",
    "        max_longitud_secuencia=5,\n",
    "        funcion_paso_modelo=funcion_paso_modelo\n",
    "    )\n",
    "    \n",
    "    print(\"Secuencias generadas (ordenadas por puntuación/longitud):\")\n",
    "    for seq_indices, score in resultados:\n",
    "        seq_words = [idx_to_word[i] for i in seq_indices]\n",
    "        print(f\"  - Secuencia: {' '.join(seq_words)}\")\n",
    "        print(f\"    Log-Prob score: {score:.4f}\")\n",
    "        print(f\"    Normalized score: {score/len(seq_indices):.4f}\\n\")\n",
    "\n",
    "run_simulation(k=2)\n",
    "run_simulation(k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361ce2e5-768e-428d-8e98-6593e91e0000",
   "metadata": {},
   "source": [
    "### **Respuesta 4**\n",
    "\n",
    "#### **Cálculo de pesos de atención aditiva (Bahdanau)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f4a0f2-ee73-4959-a355-43c9c7c96d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Calcula la función softmax de manera numéricamente estable.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x)) # Restar el máximo mejora la estabilidad\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def calcular_pesos_atencion_bahdanau(estado_decoder_anterior, estados_encoder, Wa, Ua, va):\n",
    "    \"\"\"\n",
    "    Calcula los pesos de atención de Bahdanau.\n",
    "    \n",
    "    Args:\n",
    "        estado_decoder_anterior (np.array): Vector del estado oculto anterior del decoder (s_{t-1}).\n",
    "        estados_encoder (np.array): Matriz con los estados ocultos del encoder (h_j).\n",
    "        Wa (np.array): Matriz de pesos para el estado del decoder.\n",
    "        Ua (np.array): Matriz de pesos para los estados del encoder.\n",
    "        va (np.array): Vector de pesos para calcular el score.\n",
    "        \n",
    "    Returns:\n",
    "        np.array: Vector de pesos de atención (alpha_t).\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    # Proyectamos el estado del decoder una sola vez\n",
    "    s_proyectado = np.dot(Wa, estado_decoder_anterior) # W_a * s_{t-1}\n",
    "    \n",
    "    # Iteramos sobre cada estado oculto del encoder\n",
    "    for h_j in estados_encoder:\n",
    "        # Proyectamos el estado del encoder\n",
    "        h_proyectado = np.dot(Ua, h_j) # U_a * h_j\n",
    "        \n",
    "        # Calculamos la energía (score)\n",
    "        # v_a^T * tanh(W_a*s_{t-1} + U_a*h_j)\n",
    "        suma_proyecciones = s_proyectado + h_proyectado\n",
    "        energia = np.dot(va.T, np.tanh(suma_proyecciones))\n",
    "        scores.append(energia[0]) # El resultado es una matriz 1x1, tomamos el escalar\n",
    "        \n",
    "    # Aplicamos softmax a todos los scores para obtener los pesos de atención\n",
    "    pesos_atencion = softmax(np.array(scores))\n",
    "    \n",
    "    return np.array(scores), pesos_atencion\n",
    "\n",
    "# Simulación\n",
    "np.random.seed(42)\n",
    "\n",
    "# Dimensiones\n",
    "dim_estado = 4\n",
    "dim_atencion = 3\n",
    "num_estados_encoder = 3\n",
    "\n",
    "# 1. Definir datos de ejemplo\n",
    "estado_decoder_anterior = np.random.rand(dim_estado)\n",
    "# Creamos una matriz donde cada fila es un estado del encoder\n",
    "estados_encoder = np.random.rand(num_estados_encoder, dim_estado) \n",
    "\n",
    "# 2. Inicializar matrices de pesos con dimensiones compatibles\n",
    "Wa = np.random.rand(dim_atencion, dim_estado) # (3, 4)\n",
    "Ua = np.random.rand(dim_atencion, dim_estado) # (3, 4)\n",
    "va = np.random.rand(dim_atencion, 1)          # (3, 1)\n",
    "\n",
    "# 3. Llamar a la función y mostrar resultados\n",
    "print(\"Datos de entrada y pesos \")\n",
    "print(f\"Dimensiones de Wa: {Wa.shape}\")\n",
    "print(f\"Dimensiones de Ua: {Ua.shape}\")\n",
    "print(f\"Dimensiones de va: {va.shape}\\n\")\n",
    "print(f\"Estado del decoder s_{{t-1}} (dim {estado_decoder_anterior.shape}):\\n{estado_decoder_anterior}\\n\")\n",
    "print(f\"Estados del encoder H (dim {estados_encoder.shape}):\\n{estados_encoder}\\n\")\n",
    "\n",
    "scores_calculados, pesos_finales = calcular_pesos_atencion_bahdanau(\n",
    "    estado_decoder_anterior, estados_encoder, Wa, Ua, va\n",
    ")\n",
    "\n",
    "print(\"Resultados del cálculo de atención\")\n",
    "print(f\"Scores de energía (sin normalizar):\\n{scores_calculados}\\n\")\n",
    "print(f\"Pesos de atención finales (después de softmax):\\n{pesos_finales}\\n\")\n",
    "print(f\"Verificación: La suma de los pesos es: {np.sum(pesos_finales):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f1c768-5a7b-4927-aca3-dd9b22006c70",
   "metadata": {},
   "source": [
    "#### Explicación\n",
    "\n",
    "El objetivo es calcular un peso $\\alpha_{tj}$ para cada estado del encoder $h_j$ que nos diga cuán \"relevante\" es ese estado para generar la salida actual, dado el estado del decoder $s_{t-1}$.\n",
    "\n",
    "La fórmula es: $$\\alpha_{tj} = \\frac{\\exp(e_{tj})}{\\sum_{k=1}^{N} \\exp(e_{tk})}$$Donde el score de energía $e_{tj}$ se calcula como:$$e_{tj} = v_a^T \\tanh(W_a s_{t-1} + U_a h_j)$$\n",
    "\n",
    "Analicemos las dimensiones en la implementación:\n",
    "\n",
    "1.  **$W_a s_{t-1}$ y $U_a h_j$**:\n",
    "    * $s_{t-1}$ es un vector de dimensión `(4,)`.\n",
    "    * $h_j$ es un vector de dimensión `(4,)`.\n",
    "    * Para poder sumarlos, ambos deben ser proyectados al mismo espacio, el \"espacio de atención\", que definimos con `dim_atencion = 3`.\n",
    "    * **$W_a$** y **$U_a$** son las matrices de proyección. Para transformar un vector de `(4,)` a `(3,)`, sus dimensiones deben ser `(3, 4)`.\n",
    "    * El cálculo `np.dot(Wa, s_t-1)`: `(3, 4) @ (4,)` resulta en un vector de `(3,)`.\n",
    "    * El cálculo `np.dot(Ua, h_j)`: `(3, 4) @ (4,)` también resulta en un vector de `(3,)`.\n",
    "\n",
    "2.  **$\\tanh(\\dots)$**:\n",
    "    * La suma de las dos proyecciones da un vector de `(3,)`.\n",
    "    * La función `tanh` se aplica elemento a elemento, por lo que el resultado sigue siendo un vector de `(3,)`.\n",
    "\n",
    "3.  **$v_a^T \\tanh(\\dots)$**:\n",
    "    * El propósito de **$v_a$** es tomar este vector de `dim_atencion` y colapsarlo en un único número escalar: el **score de energía**.\n",
    "    * Definimos $v_a$ con dimensión `(3, 1)`. Su transpuesta $v_a^T$ tiene dimensión `(1, 3)`.\n",
    "    * El cálculo `np.dot(va.T, tanh_output)`: `(1, 3) @ (3,)` resulta en un escalar (o una matriz `(1,1)`), que es el score $e_{tj}$.\n",
    "\n",
    "4.  **Softmax**:\n",
    "    * Repetimos este proceso para cada uno de los `num_estados_encoder = 3` estados de $h$.\n",
    "    * Obtenemos 3 scores, que agrupamos en un vector `[e_t1, e_t2, e_t3]`.\n",
    "    * La función `softmax` se aplica a este vector para convertir los scores en una distribución de probabilidad, cuyos valores (los pesos de atención $\\alpha_t$) suman 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcba712-d114-457c-b8b2-6692b9f81133",
   "metadata": {},
   "source": [
    "#### **Análisis de autoatención**\n",
    "\n",
    "Aquí exploramos los componentes fundamentales de la autoatención: las matrices **Query (Q)**, **Key (K)** y **Value (V)**, y cómo interactúan para calcular los scores de atención."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1116055c-a848-41b1-87da-67384f609450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Para reproducibilidad\n",
    "np.random.seed(99)\n",
    "\n",
    "# Contexto y datos iniciales\n",
    "# Secuencia de 3 palabras, cada una con un embedding de dimensión 4\n",
    "seq_len = 3\n",
    "embedding_dim = 4\n",
    "dk = 3 # Dimensión para Q, K, V\n",
    "\n",
    "# Matriz de embeddings de entrada X (cada fila es una palabra)\n",
    "embeddings_entrada = np.random.rand(seq_len, embedding_dim)\n",
    "\n",
    "# Matrices de pesos para proyectar los embeddings\n",
    "Wq = np.random.rand(embedding_dim, dk) # (4, 3)\n",
    "Wk = np.random.rand(embedding_dim, dk) # (4, 3)\n",
    "Wv = np.random.rand(embedding_dim, dk) # (4, 3)\n",
    "\n",
    "# Implementación\n",
    "\n",
    "# 1. Calcular matrices Q, K, V\n",
    "# X @ Wq -> (3, 4) @ (4, 3) = (3, 3)\n",
    "Q = np.dot(embeddings_entrada, Wq)\n",
    "K = np.dot(embeddings_entrada, Wk)\n",
    "V = np.dot(embeddings_entrada, Wv)\n",
    "\n",
    "# 2. Calcular los scores de atención (sin escalar ni softmax)\n",
    "# Q @ K.T -> (3, 3) @ (3, 3) = (3, 3)\n",
    "scores = np.dot(Q, K.T)\n",
    "\n",
    "# Salidas\n",
    "print(\"Datos de entrada y pesos\")\n",
    "print(f\"Embeddings de entrada X (dim {embeddings_entrada.shape}):\\n{embeddings_entrada}\\n\")\n",
    "print(f\"Pesos W_Q (dim {Wq.shape}):\\n{Wq}\\n\")\n",
    "\n",
    "print(\"Matrices Q, K, V\")\n",
    "print(f\"Matriz Q (dim {Q.shape}):\\n{Q}\\n\")\n",
    "print(f\"Matriz K (dim {K.shape}):\\n{K}\\n\")\n",
    "print(f\"Matriz V (dim {V.shape}):\\n{V}\\n\")\n",
    "\n",
    "print(\"Matriz de scores de atención\")\n",
    "print(f\"Scores = QK^T (dim {scores.shape}):\\n{scores}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce275118-be0f-49a2-951b-2fbfe84bd0f2",
   "metadata": {},
   "source": [
    "#### **Análisis conceptual**\n",
    "\n",
    "1.  **¿Qué representan las filas y columnas de la matriz $QK^T$ resultante?**\n",
    "    * Tanto las **filas** como las **columnas** representan las palabras (o tokens) de la secuencia de entrada en su respectivo orden.\n",
    "    * La matriz $QK^T$ es la **matriz de scores de energía**. El valor en la posición $(i, j)$ representa el **score de similitud o alineación** entre la **Query** de la palabra $i$ y la **Key** de la palabra $j$. Es una medida de cuán \"relevante\" es la palabra $j$ para la palabra $i$ en el contexto de esta secuencia.\n",
    "\n",
    "2.  **Si un valor en la posición $(i, j)$ de $QK^T$ es alto, ¿qué implicaría?**\n",
    "    * Un valor alto en $(i, j)$ implica una **fuerte afinidad** entre la consulta de la palabra $i$ y la clave de la palabra $j$. Antes de la normalización con softmax, esto significa que el modelo ha aprendido (a través de las proyecciones $W_Q$ y $W_K$) que para construir la nueva representación contextual de la palabra $i$, debe **prestar mucha atención** a la información contenida en la palabra $j$. Después del softmax, este score alto se convertirá en un peso de atención cercano a 1, mientras que otros scores bajos se volverán cercanos a 0.\n",
    "\n",
    "3.  **¿Cómo se usaría luego la matriz V?**\n",
    "    * El paso final consiste en crear las nuevas representaciones de las palabras. Esto se hace calculando una **suma ponderada de todos los vectores value (V)**, donde los pesos son los scores de atención que acabamos de discutir (después de aplicar el escalado y el softmax).\n",
    "    * La fórmula es: $\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$.\n",
    "    * En la práctica, la matriz de pesos de atención (el resultado del softmax) se multiplica por la matriz V. La **fila $i$** de la matriz resultante es la nueva representación de la palabra $i$. Esta nueva representación ya no es el embedding original, sino una **versión contextualizada**: una mezcla de los \"valores\" de todas las palabras de la secuencia, ponderada por la relevancia que cada palabra tiene para la palabra $i$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
