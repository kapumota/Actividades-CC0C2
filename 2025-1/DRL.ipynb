{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1700247a",
   "metadata": {},
   "source": [
    "## **Fundamentos del aprendizaje por refuerzo**\n",
    "\n",
    "El aprendizaje por refuerzo (RL, por sus siglas en inglés) es un paradigma del aprendizaje automático donde un agente aprende a tomar decisiones mediante la interacción con su entorno, recibiendo recompensas o penalizaciones según sus acciones. \n",
    "\n",
    "A diferencia del aprendizaje supervisado, donde el aprendizaje se basa en un conjunto de datos etiquetados, RL se basa en la retroalimentación recibida a partir de la experiencia directa. \n",
    "\n",
    "#### **Elementos básicos del aprendizaje por refuerzo**\n",
    "\n",
    "En RL, los principales elementos son el agente, el entorno, los estados, las acciones y las recompensas. Estos elementos se relacionan de la siguiente manera:\n",
    "\n",
    "1. **Agente**: Es el tomador de decisiones que interactúa con el entorno. El agente recibe información del entorno, toma decisiones (acciones) y aprende de las consecuencias de esas decisiones.\n",
    "\n",
    "2. **Entorno**: Es el mundo con el que interactúa el agente. El entorno responde a las acciones del agente y proporciona nuevas observaciones y recompensas.\n",
    "\n",
    "3. **Estados (s)**: Representan la situación actual del entorno. Un estado contiene toda la información necesaria para describir la situación en un momento dado.\n",
    "\n",
    "4. **Acciones (a)**: Son las decisiones o movimientos que el agente puede realizar en un estado dado. El conjunto de todas las posibles acciones se denota como A.\n",
    "\n",
    "5. **Recompensas (r)**: Son señales de retroalimentación que indican el valor de una acción en un estado específico. El objetivo del agente es maximizar la recompensa total a lo largo del tiempo.\n",
    "\n",
    "La interacción entre estos elementos puede describirse como una serie de pasos, donde el agente percibe un estado del entorno, selecciona una acción, recibe una recompensa y transita a un nuevo estado.\n",
    "\n",
    "#### **Procesos de decisión de Markov (MDPs)**\n",
    "\n",
    "Los procesos de decisión de Markov (MDPs) son una herramienta matemática utilizada para modelar problemas de RL. Un MDP se define por los siguientes componentes:\n",
    "\n",
    "1. **Conjunto de estados (S)**: Todos los posibles estados en los que el agente puede encontrarse.\n",
    "2. **Conjunto de acciones (A)**: Todas las posibles acciones que el agente puede tomar.\n",
    "3. **Función de transición (P)**: Describe la probabilidad de transición de un estado a otro, dado una acción. $P(s'|s,a)$ representa la probabilidad de moverse al estado $s'$ desde el estado $s$ tomando la acción $a$.\n",
    "4. **Función de recompensa (R)**: Define la recompensa esperada al realizar una acción en un estado particular. $R(s,a)$ es la recompensa inmediata recibida al realizar la acción $a$ en el estado $s$.\n",
    "5. **Factor de descuento ($\\gamma$)**: Es un valor entre 0 y 1 que determina la importancia de las recompensas futuras. Un factor de descuento cercano a 0 hace que el agente se enfoque en recompensas inmediatas, mientras que un valor cercano a 1 da más peso a las recompensas futuras.\n",
    "\n",
    "Los MDPs permiten formalizar el problema de RL y proporcionar una base para diseñar y analizar algoritmos.\n",
    "\n",
    "#### **Políticas y funciones de valor**\n",
    "\n",
    "En RL, una política ($\\pi$) es una estrategia que el agente sigue para decidir qué acciones tomar en cada estado. Una política puede ser determinista ($\\pi(s) = a$) o estocástica ($\\pi(a|s)$), donde la acción es elegida con una cierta probabilidad.\n",
    "\n",
    "Las funciones de valor son herramientas clave para evaluar la calidad de los estados y las acciones bajo una política determinada. Hay dos tipos principales de funciones de valor:\n",
    "\n",
    "1. **Función de valor del estado (V)**: $V^\\pi(s)$ es el valor esperado de las recompensas futuras comenzando desde el estado $s$ y siguiendo la política $\\pi$. Se define como:\n",
    "   $$\n",
    "   V^\\pi(s) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_{t+1} \\mid s_t = s \\right]\n",
    "   $$\n",
    "\n",
    "2. **Función de valor de la acción (Q)**: $Q^\\pi(s,a)$ es el valor esperado de las recompensas futuras al tomar la acción $a$ en el estado $s$ y luego seguir la política $\\pi$. Se define como:\n",
    "   $$\n",
    "   Q^\\pi(s,a) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_{t+1} \\mid s_t = s, a_t = a \\right]\n",
    "   $$\n",
    "\n",
    "El objetivo del agente es encontrar la política óptima ($\\pi^*$) que maximice estas funciones de valor.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d10036",
   "metadata": {},
   "source": [
    "#### **Métodos basados en el valor**\n",
    "\n",
    "Los métodos basados en el valor se centran en aprender una función de valor que estima la calidad de los estados y acciones. Los dos algoritmos más representativos en esta categoría son Q-Learning y SARSA.\n",
    "\n",
    "**Q-Learning**:\n",
    "Q-Learning es un algoritmo off-policy que busca aprender la función de valor de acción $Q(s, a)$. La actualización de Q-Learning se basa en la ecuación de Bellman:\n",
    "\n",
    "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)]$$\n",
    "\n",
    "Aquí, $s$ es el estado actual, $a$ es la acción tomada, $r$ es la recompensa recibida, $s'$ es el nuevo estado, $\\alpha$ es la tasa de aprendizaje, y $\\gamma$ es el factor de descuento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f09d66d",
   "metadata": {},
   "source": [
    "**SARSA**:\n",
    "    \n",
    "SARSA (State-Action-Reward-State-Action) es un algoritmo on-policy que actualiza la función de valor de acción utilizando la acción actual y la siguiente acción seleccionada por la política. La actualización de SARSA se basa en la ecuación:\n",
    "\n",
    "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma Q(s', a') - Q(s, a)]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e520365",
   "metadata": {},
   "source": [
    "#### **Métodos basados en la política**\n",
    "\n",
    "Los métodos basados en la política se enfocan en aprender directamente una política que mapea estados a acciones, sin necesidad de una función de valor intermedia. Los dos algoritmos más comunes en esta categoría son REINFORCE y Actor-Critic.\n",
    "\n",
    "**REINFORCE**:\n",
    "REINFORCE es un algoritmo de gradiente de política que ajusta los parámetros de la política para maximizar la recompensa esperada. La actualización de la política se basa en la ecuación:\n",
    "\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\log \\pi_\\theta(a|s) G_t$$\n",
    "\n",
    "Donde $G_t$ es la recompensa acumulada desde el tiempo $t$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f819b60",
   "metadata": {},
   "source": [
    "**Actor-Critic**:\n",
    "El método Actor-Critic combina una red de política (actor) y una red de valor (critic). El actor actualiza la política basándose en la ventaja estimada por el crítico. La actualización de la política sigue:\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\log \\pi_\\theta(a|s) A(s, a)$$\n",
    "\n",
    "Y la actualización de la función de valor sigue:\n",
    "\n",
    "$$\\phi \\leftarrow \\phi + \\beta \\nabla_\\phi (r + \\gamma V_\\phi(s') - V_\\phi(s))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3119edd3",
   "metadata": {},
   "source": [
    "### **Métodos híbridos**\n",
    "\n",
    "Los métodos híbridos combinan enfoques basados en el valor y en la política para aprovechar las fortalezas de ambos. Entre los algoritmos híbridos más avanzados se encuentran DDPG, PPO y A3C.\n",
    "\n",
    "**DDPG (Deep Deterministic Policy Gradient)**:\n",
    "DDPG es un algoritmo off-policy que combina DQN y el actor-critic. Utiliza una red de actor para seleccionar acciones y una red crítico para evaluar la calidad de esas acciones. También emplea una red de destino para estabilizar el entrenamiento.\n",
    "\n",
    "**PPO (Proximal Policy Optimization)**:\n",
    "PPO es un algoritmo de política basada en la proximidad que busca mejorar la estabilidad del entrenamiento limitando el tamaño del paso de actualización. La función objetivo de PPO es:\n",
    "\n",
    "$$ L^{CLIP}(\\theta) = \\mathbb{E}_t \\left[ \\min \\left( \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)} A_t, \\text{clip}\\left( \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}, 1 - \\epsilon, 1 + \\epsilon \\right) A_t \\right) \\right]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad29401",
   "metadata": {},
   "source": [
    "**A3C (Asynchronous Advantage Actor-Critic)**:\n",
    "\n",
    "A3C es un algoritmo de aprendizaje asincrónico donde múltiples agentes independientes aprenden simultáneamente y actualizan una política global. Utiliza la misma estructura que el actor-critic, pero distribuye el entrenamiento en múltiples instancias del entorno.\n",
    "\n",
    "En resumen, los métodos de aprendizaje por refuerzo abarcan una variedad de enfoques, cada uno con sus propias fortalezas y desafíos. Desde los métodos basados en el valor como Q-Learning y SARSA, hasta los métodos basados en la política como REINFORCE y Actor-Critic, y finalmente los métodos híbridos como DDPG, PPO y A3C, todos juegan un papel crucial en el desarrollo de agentes inteligentes capaces de tomar decisiones en entornos complejos y dinámicos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f36640",
   "metadata": {},
   "source": [
    "### **Métodos de Monte Carlo: simulación y estimación de valores**\n",
    "\n",
    "Los métodos de Monte Carlo son una clase de algoritmos computacionales que dependen de muestreos aleatorios repetidos para obtener resultados numéricos. Se utilizan principalmente en optimización, integración numérica y generación de muestras de una distribución de probabilidad. En el contexto del aprendizaje por refuerzo, los métodos de Monte Carlo se usan para estimar el valor esperado de una política dada, promediando las recompensas observadas a lo largo de múltiples episodios.\n",
    "\n",
    "**Simulación y estimación de valores con Monte Carlo**\n",
    "\n",
    "En el aprendizaje por refuerzo, la simulación mediante métodos de Monte Carlo implica ejecutar varios episodios de interacción entre el agente y el entorno para recopilar datos sobre las recompensas obtenidas. A partir de estas simulaciones, se pueden calcular las estimaciones de valor. Estas estimaciones se basan en la premisa de que, a largo plazo, las recompensas acumuladas reflejan el valor esperado de un estado o una acción.\n",
    "\n",
    "Para estimar los valores de una política, se sigue un procedimiento que incluye:\n",
    "\n",
    "1. **Generación de episodios:** Se generan múltiples episodios siguiendo la política actual, donde un episodio es una secuencia de estados, acciones y recompensas que termina en un estado terminal.\n",
    "2. **Cálculo de retornos:** Para cada estado visitado en un episodio, se calcula el retorno, que es la suma de recompensas futuras descontadas.\n",
    "3. **Promedio de retornos:** Se promedian los retornos de todos los episodios en los que se visitó un estado específico para obtener una estimación de su valor.\n",
    "\n",
    "Una característica fundamental de los métodos de Monte Carlo es que requieren episodios completos, lo que significa que sólo se actualizan los valores al final de un episodio. Esto puede ser una limitación en entornos donde los episodios son largos o no terminan.\n",
    "\n",
    "**Métodos de diferencias temporales (TD): TD($\\lambda$) y n-step TD**\n",
    "\n",
    "Los métodos de diferencias temporales combinan las ventajas del aprendizaje Monte Carlo y el aprendizaje dinámico, actualizando las estimaciones de valores en función de las diferencias temporales, es decir, la diferencia entre las estimaciones de valor consecutivas.\n",
    "\n",
    "**TD($\\lambda$)**\n",
    "\n",
    "TD($\\lambda$) es una técnica que unifica los métodos TD y Monte Carlo mediante el uso de trazas de elegibilidad, que son variables que asignan crédito a los estados y acciones visitados recientemente. $\\lambda$ es un parámetro que controla la ponderación de las actualizaciones.\n",
    "\n",
    "1. **Trazas de elegibilidad:** Se utilizan para dar crédito a los estados visitados recientemente. A medida que el agente se mueve a través del entorno, las trazas de elegibilidad se actualizan, decayendo con cada paso.\n",
    "2. **Actualización de valores:** Las actualizaciones de valores se realizan no solo en función del estado actual y el siguiente estado, sino también en función de los estados anteriores, ponderados por sus trazas de elegibilidad.\n",
    "\n",
    "La fórmula general de actualización para TD($\\lambda$) es:\n",
    "\n",
    "$$V(s) \\leftarrow V(s) + \\alpha \\delta_t e_t(s)$$\n",
    "donde:\n",
    "- $\\alpha$ es la tasa de aprendizaje,\n",
    "- $\\delta_t$ es el error de TD,\n",
    "- $e_t(s)$ es la traza de elegibilidad.\n",
    "\n",
    "**n-step TD**\n",
    "\n",
    "El método n-step TD extiende la idea básica de TD al utilizar recompensas de los siguientes n pasos en lugar de solo la recompensa inmediata y el valor del siguiente estado. Este método actualiza los valores basándose en n pasos futuros de interacción con el entorno.\n",
    "\n",
    "1. **Recompensas acumuladas:** Para cada estado, se acumulan las recompensas de los próximos n pasos.\n",
    "2. **Actualización de valores:** Los valores se actualizan utilizando esta suma de recompensas, proporcionando un equilibrio entre la actualización a corto plazo (TD(0)) y la actualización a largo plazo (Monte Carlo).\n",
    "\n",
    "**Exploración vs. explotación: estrategias epsilon-greedy, Upper Confidence Bound (UCB)**\n",
    "\n",
    "En el aprendizaje por refuerzo, un desafío clave es balancear la exploración de nuevas acciones con la explotación de las acciones conocidas que proporcionan las mayores recompensas. Este dilema se conoce como el trade-off exploración-explotación.\n",
    "\n",
    "**Estrategia epsilon-greedy**\n",
    "\n",
    "La estrategia epsilon-greedy es una de las técnicas más simples y efectivas para gestionar este trade-off. En esta estrategia:\n",
    "\n",
    "1. **Exploración:** Con una probabilidad ($\\epsilon$), el agente selecciona una acción al azar, lo que permite explorar nuevas acciones.\n",
    "2. **Explotación:** Con una probabilidad (1 - $\\epsilon$), el agente selecciona la acción con el mayor valor esperado (explota el conocimiento actual).\n",
    "\n",
    "El parámetro ($\\epsilon$) se puede ajustar durante el entrenamiento, a menudo comenzando con un valor alto para fomentar la exploración y disminuyéndolo gradualmente para favorecer la explotación a medida que el agente aprende más sobre el entorno.\n",
    "\n",
    "**Upper Confidence Bound (UCB)**\n",
    "\n",
    "El método Upper Confidence Bound es otra estrategia para balancear exploración y explotación, utilizando una forma más teórica y matemática basada en la teoría de la toma de decisiones en condiciones de incertidumbre.\n",
    "\n",
    "1. **Valor de confianza:** UCB asigna a cada acción un valor de confianza que aumenta con la incertidumbre de la estimación del valor de esa acción.\n",
    "2. **Selección de acciones:** El agente selecciona la acción con el valor de confianza más alto, lo que favorece las acciones con altos valores esperados y aquellas que han sido menos exploradas.\n",
    "\n",
    "La fórmula general para el valor de confianza en UCB es:\n",
    "$$Q(a) + c \\sqrt{\\frac{\\ln(t)}{N(a)}}$$\n",
    "donde:\n",
    "- $Q(a)$ es el valor estimado de la acción $a$,\n",
    "- $c$ es un parámetro que controla el grado de exploración,\n",
    "- $t$ es el número total de selecciones de acciones,\n",
    "- $N(a)$ es el número de veces que la acción $a$ ha sido seleccionada.\n",
    "\n",
    "UCB proporciona un marco matemáticamente riguroso para el trade-off exploración-explotación, asegurando que cada acción sea seleccionada un número suficiente de veces para obtener estimaciones precisas de su valor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5ec7ac",
   "metadata": {},
   "source": [
    "UCB-V es una variante del algoritmo UCB que considera la varianza en la estimación de las recompensas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c766cb",
   "metadata": {},
   "source": [
    "### **Deep Q-Networks (DQN)**\n",
    "\n",
    "Deep Q-Networks (DQN) representan una evolución significativa en el campo del Aprendizaje por Refuerzo (RL), combinando técnicas tradicionales de RL con redes neuronales profundas. Este enfoque fue popularizado por la investigación de DeepMind, donde se demostró que una red neuronal podía aprender a jugar videojuegos de Atari a nivel humano.\n",
    "\n",
    "En los métodos tradicionales de RL, la función Q se representa mediante tablas (tabular methods) que mapean cada par estado-acción a un valor Q. Sin embargo, este enfoque no es escalable a entornos con grandes espacios de estados y acciones. Aquí es donde entran las redes neuronales: en lugar de almacenar valores Q explícitamente, una red neuronal se entrena para aproximar la función Q.\n",
    "\n",
    "La arquitectura de un DQN es bastante simple: se utiliza una red neuronal con varias capas ocultas que toma el estado del entorno como entrada y produce un valor Q para cada posible acción. La actualización de los pesos de la red neuronal se realiza mediante un proceso de backpropagation, donde el objetivo es minimizar el error entre el valor Q estimado y el valor Q objetivo, el cual se define mediante la ecuación de Bellman."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eb5c39",
   "metadata": {},
   "source": [
    "### **Redes Actor-Critic**\n",
    "El enfoque Actor-Critic combina las ventajas de los métodos basados en políticas y los basados en valor. En lugar de tener una única red que aprenda la política o el valor Q, se utilizan dos redes: una para la política (actor) y otra para el valor (critic).\n",
    "\n",
    "La red Actor se encarga de seleccionar acciones según una política aprendida, mientras que la red Critic evalúa estas acciones proporcionando una estimación del valor Q. \n",
    "\n",
    "El Actor mejora su política utilizando las críticas del Critic, haciendo que este enfoque sea más estable y eficiente en comparación con los métodos puramente basados en políticas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803053cc-9b29-4857-b122-5f56ef97c584",
   "metadata": {},
   "source": [
    "### **Optimización de políticas**\n",
    "\n",
    "La optimización de políticas se refiere a la mejora continua de la política $\\pi$ para maximizar la recompensa acumulada esperada.\n",
    "\n",
    "#### **Policy Gradient**\n",
    "\n",
    "Los métodos de Policy Gradient optimizan directamente la política parametrizada $\\pi_\\theta$ mediante gradientes de la recompensa acumulada esperada.\n",
    "\n",
    "#### **Ecuaciones clave**\n",
    "\n",
    "La función objetivo a maximizar es el retorno esperado:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t) \\right]\n",
    "$$\n",
    "\n",
    "El gradiente de la política se calcula como:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^{\\infty} \\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t) Q^\\pi(s_t, a_t) \\right]\n",
    "$$\n",
    "\n",
    "Esta ecuación se deriva utilizando la regla del gradiente logarítmico (Log-Likelihood Ratio) y el teorema de la expectativa, lo que permite que el gradiente de la política sea calculado como la expectativa de los gradientes ponderados por el valor de la acción $Q^\\pi(s_t, a_t)$.\n",
    "\n",
    "#### **A2C/A3C (Asynchronous Advantage Actor-Critic)**\n",
    "\n",
    "A2C y A3C son algoritmos actor-crítico que combinan un actor que aprende la política y un crítico que evalúa la política.\n",
    "\n",
    "#### **Ecuaciones clave**\n",
    "\n",
    "El objetivo es minimizar la pérdida:\n",
    "\n",
    "$$\n",
    "L(\\theta) = -\\log \\pi_\\theta(a_t \\mid s_t) \\left( R_t - V^\\pi(s_t) \\right) + \\frac{1}{2} \\left( R_t - V^\\pi(s_t) \\right)^2\n",
    "$$\n",
    "\n",
    "Donde $R_t$ es el retorno observado y $V^\\pi(s_t)$ es el valor estimado del estado. La primera parte de la ecuación es la pérdida del actor, que se minimiza cuando las acciones que lleva a cabo la política son buenas según la estimación del crítico. La segunda parte es la pérdida del crítico, que se minimiza ajustando el valor del estado para que coincida con el retorno observado.\n",
    "\n",
    "#### **PPO (Proximal Policy Optimization)**\n",
    "\n",
    "PPO es un método que restringe la actualización de la política para evitar grandes cambios que desestabilicen el entrenamiento.\n",
    "\n",
    "#### **Ecuaciones clave**\n",
    "\n",
    "La función objetivo para PPO es:\n",
    "\n",
    "$$\n",
    "L^{\\text{CLIP}}(\\theta) = \\mathbb{E}_t \\left[ \\min \\left( r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_t \\right) \\right]\n",
    "$$\n",
    "\n",
    "Donde $r_t(\\theta) = \\frac{\\pi_\\theta(a_t \\mid s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t \\mid s_t)}$ y $\\hat{A}_t$ es la ventaja estimada. El objetivo es maximizar la recompensa esperada mientras se evita que $r_t(\\theta)$, que es la relación de probabilidad, se aleje demasiado de 1 mediante la operación de \"clipping\".\n",
    "\n",
    "#### **TRPO (Trust Region Policy Optimization)**\n",
    "\n",
    "TRPO optimiza la política dentro de una región de confianza para garantizar la mejora de la política.\n",
    "\n",
    "#### **Ecuaciones clave**\n",
    "\n",
    "El objetivo de TRPO es maximizar:\n",
    "\n",
    "$$\n",
    "L^{\\text{TRPO}}(\\theta) = \\mathbb{E}_t \\left[ \\frac{\\pi_\\theta(a_t \\mid s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t \\mid s_t)} \\hat{A}_t \\right]\n",
    "$$\n",
    "\n",
    "Sujeto a:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_t \\left[ \\text{KL} \\left[ \\pi_{\\theta_{\\text{old}}} (\\cdot \\mid s_t) \\parallel \\pi_\\theta (\\cdot \\mid s_t) \\right] \\right] \\leq \\delta\n",
    "$$\n",
    "\n",
    "Donde la función objetivo maximiza el rendimiento esperado, y la restricción asegura que el cambio en la política (medido por la divergencia KL) no sea mayor que un valor umbral $\\delta$.\n",
    "\n",
    "#### **DPPO (Distributed Proximal Policy Optimization)**\n",
    "\n",
    "DPPO extiende PPO para entornos distribuidos, permitiendo el entrenamiento paralelo de múltiples agentes.\n",
    "\n",
    "#### **Ecuaciones clave**\n",
    "\n",
    "DPPO utiliza la misma función objetivo que PPO, pero con actualizaciones distribuidas y sincronizadas de los parámetros de la política a través de múltiples trabajadores. La clave está en la gestión de la sincronización y agregación de las actualizaciones para asegurar una convergencia estable y eficiente.\n",
    "\n",
    "#### **TD3 (Twin Delayed Deep Deterministic Policy Gradient)**\n",
    "\n",
    "TD3 es una mejora del DDPG (Deep Deterministic Policy Gradient) que reduce la sobreestimación del valor de la acción mediante el uso de dos críticos y actualizaciones retrasadas.\n",
    "\n",
    "#### **Ecuaciones clave**\n",
    "\n",
    "Las actualizaciones de los críticos son:\n",
    "\n",
    "$$\n",
    "y_t = r_t + \\gamma \\min_{i=1,2} Q_{\\theta_i'}(s_{t+1}, \\pi_{\\phi'}(s_{t+1}))\n",
    "$$\n",
    "\n",
    "Donde $y_t$ es la estimación de la recompensa futura, $Q_{\\theta_i'}$ son las funciones de valor de los dos críticos, y $\\pi_{\\phi'}$ es la política actualizada.\n",
    "\n",
    "Las actualizaciones del actor son retrasadas:\n",
    "\n",
    "$$\n",
    "\\nabla_\\phi J(\\phi) = \\mathbb{E}_t \\left[ \\nabla_a Q_{\\theta_1}(s_t, a) \\mid_{a = \\pi_\\phi(s_t)} \\nabla_\\phi \\pi_\\phi(s_t) \\right]\n",
    "$$\n",
    "\n",
    "El actor se actualiza usando el gradiente de la función de valor del primer crítico, asegurando que las actualizaciones sean más estables al introducir un retraso.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b3d2e6-c9d0-4345-b545-9f72c9c62167",
   "metadata": {},
   "source": [
    "### **Aprendizaje por Refuerzo basado en modelos**\n",
    "\n",
    "El Aprendizaje por Refuerzo Basado en Modelo (Model-Based RL) implica la construcción y utilización de un modelo explícito del entorno para la planificación y toma de decisiones. Este enfoque puede ser más eficiente en cuanto a muestras que los métodos sin modelo (Model-Free RL), ya que permite que el agente utilice simulaciones del modelo para aprender y mejorar su política.\n",
    "\n",
    "#### **Aprender el modelo**\n",
    "\n",
    "Aprender un modelo del entorno implica construir una representación interna que capture las dinámicas del entorno. Este modelo puede ser utilizado para predecir las transiciones de estado y recompensas, facilitando la planificación y la toma de decisiones.\n",
    "\n",
    "#### **World Models**\n",
    "\n",
    "Los World Models son un enfoque en el que el agente aprende una representación compacta del entorno y utiliza esta representación para planificar y tomar decisiones.\n",
    "\n",
    "**Componentes clave**\n",
    "\n",
    "1. **VAE (Variational Autoencoder)**: Se utiliza para aprender una representación latente de los estados del entorno.\n",
    "\n",
    "   - **Función de pérdida del VAE**:\n",
    "     $$\n",
    "     \\mathcal{L}_{\\text{VAE}} = \\mathbb{E}_{q_\\phi(z|s)} \\left[ \\log p_\\theta(s|z) \\right] - D_{\\text{KL}}(q_\\phi(z|s) \\| p(z))\n",
    "     $$\n",
    "     \n",
    "     - **Término de reconstrucción**: $\\mathbb{E}_{q_\\phi(z|s)} \\left[ \\log p_\\theta(s|z) \\right]$ mide cuán bien el VAE puede reconstruir el estado original $s$ a partir de la representación latente $z$.\n",
    "     - **Término de regularización**: $D_{\\text{KL}}(q_\\phi(z|s) \\| p(z))$ es la divergencia KL entre la distribución aproximada $q_\\phi(z|s)$ y una distribución prior $p(z)$ (normalmente una distribución gaussiana).\n",
    "\n",
    "2. **MDN-RNN (Mixture Density Network - Recurrent Neural Network)**: Modelo recurrente que predice la próxima representación latente y recompensa dadas las representaciones latentes actuales y acciones.\n",
    "\n",
    "   - **Función de pérdida del MDN-RNN**:\n",
    "     $$\n",
    "     p(z_{t+1}, r_t | z_t, a_t) = \\text{MDN-RNN}(z_t, a_t)\n",
    "     $$\n",
    "     - La salida del MDN-RNN es una combinación de varias distribuciones gausianas que modelan las posibles próximas representaciones latentes $z_{t+1}$ y recompensas $r_t$.\n",
    "\n",
    "3. **Controller**: Utiliza la representación latente y las predicciones del MDN-RNN para tomar decisiones de acción.\n",
    "\n",
    "#### **I2A (Imagination-Augmented Agents)**\n",
    "\n",
    "I2A utiliza un modelo imaginativo para simular futuros posibles y mejorar la toma de decisiones.\n",
    "\n",
    "**Componentes clave**\n",
    "\n",
    "1. **Modelo imaginativo**: Simula transiciones futuras del entorno.\n",
    "\n",
    "   - **Modelo de transición**:\n",
    "     $$\n",
    "     \\hat{s}_{t+1}, \\hat{r}_t = f(s_t, a_t)\n",
    "     $$\n",
    "     - Aquí, $f(s_t, a_t)$ representa un modelo que predice el siguiente estado $\\hat{s}_{t+1}$ y la recompensa $\\hat{r}_t$ dado el estado actual $s_t$ y la acción $a_t$.\n",
    "\n",
    "2. **Imagination core**: Genera múltiples trayectorias imaginadas a partir del estado actual y posibles acciones.\n",
    "\n",
    "   - **Trayectorias imaginadas**:\n",
    "     $$\n",
    "     \\{ (\\hat{s}_{t+1}^i, \\hat{r}_t^i) \\}_{i=1}^n\n",
    "     $$\n",
    "     - Este conjunto de trayectorias imaginadas permite que el agente considere varios futuros posibles y sus respectivas recompensas.\n",
    "\n",
    "3. **Policy network**: Integra las trayectorias imaginadas para seleccionar la acción óptima.\n",
    "\n",
    "   - **Selección de acción**:\n",
    "     $$\n",
    "     a_t = \\pi(s_t, \\{\\hat{s}_{t+1}^i, \\hat{r}_t^i \\}_{i=1}^n)\n",
    "     $$\n",
    "     - La red de políticas $\\pi$ toma como entrada el estado actual $s_t$ y las trayectorias imaginadas para seleccionar la mejor acción $a_t$.\n",
    "\n",
    "#### **MBMF (Model-Based Model-Free)**\n",
    "\n",
    "MBMF combina elementos de RL basado en modelo y sin modelo para aprovechar las ventajas de ambos.\n",
    "\n",
    "**Componentes clave**\n",
    "\n",
    "1. **Modelo dinámico**: Predice transiciones y recompensas.\n",
    "\n",
    "   - **Modelo de transición**:\n",
    "     $$\n",
    "     \\hat{s}_{t+1}, \\hat{r}_t = f(s_t, a_t)\n",
    "     $$\n",
    "\n",
    "2. **Planificación corta**: Utiliza el modelo dinámico para planificar a corto plazo y generar rollouts.\n",
    "\n",
    "   - **Rollouts a corto plazo**:\n",
    "     $$\n",
    "     \\{ (\\hat{s}_{t+1}^k, \\hat{r}_t^k) \\}_{k=1}^K\n",
    "     $$\n",
    "     - Se generan múltiples secuencias de estados y recompensas simuladas para un horizonte de planificación corto.\n",
    "\n",
    "3. **Entrenamiento sin modelo**: Utiliza los rollouts generados para actualizar una política o función de valor sin modelo.\n",
    "\n",
    "   - **Actualización de la política**:\n",
    "     $$\n",
    "     \\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta \\mathcal{L}(\\pi_\\theta, \\{ (\\hat{s}_{t+1}^k, \\hat{r}_t^k) \\}_{k=1}^K)\n",
    "     $$\n",
    "     - Los rollouts simulados se utilizan para calcular las gradientes y actualizar los parámetros de la política $\\theta$.\n",
    "\n",
    "#### **MBVE (Model-Based Value Expansion)**\n",
    "\n",
    "MBVE utiliza un modelo del entorno para expandir la estimación de valor más allá del horizonte de planificación actual.\n",
    "\n",
    "**Componentes clave**\n",
    "\n",
    "1. **Modelo dinámico**: Predice transiciones y recompensas.\n",
    "\n",
    "   - **Modelo de transición**:\n",
    "     $$\n",
    "     \\hat{s}_{t+1}, \\hat{r}_t = f(s_t, a_t)\n",
    "     $$\n",
    "\n",
    "2. **Expansión de valor**: Utiliza el modelo para expandir la estimación de valor más allá del horizonte.\n",
    "\n",
    "   - **Expansión de valor**:\n",
    "     $$\n",
    "     V(s_t) = r_t + \\gamma \\hat{r}_{t+1} + \\gamma^2 \\hat{r}_{t+2} + \\dots + \\gamma^H V_\\text{target}(\\hat{s}_{t+H})\n",
    "     $$\n",
    "     - Esta ecuación combina las recompensas inmediatas predichas $\\hat{r}_t$ con el valor estimado a largo plazo $V_\\text{target}$.\n",
    "\n",
    "3. **Entrenamiento de valor**: Actualiza la función de valor utilizando la expansión de valor.\n",
    "\n",
    "   - **Actualización de la función de valor**:\n",
    "     $$\n",
    "     \\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta \\mathcal{L}(V_\\theta, V(s_t))\n",
    "     $$\n",
    "     - Se utiliza la expansión de valor para calcular las gradientes y actualizar los parámetros de la función de valor $\\theta$.\n",
    "\n",
    "#### **Usar un modelo dado**\n",
    "\n",
    "En algunos casos, se utiliza un modelo predefinido del entorno para facilitar la toma de decisiones.\n",
    "\n",
    "#### **AlphaZero**\n",
    "\n",
    "AlphaZero es un algoritmo que combina búsqueda en árbol de Monte Carlo (MCTS) con redes neuronales profundas para jugar juegos de tablero de manera superhumana.\n",
    "\n",
    "**Componentes clave**\n",
    "\n",
    "1. **Red neuronal**: Estima la política y el valor del estado.\n",
    "\n",
    "   - **Red neuronal**:\n",
    "     $$\n",
    "     \\pi(a|s), v(s) = f_\\theta(s)\n",
    "     $$\n",
    "     - La red neuronal parametrizada por $\\theta$ estima la probabilidad de cada acción $\\pi(a|s)$ y el valor del estado $v(s)$.\n",
    "\n",
    "2. **Búsqueda MCTS**: Realiza simulaciones para explorar posibles futuras secuencias de jugadas.\n",
    "\n",
    "   - **Selección**: Elige el nodo con el mayor valor de UCB.\n",
    "     $$\n",
    "     \\text{UCB}(s, a) = Q(s, a) + c \\sqrt{\\frac{\\log N(s)}{N(s, a)}}\n",
    "     $$\n",
    "     - Aquí, $Q(s, a)$ es el valor esperado de la acción $a$ en el estado $s$, $N(s)$ es el número total de visitas al nodo $s$, y $N(s, a)$ es el número de visitas al nodo hijo correspondiente a la acción $a$.\n",
    "\n",
    "   - **Expansión**: Añade un nuevo nodo al árbol de búsqueda.\n",
    "\n",
    "   - **Simulación**: Realiza un rollout desde el nodo expandido hasta un estado terminal o un número fijo de pasos.\n",
    "\n",
    "   - **Backup**: Actualiza los valores de $Q(s, a)$ hacia atrás en el árbol.\n",
    "     $$\n",
    "     Q(s, a) \\leftarrow Q(s, a) + \\frac{1}{N(s, a)} \\left( v - Q(s, a) \\right)\n",
    "     $$\n",
    "     - Aquí, $v$ es el valor simulado desde el nodo expandido.\n",
    "\n",
    "3. **Actualización de la política**: Utiliza las estadísticas de las simulaciones MCTS para actualizar la política.\n",
    "\n",
    "   - **Nueva política**:\n",
    "     $$\n",
    "     \\pi_\\text{new}(a|s) \\propto \\text{visit count}(a|s)\n",
    "     $$\n",
    "     - La nueva política se actualiza en función del conteo de visitas de las acciones durante la búsqueda MCTS.\n",
    "\n",
    "4. **Entrenamiento**: Actualiza los parámetros de la red neuronal usando las jugadas simuladas y el valor de las jugadas.\n",
    "\n",
    "   - **Pérdida de entrenamiento**:\n",
    "     $$\n",
    "     \\mathcal{L}(\\pi_\\theta, \\pi_\\text{new}, v_\\theta, z) = (z - v_\\theta)^2 - \\pi_\\text{new} \\cdot \\log \\pi_\\theta\n",
    "     $$\n",
    "     - Aquí, $z$ es el resultado del juego (1, 0, -1) y $\\pi_\\text{new}$ es la política mejorada de MCTS.\n",
    "\n",
    "   - **Actualización de los parámetros**:\n",
    "     $$\n",
    "     \\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta \\mathcal{L}(\\pi_\\theta, \\pi_\\text{new}, v_\\theta, z)\n",
    "     $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6340d9d2-44be-45b7-9159-3395b7efdee3",
   "metadata": {},
   "source": [
    "### **Aprendizaje por refuerzo desde retroalimentación humana (RLHF)**\n",
    "\n",
    "#### **El aprendizaje por imitación (IL)** \n",
    "\n",
    "Es un campo del aprendizaje automático en el cual un agente aprende a realizar tareas replicando el comportamiento de un experto. Este enfoque es especialmente útil en escenarios donde es difícil definir una función de recompensa precisa o cuando la exploración directa en el entorno es costosa o peligrosa.\n",
    "\n",
    "**1 . Behavioral cloning (BC)**\n",
    "Behavioral cloning (BC) es una técnica básica de aprendizaje por imitación que utiliza aprendizaje supervisado para entrenar al agente a partir de demostraciones de expertos.\n",
    "\n",
    "**Proceso de BC:**\n",
    "\n",
    "- Recopilación de datos: Se recopilan datos de demostraciones de un experto, que consisten en pares de estados y acciones $(s, a)$.\n",
    "- Entrenamiento del modelo: Se entrena un modelo de política (por ejemplo, una red neuronal) utilizando los pares de estados y acciones. El objetivo es minimizar la diferencia entre las acciones predichas por el modelo y las acciones del experto en los estados correspondientes.\n",
    "- Ejecución de la política: El modelo entrenado se utiliza para predecir acciones en nuevos estados durante la ejecución.\n",
    "\n",
    "**Ventajas y desventajas**:\n",
    "\n",
    "* Ventajas: Simple de implementar y puede funcionar bien si las demostraciones del experto cubren adecuadamente el espacio de estados.\n",
    "* Desventajas: Puede sufrir de errores de compounding, donde pequeños errores en la predicción de acciones pueden acumularse y llevar a un desempeño pobre. BC también depende en gran medida de la calidad y cobertura de las demostraciones del experto.\n",
    "\n",
    "**2 . DAgger (Dataset Aggregation)**\n",
    "\n",
    "DAgger (Dataset Aggregation) es una técnica que mejora BC al abordar el problema de los errores de compounding. DAgger es un enfoque iterativo que combina el aprendizaje supervisado con la retroalimentación en tiempo real del experto.\n",
    "\n",
    "**Proceso de DAgger:**\n",
    "\n",
    "- Recopilación de datos inicial: Se recopila un conjunto inicial de demostraciones del experto.\n",
    "- Entrenamiento inicial: Se entrena una política inicial usando BC con los datos iniciales.\n",
    "- Iteraciones de DAgger:\n",
    "    * Ejecución de la política actual: La política entrenada se ejecuta en el entorno, generando nuevas trayectorias de estados.\n",
    "    * Corrección del experto: El experto revisa las acciones tomadas por la política en estos nuevos estados y proporciona las acciones correctas.\n",
    "    * Agregación de datos: Los nuevos pares de estados y acciones proporcionados por el experto se agregan al conjunto de datos de entrenamiento.\n",
    "    * Reentrenamiento: La política se reentrena con el conjunto de datos ampliado.\n",
    " \n",
    "**3 . Generative adversarial imitation learning (GAIL):**\n",
    "\n",
    "GAIL combina aprendizaje por imitación con técnicas de aprendizaje generativo adversarial. En GAIL, un generador (la política del agente) y un discriminador (que distingue entre trayectorias generadas por el agente y trayectorias del experto) se entrenan conjuntamente. El objetivo del generador es producir trayectorias que sean indistinguibles de las trayectorias del experto, según el discriminador."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448779f8-dce3-45ab-9f9e-2539937c38fc",
   "metadata": {},
   "source": [
    "#### **Aprendiza inverso de reforzamiento (IRL)**\n",
    "\n",
    "El aprendizaje por refuerzo inverso (Inverse Reinforcement Learning, IRL) es un campo de la inteligencia artificial donde el objetivo es inferir la función de recompensa que un agente está tratando de maximizar a partir de su comportamiento observado. A diferencia del aprendizaje por refuerzo tradicional, donde la función de recompensa está dada y el objetivo es encontrar la política óptima, en IRL se observa la política del experto y se trata de descubrir la función de recompensa que podría haber llevado a esa política.\n",
    "\n",
    "#### **Máxima entropía inverse reinforcement learning (MaxEnt IRL)**\n",
    "\n",
    "MaxEnt IRL es un algoritmo que busca inferir una función de recompensa a partir de demostraciones de expertos, utilizando el principio de máxima entropía para manejar la incertidumbre de manera robusta. Este enfoque garantiza que entre todas las distribuciones posibles de trayectorias que coinciden con las demostraciones, se selecciona aquella que tiene la máxima entropía, evitando suposiciones innecesarias y promoviendo la diversidad.\n",
    "\n",
    "**Proceso de MaxEnt IRL**\n",
    "\n",
    "- Recopilación de datos: Obtener demostraciones de expertos que consisten en trayectorias de estados y acciones.\n",
    "- Modelado de trayectorias: Representar las trayectorias como secuencias de estados y acciones.\n",
    "- Cálculo de la entropía: Utilizar el principio de máxima entropía para modelar la probabilidad de las trayectorias.\n",
    "- Optimización: Ajustar la función de recompensa para maximizar la probabilidad de las trayectorias observadas.\n",
    "\n",
    "A continuación, se presenta un ejemplo simplificado de cómo implementar MaxEnt IRL en PyTorch. Este ejemplo asume un entorno simple donde se conocen las transiciones de estados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8c5426-707e-4b71-a887-733d3016c668",
   "metadata": {},
   "source": [
    "#### **Aprendizaje por Imitación mediante Aprendizaje por Refuerzo Inverso**\n",
    "\n",
    "El concepto de \"Aprendizaje por Imitación\" implica aprender una política similar a la de un experto mediante el aprendizaje de la función de recompensa subyacente que el experto está optimizando. Aquí te presento una implementación básica utilizando PyTorch.\n",
    "\n",
    "La idea principal es iterar entre dos fases:\n",
    "\n",
    "* Actualizar la política: Usar una técnica de RL estándar para optimizar la política dada una función de recompensa.\n",
    "* Actualizar la función de recompensa: Usar las trayectorias generadas por la política actual para ajustar la función de recompensa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1542c659-a862-4d53-9c14-664f036da2f5",
   "metadata": {},
   "source": [
    "#### **Aprendizaje profundo de refuerzo inverso (Deep IRL)**\n",
    "\n",
    "Deep IRL extiende los métodos tradicionales de IRL utilizando redes neuronales profundas para modelar la función de recompensa. Este enfoque permite manejar problemas más complejos con estados de alta dimensionalidad.\n",
    "\n",
    "**Procesos**\n",
    "\n",
    "* Definir una red neural para la función de recompensa.\n",
    "* Optimizar la función de recompensa usando trayectorias observadas.\n",
    "* Entrenar una política usando RL con la función de recompensa aprendida.\n",
    "\n",
    "Revisar: [Maximum Entropy Deep Inverse Reinforcement Learning](https://arxiv.org/abs/1507.04888)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c28145b-9364-48be-8576-99f6fe3b622f",
   "metadata": {},
   "source": [
    "#### **Aprendizaje por preferencia**\n",
    "\n",
    "El aprendizaje por preferencias es una subárea del aprendizaje automático que se centra en aprender a partir de preferencias en lugar de ejemplos etiquetados de manera explícita. En lugar de aprender a partir de pares de datos de entrada y salida, el modelo aprende a partir de comparaciones o rankings de diferentes opciones. \n",
    "\n",
    "Este enfoque es especialmente útil en situaciones donde las etiquetas exactas son difíciles de obtener, pero es fácil obtener preferencias relativas.\n",
    "\n",
    "Por ejemplo, en lugar de saber que la opción A tiene una recompensa de 10 y la opción B tiene una recompensa de 5, el modelo sólo sabe que A es preferida sobre B. Este tipo de aprendizaje es común en aplicaciones como sistemas de recomendación, donde las preferencias de los usuarios son más fáciles de obtener que las evaluaciones cuantitativas exactas.\n",
    "\n",
    "En el contexto del aprendizaje por refuerzo, el aprendizaje por refuerzo basado en preferencias (PBRL) se centra en aprender políticas óptimas a partir de preferencias en lugar de recompensas numéricas explícitas. Esto es útil en situaciones donde es difícil definir una función de recompensa exacta, pero se pueden obtener comparaciones de resultados o trayectorias.\n",
    "\n",
    "#### **El modelo Bradley-Terry**\n",
    "\n",
    "Es un modelo probabilístico utilizado para predecir el resultado de comparaciones binarias entre pares de opciones. Es ampliamente utilizado en la teoría de la decisión y en el aprendizaje por preferencias para modelar preferencias en varios contextos, como competiciones deportivas, sistemas de recomendación y encuestas de opinión.\n",
    "\n",
    "En el modelo Bradley-Terry, se asume que cada opción $i$ tiene una habilidad o \"score\" $\\theta_i$. La probabilidad de que la opción $i$ sea preferida sobre la opción $j$ se modela como:\n",
    "\n",
    "$$P(i \\succ j) = \\frac{\\theta_i}{\\theta_i + \\theta_j}$$\n",
    "\n",
    "donde $i \\succ j$ indica que la opción $i$ es preferida sobre la opción $j$.\n",
    "\n",
    "**Estimación de parámetros**\n",
    "\n",
    "Para estimar los parámetros $\\theta_i$ de cada opción $i$, se puede utilizar el método de máxima verosimilitud. La función de verosimilitud para el conjunto de comparaciones observadas se maximiza con respecto a los parámetros $\\theta$.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465a90e9-5276-4464-80e1-f19a20d9e283",
   "metadata": {},
   "source": [
    "#### **PBRL (Preference-based Reinforcement Learning)**\n",
    "El PBRL es una extensión del aprendizaje por refuerzo (RL) que utiliza preferencias en lugar de recompensas numéricas explícitas para guiar el aprendizaje del agente. En PBRL, el agente recibe retroalimentación en forma de comparaciones entre diferentes trayectorias o acciones, indicando cuál es preferida.\n",
    "\n",
    "**Conceptos clave en PBRL**\n",
    "\n",
    "* Preferencias en lugar de recompensas: En lugar de recibir recompensas numéricas después de cada acción, el agente recibe comparaciones de trayectorias o acciones.\n",
    "* Función de recompensa implícita: El agente aprende una función de recompensa implícita que satisface las preferencias observadas.\n",
    "* Actualización de la política: La política del agente se actualiza para maximizar la probabilidad de generar trayectorias que sean consistentes con las preferencias observadas.\n",
    "  \n",
    "#### **Algoritmos de PBRL**\n",
    "\n",
    "**Preferencia por trayectorias (trajectory preference learning):**\n",
    "\n",
    "En este enfoque, el agente aprende a partir de comparaciones entre trayectorias completas. Las trayectorias son secuencias de estados y acciones, y el agente recibe retroalimentación sobre qué trayectorias son preferidas.\n",
    "\n",
    "- Ejemplo de algoritmo: Maximum Entropy Inverse Reinforcement Learning (MaxEnt IRL), donde se aprende una función de recompensa que maximiza la entropía de la política condicionada a las preferencias observadas.\n",
    "\n",
    "**Preferencia por parejas de estados (state pair preference learning):**\n",
    "\n",
    "Aquí, el agente aprende a partir de comparaciones entre pares de estados o pares de estado-acción. La retroalimentación indica qué par es preferido.\n",
    "\n",
    "- Ejemplo de algoritmo: Preference-Based Policy Iteration (PBPI), donde se actualiza la política basada en las preferencias entre pares de estados.\n",
    "\n",
    "**Preferencia por acciones (action preference learning):**\n",
    "\n",
    "En este enfoque, el agente aprende preferencias entre diferentes acciones en un estado dado. La retroalimentación se proporciona sobre cuál acción es preferida en un estado específico.\n",
    "\n",
    "- Ejemplo de algoritmo: Bayesian Preference Learning, donde se usa inferencia bayesiana para aprender las preferencias y actualizar la política en consecuencia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b49633-747d-465a-8734-ecc6ced8691e",
   "metadata": {},
   "source": [
    "Hay varios algoritmos más de PBRL que utilizan técnicas como el modelo de Bradley-Terry para manejar preferencias en lugar de recompensas numéricas tradicionales. \n",
    "\n",
    "**Learning from ranked trajectories (RaPP)**\n",
    "\n",
    "El algoritmo RaPP se basa en la idea de aprender una política óptima a partir de trayectorias que están ordenadas según las preferencias. En este enfoque, el agente aprende una función de valor que respeta el orden de las trayectorias proporcionadas.\n",
    "\n",
    "Proceso de RaPP\n",
    "\n",
    "- Recopilación de datos: Obtener conjuntos de trayectorias ordenadas por preferencias.\n",
    "- Modelado de la función de valor: Utilizar un modelo para estimar los valores de las trayectorias.\n",
    "- Optimización: Ajustar la política para maximizar la preferencia de las trayectorias ordenadas.\n",
    "\n",
    "**Coactive learning**\n",
    "\n",
    "Coactive learning es un enfoque donde un agente interactúa con un experto y recibe feedback en forma de preferencias sobre acciones. Este feedback se utiliza para ajustar la política del agente.\n",
    "\n",
    "Proceso de coactive learning\n",
    "\n",
    "- Interacción: El agente toma una acción y recibe feedback en forma de preferencia.\n",
    "- Actualización: La política se actualiza para preferir las acciones que el experto prefiere.\n",
    "- Repetición: Este proceso se repite para refinar la política del agente.\n",
    "\n",
    "**Active preference-based learning (APBL)**\n",
    "\n",
    "APBL es un enfoque activo donde el agente selecciona pares de trayectorias para comparación con el fin de aprender de manera más eficiente. Este enfoque utiliza técnicas de selección activa para mejorar el aprendizaje.\n",
    "\n",
    "Proceso de APBL\n",
    "\n",
    "- Selección activa: El agente selecciona pares de trayectorias que son más informativas.\n",
    "- Feedback de preferencias: El agente recibe feedback en forma de preferencias.\n",
    "- Actualización de la política: La política se ajusta para reflejar el feedback recibido."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
