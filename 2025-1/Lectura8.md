### Avances en atención posicional y adaptativa

Más allá de las codificaciones posicionales aprendibles, relativas o rotatorias, un reciente enfoque explora la **Distance-Enhanced Attention Scores**, que añade un término explícito en las puntuaciones de atención para ponderar la contribución de cada par de posiciones según su distancia absoluta. En lugar de depender únicamente de sesgos aprendibles $b_{i-j}$, se introduce una función $f(|i-j|)$ (por ejemplo, un kernel gaussiano o exponencial) que penaliza progresivamente la atención entre tokens alejados. El resultado es una matriz de atención donde las posiciones próximas obtienen un refuerzo natural que decae de forma controlada con la distancia, ayudando a modelos a centrarse en contextos locales relevantes sin descuidar relaciones de largo alcance.

Relacionado, el **Distance Aware Transformer** amplía esta idea incorporando no solo un sesgo escalar, sino vectores de parámetro dependientes de la distancia. Cada cabecera de atención aprende una serie de vectores $\Delta_1, \Delta_2, \dots, \Delta_K$ que se seleccionan según la diferencia de posición cuantizada en rangos discretos. Así, la atención llega a manejar explícitamente múltiples escalas de proximidad, lo que resulta útil en secuencias muy largas donde los patrones de dependencia pueden variar drásticamente entre ámbitos cercanos y alejados.

Otra técnica ingeniosa es **Attention with Linear Biases (ALiBi)**. En lugar de sumar codificaciones posicionales a los embeddings, ALiBi modifica las puntuaciones de producto punto directamente añadiendo un sesgo lineal proporcional a la diferencia de posiciones:

$$
\alpha_{ij} = \frac{Q_i K_j^\top}{\sqrt{d_k}} + m \cdot (j - i),
$$

donde $m$ es una pendiente negativa aprendible por cabecera. Este sesgo hace que, a igualdad de similitud semántica, la atención favorezca posiciones anteriores cercanas, y permite extrapolar a longitudes de secuencia mayores que las vistas durante el entrenamiento, sin necesidad de codificaciones posicionales adicionales.

El **Universal Transformer** propone un paradigma aún más dinámico: en cada capa, las mismas subcapas de atención y feed-forward se aplican de manera recurrente un número de pasos que puede variar por posición. Gracias a un mecanismo de control basado en un parámetro aprendido, cada token decide cuántas iteraciones de transformación requiere, permitiendo profundizar más en tokens semánticamente complejos y detenerse antes en tokens más triviales. Así se combina la potencia de los Transformers con la adaptabilidad de las redes recurrentes, logrando un uso más eficiente del cómputo.

El concepto de adaptabilidad se lleva también al **Adaptive Attention Span**, donde cada cabecera de atención ajusta dinámicamente el rango máximo de posiciones que atiende. En lugar de atender a toda la secuencia previa, cada cabecera aprende un parámetro escalar $L$ (o un vector de límites por capa) que indica su ventana de atención efectiva. Durante el entrenamiento, un término de regularización impulsa a las cabeceras a reducir su span si no aporta ganancia de performance. El resultado es un modelo que, automáticamente, entiende qué partes del contexto requieren atención amplia y cuáles pueden simplificarse.

Por otro lado, el **Depth-Adaptive Transformer** adapta la **profundidad** de la red a cada token. Similar al Universal Transformer pero aplicado a capas, se emplea un controlador que decide hasta qué capa debe propagarse la representación de un token antes de detenerse. Esto permite dedicar más capas de computación a tokens con alta complejidad semántica y detener el procesamiento de tokens redundantes o previsibles, reduciendo significativamente el costo de inferencia.

En el terreno de la eficiencia y la confianza en la generación, el **Confident Adaptive Language Model (CALM)** introduce un criterio de parada temprana en generación basado en la entropía de la distribución de salida. Cuando la predicción para un token alcanza cierta certeza (entropía inferior a un umbral), el modelo omite pasos adicionales de recomputación o refinamiento para ese token, acelerando el proceso manteniendo la calidad.

A nivel de eficiencia general de atención, surgen arquitecturas de **Efficient Attention** que reformulan los cálculos de producto punto en variantes aproximadas con complejidad cercana a lineal. Por ejemplo, métodos basados en **kernelized attention** proyectan las queries y keys a un espacio de característica donde la atención puede expresarse como producto de sumas, evitando la construcción completa de la matriz $QK^\top$. Técnicas como **Linformer** o **Performer** se inscriben en esta línea, reduciendo tanto memoria como tiempo de cómputo.

En paralelo, proliferan patrones de **Sparse Attention Patterns**: se selecciona un subconjunto de posiciones sobre las que aplicar la atención, basándose en rejillas regulares, ventanas locales deslizantes, bloques dilatados o agrupamientos por clustering semántico. Un ejemplo notable es el **Sparse Transformer**, que alterna atención en bloques de largo alcance con atención local en ventanas fijas, logrando atender a la totalidad de la secuencia de manera escalable.

El **Blockwise Attention** lleva el concepto de bloques un paso más allá: se divide la secuencia en bloques contiguos y se aplican distintas estrategias de atención intra-bloque y entre-bloques. Dentro de un bloque se atiende exhaustivamente a todas las posiciones (complejidad cuadrática limitada al bloque), mientras que entre bloques solo se realiza atención a ciertas posiciones representativas o se utilizan proyecciones reducidas. Este diseño aprovecha al máximo la densidad de atención local y reduce drásticamente la carga global.

La combinación de atención dispersa con codificaciones avanzadas, por ejemplo, ALiBi dentro de bloques, o sesgos de distancia en ventanas locales, ofrece un arsenal de posibilidades para construir Transformers que escalen a miles o decenas de miles de tokens, sin explotar la memoria ni renunciar a la capacidad de capturar patrones complejos.

En su conjunto, estos avances, Distance-Enhanced Scores, Transformers conscientes de la distancia, ALiBi, Universal y Depth-Adaptive, atención adaptativa en span y bloques, métodos kernelizados de Efficient Attention, y arquitecturas con memoria jerarquizada o externa, configuran un paisaje inmenso de posibilidades para ampliar el contexto, refinar el uso de cómputo y dotar a los modelos de capacidades de memoria y reflexión más humanas. La fusión de estas ideas impulsa el desarrollo de sistemas capaces de procesar documentos completos, diálogos extensos y flujos de datos continuos, todo ello preservando la eficiencia y la escalabilidad necesarias para aplicaciones reales.

###  Contextos extremadamente largos y patrones de dependencia de distinta escala

Para afrontar los retos de manejar contextos extremadamente largos y patrones de dependencia de distinta escala, muchas arquitecturas combinan estrategias de atención local y global. En estos modelos, cada token atiende exhaustivamente a un vecindario inmediato (atención local) mientras que un conjunto reducido de "pivotes" o tokens seleccionados participa en una atención global compartida. De este modo, se garantiza que la información crítica dispersa a lo largo del documento siga siendo accesible sin asumir la carga cuadrática de atender a todas las posiciones simultáneamente.

Un ejemplo pionero de este enfoque es **ETC (Extended Transformer Construction)** de Ainslie et al. (2019). ETC introduce dos flujos de atención: uno local, en el que cada token puede atender a sus $w$ vecinos más cercanos, y otro global, consistente en un pequeño conjunto de tokens designados que pueden atender y ser atendidos por cualquier posición de la secuencia. Los tokens globales suelen incluir delimitadores de sección, encabezados o marcadores semánticos. Esta dualidad permite procesar miles de tokens con un coste cercano a lineal, mientras que la atención global garantiza la coherencia y el paso de información entre segmentos distantes.

En la misma línea, **Longformer** (Beltagy et al., 2020) propone un patrón de atención dilatada para la porción local (donde cada token atiende a los $k$ vecinos más próximos), junto con "saltos" regulares de atención global en intervalos fijos. Además, permite que cualquier token etiquetado como "global" acceda a la secuencia entera, lo que resulta ideal para tareas de clasificación de documentos largos o para seguir vínculos referenciales de largo alcance. Longformer soporta entradas de decenas de miles de tokens con un uso eficiente de memoria y ha demostrado eficacia en rescatar información clave en textos legales, científicos o notas médicas extensas.

**Big Bird** amplía aún más el concepto de atención dispersa: combina ventanas locales, atención a un conjunto aleatorio de posiciones (para romper patrones rígidos y mejorar la conectividad) y atención a tokens globales. Este esquema mixto ofrece garantías teóricas de aproximación de la atención densa completa (preservando invarianza en la longitud de secuencia) y permite un paralelismo efectivo. Big Bird ha alcanzado resultados competitivos en benchmarks de largo contexto y, gracias a su combinación de estrategias, ofrece un poderoso marco para manejar documentos masivos.

Por su parte, la **atención basada en contenido (Content-based Attention)** asigna pesos según la similitud semántica de las representaciones de los tokens, sin incorporar directamente información posicional. En contextos donde la proximidad física es menos relevante que la asociación temática (por ejemplo, enlazar conceptos dispersos en un documento científico), priorizar la atención por contenido puede mejorar la coherencia inferida, siempre que se complemente con algún mecanismo posicional para evitar ambigüedades.

Para acelerar aún más la atención en secuencias largas, varias arquitecturas emplean **Locality-Sensitive Hashing (LSH) Attention**. Modelos como **Reformer** agrupan las queries y keys en buckets mediante hashing sensible a la similitud: cada token solo atiende a otros tokens que caen en el mismo bucket, garantizando que solo se procesen pares con alta probabilidad de relevancia. Con ello se reduce drásticamente la complejidad de $\mathcal{O}(n^2)$ a cerca de $\mathcal{O}(n \log n)$, manteniendo una cobertura probabílistica de las dependencias importantes.

En el plano de la profundidad de red, los **Reversible Residual Networks** permiten reconstruir las activaciones de capas previas a partir de las salidas, lo que elimina la necesidad de almacenarlas durante el forward pass. Cuando se integra con Transformers, esta técnica reduce drásticamente la huella de memoria de la retropropagación, permitiendo usar redes más profundas sin agotar la RAM de la GPU. Cada bloque reversible solo necesita retener su salida y algunos parámetros, reconstruyéndose on-the-fly durante el backprop.

Otra estrategia de compresión computacional es la **Low-Rank Attention**, que aproxima la matriz $QK^\top$ por un producto de rangos inferiores, $\tilde{Q}\,\tilde{K}^\top$, donde $\tilde{Q},\tilde{K}\in\mathbb{R}^{n\times r}$ y $r \ll d$. Con ello, la multiplicación y el softmax resultan mucho más rápidos y consumen menos memoria, especialmente cuando las queries y keys tienen correlaciones que hacen redundante la representación completa.

El **Linformer** va en paralelo con esta idea, proponiendo que las keys y values pueden proyectarse a un espacio de dimensión $k$ mucho menor que la longitud de la secuencia, empleando matrices de proyección compartidas por todas las posiciones. Así, la atención se calcula sobre tensores de tamaño $n\times k$ en lugar de $n\times n$, logrando complejidad lineal y demostrando que, in vitro, grandes cantidades de información posicional pueden comprimirse sin perder capacidad de modelado.

Más allá de las proyecciones deterministas, los **Random Feature Attention** (RFA) o **Performer** usan técnicas de aproximación basadas en transformaciones aleatorias de Fourier para representar el softmax de producto punto como un kernel dot-product en un espacio de características. Con un mapeo $\phi: \mathbb{R}^d \to \mathbb{R}^r$ elegido aleatoriamente, la atención se puede reescribir como

$$
\text{Attention}(Q,K,V) \approx \phi(Q)\bigl(\phi(K)^\top V\bigr),
$$

logrando un escalado lineal $O(nr)$. Además, el **Causal Attention RFA** extiende esta idea al caso autoregresivo, garantizando que el cálculo dependa únicamente de posiciones previas a la actual, lo que resulta crucial para modelos de generación de texto.

El **Performer** es una implementación práctica de RFA que ofrece garantías teóricas de convergencia a la atención softmax real cuando $r$ crece, y que ha sido adoptada con éxito en modelos de conversación y traducción de gran escala.

Gracias a la combinación de estos avances, localidad y globalidad híbridas, hashing posicional, redes reversibles, factorizaciones de bajo rango, transformaciones aleatorias, atención dispersa y projections lineales, los Transformers modernos pueden escalar a contextos cada vez más extensos, profundizar en estructuras semánticas complejas y desplegarse en entornos de recursos ajustados sin renunciar a la calidad de modelado de secuencias de texto, código o cualquier tipo de datos secuenciales.

