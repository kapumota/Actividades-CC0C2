{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71a8df3f-fc68-4d81-9298-7b53a557b05a",
   "metadata": {},
   "source": [
    "### **Examen Parcial CC0C2**\n",
    "\n",
    "**Indicaciones**\n",
    "\n",
    "\n",
    "- **Duración del examen:** 3 horas.\n",
    "- **Recursos permitidos:** Apuntes de clase y repositorios oficiales del curso (p. e. GitHub del temario).\n",
    "- **Recursos prohibidos:** ChatGPT, otros servicios de IA, foros públicos, sitios de intercambio de respuestas o cualquier herramienta de generación automática de código o texto.\n",
    "- **Entrega obligatoria:** Todas las secciones deben completarse para obtener puntuación completa. Si no se entregan de forma íntegra, la máxima nota será 1 punto en esa pregunta.\n",
    "- **Anulación del examen** : Cualquier uso de IA o copia detectada resultará en calificación 0 en TODO el examen, sin opción de recuperación.\n",
    "- **Archivo comprimido:** Entrega un archivo zip con tus nombres y apellidos y dentro tus respuestas en carpetas llamadas Ejercicio1, Ejercicio2, Ejercicio3, Ejercicio4. Se penalizará 1 punto por pregunta sino se respeta esta indicación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86dec4c-9300-4bed-8c08-c972a8f1f9fe",
   "metadata": {},
   "source": [
    "**Ejercicio 1: Implementación didáctica de Byte-Pair Encoding (BPE)**\n",
    "\n",
    "El propósito de este ejercicio es que comprendas a fondo las estructuras de datos y los pasos algorítmicos que sustentan Byte-Pair Encoding, sin distraerte con la escala de un corpus real. Por ello, trabajarás sobre textos breves —puedes cargar un archivo `.txt` con pocas líneas o generar un pequeño conjunto de frases dentro del propio script— y te centrarás en hacer cada decisión visible y justificable.\n",
    "\n",
    "**Primera parte: cargador autocontenible**\n",
    "\n",
    "Desarrolla una función\n",
    "\n",
    "```python\n",
    "crear_data_loader(ruta: Optional[str], tamaño_batch: int) -> Iterator[List[str]]\n",
    "```\n",
    "\n",
    "* Si `ruta` es `None`, el loader deberá fabricar un mini-corpus (p. ej. tres citas célebres o un párrafo de lorem ipsum).\n",
    "* El iterador debe entregar lotes **en el orden original** y reiniciarse limpiamente con `iter(loader)` para garantizar reproducibilidad.\n",
    "\n",
    "\n",
    "**Segunda parte: aprendizaje de reglas BPE paso a paso**\n",
    "\n",
    "Implementa\n",
    "\n",
    "```python\n",
    "entrenar_bpe_simple(\n",
    "    corpus: List[str],\n",
    "    num_merges: int\n",
    ") -> List[Tuple[str, str]]\n",
    "```\n",
    "\n",
    "* Tokenizado a nivel de carácter, añadiendo el marcador de fin de palabra `</w>`.\n",
    "* En cada iteración, localiza el par adyacente más frecuente, fusiónalo y **actualiza sólo las frecuencias de las palabras afectadas** (sin recomputar todo).\n",
    "* Mantén registro de:\n",
    "\n",
    "  1. Iteración\n",
    "  2. Par fusionado\n",
    "  3. Frecuencias antes y después\n",
    "* Al terminar, **imprime automáticamente** las primeras **5** reglas aprendidas con un breve comentario en el código explicando cómo lograste la actualización eficiente.\n",
    "\n",
    "\n",
    "**Tercera parte: tokenización y des-tokenización**\n",
    "\n",
    "Crea dos funciones:\n",
    "\n",
    "```python\n",
    "tokenizar_con_bpe(texto: str, reglas: List[Tuple[str,str]]) -> List[str]\n",
    "detokenizar_bpe(tokens: List[str]) -> str\n",
    "```\n",
    "\n",
    "* `tokenizar_con_bpe` aplica las fusiones en orden.\n",
    "* `detokenizar_bpe` invierte el proceso.\n",
    "* En `__main__`, incluye **al menos dos frases distintas** y, para cada una, varios `assert` de comprobación, por ejemplo:\n",
    "\n",
    "  ```python\n",
    "  original = \"tu frase aquí\"\n",
    "  tokens   = tokenizar_con_bpe(original, reglas)\n",
    "  recovered = detokenizar_bpe(tokens)\n",
    "  assert recovered.replace(\" </w>\", \"\") == original, \"Round-trip falló\"\n",
    "  print(\"Round-trip OK para:\", original)\n",
    "\n",
    "  # Verificación de vocabulario y pares comunes:\n",
    "  assert len(vocab_inicial) > len(vocab_final), \"El vocabulario no se ha reducido\"\n",
    "  assert any(pair in reglas for pair in [(\"t\",\"h\"), (\"e\",\"r\")]), \"No se detectaron pares comunes\"\n",
    "  ```\n",
    "\n",
    "**Cuarta parte: análisis de complejidad**\n",
    "\n",
    "En el archivo  `Instrucciones.md` (o en el docstring de la función principal), explica el coste temporal de realizar $n$ fusiones cuando el vocabulario tiene $V$ tipos y el corpus $T$ tokens. Señala hasta qué punto tu estrategia (p. ej. un multiset de pares con heap y punteros inversos) escala antes de volverse cuello de botella.\n",
    "\n",
    "**Prueba canónica**\n",
    "\n",
    "En el bloque:\n",
    "\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    # — demo mínima —\n",
    "    # 1) genera un corpus diminuto\n",
    "    # 2) ejecuta 20 fusiones\n",
    "    # 3) imprime las 5 primeras reglas (con frecuencias antes/después)\n",
    "    # 4) round-trip con asserts para 2 frases y comprobaciones extra\n",
    "```\n",
    "\n",
    "al ejecutar:\n",
    "\n",
    "```bash\n",
    "python bpe.py\n",
    "```\n",
    "\n",
    "debes ver:\n",
    "\n",
    "1. Tres líneas con las **5 primeras fusiones** y sus frecuencias antes/después.\n",
    "2. Dos mensajes `Round-trip OK para: ...`.\n",
    "3. No debe haber errores de `assert`.\n",
    "\n",
    "\n",
    "**Entregable**\n",
    "\n",
    "1. **`bpe.py`**\n",
    "\n",
    "   * Contiene las 4 funciones (`crear_data_loader`, `entrenar_bpe_simple`, `tokenizar_con_bpe`, `detokenizar_bpe`) y la demo en `__main__` con los `assert` de verificación.\n",
    "2. **`Instrucciones.md`**\n",
    "\n",
    "   * Pasos de ejecución (`python bpe.py`), salida esperada de las 5 primeras reglas y mensajes .\n",
    "   * Breve explicación de la actualización incremental de frecuencias.\n",
    "3. **`sample_corpus.txt`** (opcional)\n",
    "\n",
    "   * Un corpus de \\~20 líneas si quieres pasar `ruta`.\n",
    "4. **`requirements.txt`**\n",
    "\n",
    "   * Dejar vacío si no hay dependencias externas, o incluir mínimamente `pytest` sólo si decides usarlo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ce4ecf8-6a48-44d6-a963-260d5b507b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f2ba30-8a73-44fe-b058-f94ba993958b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**Ejercicio 2: Word Embeddings y razonamiento analógico**\n",
    "\n",
    "- Explica por qué entrenar Word2Vec con el objetivo de predecir contexto produce vectores que capturan semántica. ¿En qué se diferencia conceptualmente CBOW de Skip-gram y qué impacto tiene el tamaño de ventana en el significado aprendido?\n",
    "- Deriva la fórmula de la similitud coseno entre dos vectores e interpreta su rango. ¿Por qué se prefiere sobre la distancia Euclídea en espacios de alta dimensión al comparar palabras?\n",
    "- Define \"sesgo\" en el contexto de embeddings. Propón un experimento sencillo (sin código) para detectar sesgo de género en un modelo pre-entrenado y discute dos posibles estrategias de mitigación.\n",
    "\n",
    "**Implementaciones**\n",
    "\n",
    "1. **Búsqueda de palabras similares**\n",
    "   *Contexto:* Generarás embeddings sintéticos reproducibles a partir de un vocabulario de ≈ 30 palabras (incluye pares de género y profesiones).\n",
    "   *Tarea:*\n",
    "\n",
    "   * a) Implementa `crear_embeddings_semilla(vocab, dim=50, seed=42)` que asigne a cada palabra un vector gaussiano fijo.\n",
    "   * b) Implementa `encontrar_palabras_mas_similares(palabra_objetivo, embeddings, top_n=5)` usando similitud coseno y manejo de *out-of-vocabulary*.\n",
    "   * Prueba: Elige tres palabras y muestra sus cinco vecinos más cercanos (vector y score). Analiza brevemente la coherencia (esperarás coherencia limitada, pues los vectores son aleatorios).\n",
    "\n",
    "2. **Resolución de analogías y detección de sesgo**\n",
    "   *Contexto:* Utiliza los mismos embeddings del punto anterior.\n",
    "   *Tarea:*\n",
    "\n",
    "   * a) Implementa `resolver_analogia(w1, w2, w3, embeddings)` que calcule `vec(w2) − vec(w1) + vec(w3)` y devuelva la palabra más cercana, excluyendo `w1`, `w2`, `w3`.\n",
    "   * b) Ejecuta dos analogías distintas (p. ej. \"rey : hombre :: ? : mujer\" y \"doctor : hospital :: maestro : ?\") y comenta si los resultados tienen sentido.\n",
    "   * c) Diseña un experimento mínimo que pueda exponer un sesgo (p. ej. compara las distancias de *ingeniero* a *hombre* y *mujer*). Debido a la naturaleza aleatoria de los vectores, el \"sesgo\" observado será fortuito; discute por qué, en embeddings reales, ese efecto suele ser sistemático y cómo podría medirse formalmente.\n",
    "\n",
    "3. **Prueba:**\n",
    "     - Elige 3 palabras de tus embeddings y encuentra sus 5 palabras más similares.\n",
    "     - Intenta resolver 2 analogías diferentes usando tus embeddings. Analiza si los resultados tienen sentido.\n",
    "     - Identifica (si es posible con tu pequeño conjunto) un ejemplo que podría sugerir un sesgo en los embeddings (ej. asociaciones de género con profesiones). Discute por qué podría ser un sesgo.\n",
    "\n",
    "\n",
    "**Entregable**\n",
    "\n",
    "* **`word2vec_toy.py`** -> funciones + demo en `__main__`.\n",
    "* Cualquier salida requerida (vecinos, analogías, discusión) debe imprimirse en consola de forma legible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd34ffa-ccb2-43e5-896e-a8797adcea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf08fd0a-77d4-41ec-8a2f-e291cdb74b3a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**Ejercicio 3: Redes neuronales para NLP y secuencia a secuencia básico**\n",
    "\n",
    "- Explica la diferencia entre **greedy decoding**, **beam search** y el **muestreo con temperatura** durante la generación. ¿Qué ventajas y desventajas tiene cada método? ¿Cuándo emplearías *teacher forcing* en el entrenamiento y por qué?\n",
    "- ¿Por qué un único vector de contexto limita los Seq2Seq clásicos al traducir oraciones largas?. Describe la atención \"global\" de Bahdanau y cómo soluciona dicho problema.\n",
    "- ¿Qué función cumple la máscara de atención cuando se usa *teacher forcing*?\n",
    "- Define *gradient clipping* por valor y por norma. Explica cómo mitigan la explosión de gradientes en RNN/LSTM. Menciona un efecto negativo de usar un umbral de clipping demasiado bajo.\n",
    "- ¿Qué es la *teacher-forcing ratio*?. Propón dos escenario para reducirla a lo largo del entrenamiento y justifica sus ventajas.\n",
    "\n",
    "\n",
    "\n",
    "1. **Clasificador de sentimiento con RNN/LSTM/GRU**\n",
    "\n",
    "   **Contexto:** Se te proporciona un micro-dataset de 20 frases etiquetadas en positivo (1) o negativo (0).\n",
    "\n",
    "   **Dataset (TSV) — `data/sentiment_tiny.tsv`**\n",
    "\n",
    "   ```\n",
    "   I love this place!\t1\n",
    "   Worst service ever.\t0\n",
    "   The food was amazing.\t1\n",
    "   I will never come back.\t0\n",
    "   Absolutely fantastic experience!\t1\n",
    "   Terrible, simply terrible.\t0\n",
    "   Not bad at all.\t1\n",
    "   It was okay, nothing special.\t0\n",
    "   I’m delighted with the result.\t1\n",
    "   This is disappointing.\t0\n",
    "   Great job, team!\t1\n",
    "   I hate waiting so long.\t0\n",
    "   Superb quality and taste.\t1\n",
    "   The product broke instantly.\t0\n",
    "   Highly recommend it.\t1\n",
    "   Save your money, skip this.\t0\n",
    "   Totally worth the price.\t1\n",
    "   Service was rude and slow.\t0\n",
    "   Exceeded my expectations.\t1\n",
    "   I regret buying this.\t0\n",
    "   ```\n",
    "\n",
    "   *Si prefieres no usar archivos externos, declara en tu código las listas `sentences` y `labels` con el mismo contenido.*\n",
    " * **Tarea:**\n",
    "   * Preprocesa el texto: tokeniza, crea un vocabulario, y convierte las secuencias de texto a secuencias de índices. Realiza padding para que todas las secuencias tengan la misma longitud.\n",
    "    * Construye un modelo de clasificación de sentimiento usando una capa de Embedding de PyTorch, seguida por una capa RNN simple, LSTM o GRU, y finalmente una capa Densa para la clasificación.\n",
    "    * Entrena el modelo con los datos proporcionados.\n",
    "    * Evalúa el modelo en un pequeño conjunto de test (precisión, recall, F1-score).\n",
    "    * Entrena el modelo una vez con RNN simple y otra con LSTM (o GRU). Compara brevemente sus rendimientos y tiempos de entrenamiento (si son notorios).\n",
    "    * Debes justificar la elección de tus hiperparámetros (tamaño del embedding, unidades en la capa recurrente, etc.). La implementación debe ser clara y mostrar el flujo de datos. Discute brevemente cómo el padding podría afectar el rendimiento y si hay alternativas (aunque no necesitas implementarlas).\n",
    "\n",
    "**Entregable:** Código Python o en PyTorch, resultados de la evaluación, y una discusión sobre tus elecciones de arquitectura y los resultados comparativos.\n",
    "\n",
    "2.  **Simulador de Decoder Seq2Seq con Beam Search :**\n",
    "    * **Contexto:** No vas a entrenar un modelo Seq2Seq completo, sino a implementar la lógica de decodificación beam search, asumiendo que tienes acceso a las salidas de probabilidad de un \"modelo\" caja negra.\n",
    "    * **Tarea:**\n",
    "        * Implementa una función `beam_search_decoder(estado_inicial_decoder, k_beam, max_longitud_secuencia, funcion_paso_modelo)`.\n",
    "        * `estado_inicial_decoder`: Representa el estado inicial del decoder (podría ser simplemente un token de inicio).\n",
    "        * `k_beam`: El tamaño del haz (beam width).\n",
    "        * `max_longitud_secuencia`: Longitud máxima de la secuencia generada.\n",
    "        * `funcion_paso_modelo(secuencia_parcial_actual, estado_decoder_previo)`: Esta es una función **simulada** que tú definirás. Debe tomar una secuencia parcial y devolver una distribución de probabilidad sobre el siguiente token del vocabulario (un diccionario `token: probabilidad`). Para simularla, puedes usar probabilidades fijas o aleatorias con cierta lógica (ej. favorecer ciertos tokens después de otros).\n",
    "        * Tu función `beam_search_decoder` debe mantener `k_beam` secuencias candidatas en cada paso, expandirlas, y seleccionar las `k_beam` mejores basadas en su probabilidad acumulada (log-probabilidad es mejor).\n",
    "    * **Importante:** La lógica de mantener y podar los haces correctamente es crucial. Debes explicar claramente cómo calculas las puntuaciones de las hipótesis y cómo manejas el final de la secuencia (token `<EOS>`).\n",
    "    * **Simulación:**\n",
    "        * Define un vocabulario pequeño (ej. 5-6 palabras, incluyendo `<SOS>`, `<EOS>`).\n",
    "        * Crea tu `funcion_paso_modelo` simulada. Haz que tenga algún comportamiento predecible pero no trivial (ej. si la secuencia actual es \"A B\", que \"C\" tenga alta probabilidad).\n",
    "        * Ejecuta tu `beam_search_decoder` con `k_beam=2` y `k_beam=3` y muestra las secuencias generadas y sus puntuaciones. Compara los resultados.\n",
    "\n",
    "**Entregable:** Código Python de `beam_search_decoder` y tu `funcion_paso_modelo` simulada, los resultados de las ejecuciones y una explicación detallada del algoritmo de beam search tal como lo implementaste.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b167102-75dc-4f98-adab-daa4d1b89a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361ce2e5-768e-428d-8e98-6593e91e0000",
   "metadata": {},
   "source": [
    "**Ejercicio 4: Mecanismos de atención y preparación para LLM**\n",
    "\n",
    "1.  **Cálculo de pesos de atención aditiva:**\n",
    "    * **Contexto:** Estás en un paso de decodificación de un modelo Seq2Seq con atención de Bahdanau. Tienes el estado oculto anterior del decoder ($s_{t-1}$) y todos los estados ocultos del encoder ($h_1, h_2, ..., h_N$).\n",
    "    * **Tarea:**\n",
    "        * Implementa una función `calcular_pesos_atencion_bahdanau(estado_decoder_anterior, estados_encoder)`.\n",
    "        * `estado_decoder_anterior`: Un vector numpy que representa $s_{t-1}$.\n",
    "        * `estados_encoder`: Una lista de vectores numpy (o una matriz numpy donde cada fila es un $h_j$) que representa los estados ocultos del encoder.\n",
    "        * Dentro de la función, debes implementar el cálculo del score de alineación (energía) $e_{tj} = v_a^T \\tanh(W_a s_{t-1} + U_a h_j)$.\n",
    "            * Asume que $W_a$, $U_a$ son matrices de pesos y $v_a$ es un vector de pesos. **Debes inicializar estas matrices/vectores con dimensiones compatibles** (ej. si los estados ocultos son de dim D, $W_a$ y $U_a$ podrían ser DxD, y $v_a$ de D_attn x 1, donde la capa tanh interna produce D_attn). Elige dimensiones pequeñas y razonables (ej. D=4, D_attn=3).\n",
    "        * Luego, calcula los pesos de atención $\\alpha_{tj}$ aplicando softmax sobre los scores $e_{tj}$.\n",
    "        * La función debe devolver los pesos de atención $\\alpha_t$.\n",
    "    * **Simulación:**\n",
    "        * Define un `estado_decoder_anterior` de ejemplo (ej. `np.random.rand(4)`).\n",
    "        * Define una lista de 3 `estados_encoder` de ejemplo (ej. `[np.random.rand(4), np.random.rand(4), np.random.rand(4)]`).\n",
    "        * Inicializa tus matrices de pesos $W_a, U_a, v_a$ (puedes usar `np.random.rand` o valores fijos para reproducibilidad). Muestra sus dimensiones.\n",
    "        * Llama a tu función y muestra los scores de energía calculados y los pesos de atención finales. Verifica que los pesos sumen aproximadamente 1.\n",
    "    * La correcta implementación de las operaciones matriciales y el softmax. La explicación de cómo las dimensiones de $W_a, U_a, v_a$ afectan el cálculo y cómo se relacionan con las dimensiones de los estados del encoder y decoder.\n",
    "\n",
    "**Entregable:** Código Python, las matrices de pesos inicializadas, los scores de energía y los pesos de atención calculados. Una explicación detallada de la matemática implementada y las dimensiones.\n",
    "    \n",
    "2.  **Mini-análisis de autoatención (conceptual con implementación parcial):**\n",
    "    * **Contexto:** Quieres entender cómo la autoatención podría funcionar para una secuencia corta, enfocándote en el cálculo de Q, K, V y la primera parte del score de atención.\n",
    "    * **Tarea:**\n",
    "        * Dada una secuencia de entrada de 3 palabras, donde cada palabra está representada por un embedding simple (ej. vector de dimensión 4):\n",
    "            `embeddings_entrada = [emb_palabra1, emb_palabra2, emb_palabra3]`\n",
    "            (Puedes usar `np.random.rand(4)` para cada `emb_palabraX`).\n",
    "        * Define matrices de pesos $W_Q, W_K, W_V$ (ej. de dimensión 4x3, para proyectar los embeddings de dim 4 a una dim_k=3). Inicialízalas.\n",
    "        * **Implementa:**\n",
    "            1.  Calcula las matrices Q, K, V multiplicando `embeddings_entrada` (o una matriz formada por ellos) por $W_Q, W_K, W_V$ respectivamente. Muestra las dimensiones resultantes.\n",
    "            2.  Calcula los scores de atención  como $QK^T$. No necesitas aplicar el escalado por $\\sqrt{d_k}$ ni el softmax para esta pregunta, solo el producto matricial. Muestra esta matriz de scores.\n",
    "        * **Análisis:**\n",
    "            * ¿Qué representan las filas y columnas de la matriz $QK^T$ resultante?\n",
    "            * Si un valor en la posición $(i, j)$ de $QK^T$ es alto, ¿qué implicaría sobre la relación entre la palabra $i$ y la palabra $j$ en el contexto de la autoatención (antes del softmax)?\n",
    "            * Explica brevemente cómo se usaría luego la matriz V junto con los scores de atención (después de softmax) para obtener las nuevas representaciones de las palabras.\n",
    "    * **Desafío:** Conectar la implementación de las multiplicaciones matriciales con la comprensión conceptual de Q, K, V y lo que representa la matriz $QK^T$. La explicación debe ser clara y demostrar entendimiento del flujo.\n",
    "\n",
    "**Entregable:** Código Python para los cálculos, las matrices Q, K, V y $QK^T$ resultantes. Un análisis escrito respondiendo a las preguntas.\n",
    "\n",
    "**Observación**\n",
    "\n",
    "En ambos apartados del ejercicio, los **scores de energía** (también llamados *alignment scores* o simplemente *scores*) son los valores **sin normalizar** que indican cuánta \"atención\" debería asignarse a cada par de elementos *antes* de aplicar la soft-max.\n",
    "\n",
    "| Contexto                                      | Fórmula del score                                           | ¿Qué mide?                                                                                      |\n",
    "| --------------------------------------------- | ----------------------------------------------------------- | ----------------------------------------------------------------------------------------------- |\n",
    "| **Atención aditiva de Bahdanau (pregunta 1)** | $e_{tj}=v_a^{\\top}\\tanh\\!\\bigl(W_a\\,s_{t-1}+U_a\\,h_j\\bigr)$ | Compatibilidad entre el estado del *decoder* en el paso $t-1$ y el estado $h_j$ del *encoder*.  |\n",
    "| **Auto-atención (pregunta 2)**                | $(QK^{\\top})_{ij}=q_i^{\\top}k_j$                            | Similitud (producto punto) entre la **query** de la palabra $i$ y la **key** de la palabra $j$. |\n",
    "\n",
    "* Los scores todavía **no son probabilidades**; pueden ser positivos o negativos y no están acotados.\n",
    "* Tras calcularlos, se aplica soft-max (y en Transformers el escalado $1/\\sqrt{d_k}$) para obtener los pesos de atención $\\alpha$ que sí suman 1 y se usan para promediar los vectores V.\n",
    "\n",
    "En la parte 2, la matriz $QK^{\\top}$ **es, precisamente, la matriz de scores de energía** de la auto-atención multiplicativa: cada entrada $(i,j)$ cuantifica cuánto debería influir la palabra $j$ en la representación actualizada de la palabra $i$ antes de la normalización.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f4a0f2-ee73-4959-a355-43c9c7c96d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
