{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Modelos de lenguaje causales con decodificador**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este tutorial, aprenderemos cómo construir y entrenar un modelo tipo GPT basado en decodificador, que es excelente para generar texto y otras tareas de procesamiento de lenguaje natural. Comenzaremos con lo básico, como preparar el entorno y preparar los datos dividiéndolos en tokens y convirtiendo esos tokens en números que el modelo pueda entender. Luego, profundizaremos en la construcción del modelo en sí, enfocándonos en cómo aprende a prestar atención a diferentes partes del texto para generar texto. A lo largo del camino, cubriremos cómo entrenar el modelo con datos. Finalmente, veremos cómo usar el modelo entrenado para crear texto basado en lo que ha aprendido.\n",
    "\n",
    "### **Modelos GPT**\n",
    "\n",
    "GPT (Generative Pretrained Transformer) es un modelo solo de decodificador porque se entrena utilizando un objetivo de modelado de lenguaje causal, donde el objetivo es predecir el siguiente token en una secuencia dados los tokens anteriores. Durante el entrenamiento, la secuencia de entrada se desplaza hacia la derecha, y el modelo aprende a generar tokens de salida de forma autoregresiva, uno a la vez. Este proceso permite que GPT genere texto coherente y contextualmente relevante basado en el mensaje de entrada dado. En este cuaderno aprenderemos cómo crear y entrenar un modelo tipo GPT solo de decodificador. Sin embargo, ten en cuenta que los modelos GPT reales son modelos más grandes y se entrenan con datos de entrenamiento masivos para tareas específicas de NLP.\n",
    "\n",
    "### **GPT vs. ChatGPT**\n",
    "\n",
    "GPT y ChatGPT son ambos modelos de IA desarrollados por OpenAI, pero cumplen diferentes propósitos y tienen funcionalidades distintas.\n",
    "\n",
    "GPT es una familia de modelos de lenguaje a gran escala basados en transformers entrenados con datos diversos de texto en internet. Los modelos GPT están diseñados para una amplia gama de tareas de procesamiento de lenguaje natural, como generación de texto, traducción, resumen y respuesta a preguntas. Generan respuestas basadas en el texto de entrada (prompt) pero no mantienen un historial de conversación consistente.\n",
    "\n",
    "Por otro lado, ChatGPT es una versión ajustada del modelo GPT, diseñada específicamente para aplicaciones de inteligencia artificial conversacional. Está entrenado para mantener un historial de conversación consistente y generar respuestas contextualmente relevantes, lo que lo hace más adecuado para interacciones tipo chatbot. ChatGPT sobresale en la comprensión y generación de diálogos similares a los humanos, proporcionando respuestas coherentes y atractivas en un entorno conversacional.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Configuraciones**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Instalando las librerías necesarias**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U torchdata==0.7.1\n",
    "#!pip install -Uqq portalocker>=2.0.0\n",
    "#!pip install -qq torchtext==0.17.1\n",
    "#!pip install -qq matplotlib\n",
    "#!pip install -qq transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando librerías requeridas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **torchdata**: Mejora las funcionalidades de carga y preprocesamiento de datos para PyTorch, optimizando el flujo de trabajo para los modelos de aprendizaje automático.\n",
    "* **portalocker**: Proporciona un mecanismo para bloquear archivos, asegurando que solo un proceso pueda acceder a un archivo a la vez, útil para gestionar recursos de archivos en aplicaciones concurrentes.\n",
    "* **torchtext**: Ofrece utilidades para el procesamiento de texto y conjuntos de datos en PyTorch, simplificando la preparación de datos para tareas de procesamiento de lenguaje natural (NLP).\n",
    "* **matplotlib**: Una librería de gráficos para crear visualizaciones estáticas, interactivas y animadas en Python, comúnmente utilizada para visualización de datos y creación de gráficos.\n",
    "\n",
    "Cada una de estas librerías se utiliza para manejar diferentes aspectos de la preparación de datos, procesamiento y entrenamiento de modelos para aplicaciones de aprendizaje automático y procesamiento de lenguaje natural, mejorando el flujo de trabajo general y las capacidades del proyecto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from typing import Iterable, List\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.datasets import IMDB,PennTreebank\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import time\n",
    "from torch.optim import Adam\n",
    "\n",
    "#Código para la supresión de warning en el código\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Pipeline de texto**\n",
    "\n",
    "#### Conjunto de datos\n",
    "\n",
    "El código carga el conjunto de datos IMDB en conjuntos de entrenamiento y validación. Luego crea un iterador para el conjunto de entrenamiento y recorre las primeras 10 muestras, imprimiendo cada una. Este proceso simula cómo uno podría iterar manualmente sobre un conjunto de datos sin usar el `DataLoader` de PyTorch para el procesamiento por lotes y la gestión de datos.\n",
    "\n",
    "Al entrenar modelos de lenguaje, generalmente se recomienda usar texto de dominio general. Sin embargo, en este caso, estamos utilizando el conjunto de datos IMDB, que es adecuado para tareas de clasificación. No obstante, usamos IMDB debido a su tamaño reducido y compatibilidad con máquinas que tienen memoria RAM limitada. Para tareas de modelado de lenguaje, algunos conjuntos de datos que puedes considerar incluyen: [PennTreebank](https://pytorch.org/text/0.8.1/datasets.html#penntreebank), [WikiText-2](https://pytorch.org/text/0.8.1/datasets.html#wikitext-2), [WikiText103](https://pytorch.org/text/0.8.1/datasets.html#wikitext103)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga el conjunto de datos\n",
    "train_iter, val_iter = IMDB()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializa un iterador para el cargador de datos de entrenamiento:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_itr = iter(train_iter)  # Inicializa un iterador para el conjunto de entrenamiento\n",
    "\n",
    "# Obtiene el tercer registro (avanzando uno por uno)\n",
    "next(data_itr)  # Primer registro\n",
    "next(data_itr)  # Segundo registro\n",
    "next(data_itr)  # Tercer registro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definamos el dispositivo (CPU o GPU) para el entrenamiento. Verificaremos si una GPU está disponible y la utilizaremos, de lo contrario usaremos la CPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Preprocesamiento de datos**\n",
    "\n",
    "El código proporcionado se utiliza para el preprocesamiento de datos de texto, particularmente para tareas de NLP, con un enfoque en la tokenización y la construcción de vocabulario.\n",
    "\n",
    "* **Índices y símbolos especiales**: Inicializa tokens especiales (`<unk>`, `<pad>` y una cadena vacía para EOS) con sus índices correspondientes (`0`, `1` y `2`). Estos tokens se usan para palabras desconocidas, relleno (padding) y final de oración, respectivamente.\n",
    "\n",
    "  * `UNK_IDX`: Índice para palabras desconocidas.\n",
    "  * `PAD_IDX`: Índice usado para rellenar oraciones más cortas en un lote, asegurando longitud uniforme.\n",
    "  * `EOS_IDX`: Índice que representa el final de una oración (aunque no se usa explícitamente aquí, ya que el símbolo EOS se define como una cadena vacía).\n",
    "\n",
    "* **Función `yield_tokens`**: Una función generadora que itera sobre un conjunto de datos (`data_iter`), tokeniza cada muestra de datos usando una función `tokenizer`, y produce una muestra tokenizada a la vez.\n",
    "\n",
    "* **Construcción del vocabulario**: Construye un vocabulario a partir del conjunto de datos tokenizado. La función `build_vocab_from_iterator` procesa los tokens generados por `yield_tokens`, incluye los tokens especiales (`special_symbols`) al inicio del vocabulario y establece una frecuencia mínima (`min_freq=1`) para que los tokens sean incluidos.\n",
    "\n",
    "* **Índice por defecto para tokens desconocidos**: Establece un índice por defecto para los tokens que no se encuentren en el vocabulario (`UNK_IDX`), asegurando que las palabras fuera de vocabulario se manejen como tokens desconocidos.\n",
    "\n",
    "* **Función `text_to_index`**: Convierte un texto dado en una secuencia de índices basada en el vocabulario construido. Esta función es esencial para transformar texto crudo en un formato numérico que pueda ser procesado por modelos de aprendizaje automático.\n",
    "\n",
    "* **Función `index_to_en`**: Transforma una secuencia de índices de vuelta a una cadena legible. Es útil para interpretar las salidas de los modelos y convertir predicciones numéricas nuevamente en texto.\n",
    "\n",
    "* **Verificación de la funcionalidad**: Demuestra el uso de `index_to_en` al convertir un tensor de índices `[0,1,2]` de regreso a sus símbolos especiales correspondientes. Esto ayuda a verificar que las funciones de vocabulario y conversión de índices están funcionando como se espera.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define los símbolos especiales y sus índices\n",
    "UNK_IDX, PAD_IDX, EOS_IDX = 0, 1, 2\n",
    "\n",
    "# Asegurarse de que los tokens estén en el orden de sus índices para insertarlos correctamente en el vocabulario\n",
    "special_symbols = ['<unk>', '<pad>', '<|endoftext|>']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "\n",
    "    for _,data_sample in data_iter:\n",
    "        yield  tokenizer(data_sample)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=special_symbols, special_first=True)\n",
    "vocab.set_default_index(UNK_IDX)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Nota: El bloque anterior debería completarse en menos de 20 segundos. Si tarda más que eso, se recomienda `reiniciar el kernel` y ejecutar las celdas posteriores a la celda que contiene los comandos `pip install` para asegurarse de que la función mencionada anteriormente opere como se espera.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Texto a índices y índice a texto**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_index=lambda text: [vocab(token) for token in tokenizer(text)]\n",
    "index_to_en = lambda seq_en: \" \".join([vocab.get_itos()[index] for index in seq_en])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verificación\n",
    "index_to_en(torch.tensor([0,1,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función de colación (Collate)\n",
    "\n",
    "En el contexto del modelo decodificador, buscamos crear una función de colación. Esta función toma un bloque de texto como entrada y produce un bloque de texto modificado como salida. La transformación del texto se logra mediante el uso de la función `get_sample(block_size, text)`. La función **get\\_sample** genera una muestra aleatoria de texto (src\\_sequence) y su secuencia subsiguiente (tgt\\_sequence) a partir de un texto dado para el entrenamiento de modelos de lenguaje. Se asegura de que la muestra se ajuste al tamaño de bloque especificado y se adapta si el texto es más corto que dicho bloque, devolviendo tanto la secuencia de entrada como la de salida para el modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(block_size, text):\n",
    "    # Determina la longitud del texto de entrada\n",
    "    sample_leg = len(text)\n",
    "\n",
    "    # Calcula el punto de parada para seleccionar aleatoriamente una muestra\n",
    "    # Esto asegura que la muestra seleccionada no exceda la longitud del texto\n",
    "    random_sample_stop = sample_leg - block_size\n",
    "\n",
    "    # Verifica si se puede tomar una muestra aleatoria (si el texto es más largo que block_size)\n",
    "    if random_sample_stop >= 1:\n",
    "        # Selecciona aleatoriamente un punto de inicio para la muestra\n",
    "        random_start = torch.randint(low=0, high=random_sample_stop, size=(1,)).item()\n",
    "        # Define el punto final de la muestra\n",
    "        stop = random_start + block_size\n",
    "\n",
    "        # Crea las secuencias de entrada y objetivo\n",
    "        src_sequence = text[random_start:stop]\n",
    "        tgt_sequence = text[random_start + 1:stop + 1]\n",
    "\n",
    "    # Maneja el caso en que la longitud del texto es igual o menor al tamaño del bloque\n",
    "    elif random_sample_stop <= 0:\n",
    "        # Comienza desde el inicio y usar todo el texto disponible\n",
    "        random_start = 0\n",
    "        stop = sample_leg\n",
    "        src_sequence = text[random_start:stop]\n",
    "        tgt_sequence = text[random_start + 1:stop]\n",
    "        # Añade una cadena vacía para mantener la alineación de secuencias\n",
    "        tgt_sequence.append('<|endoftext|>')\n",
    "\n",
    "    return src_sequence, tgt_sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probemos primero `get_sample(block_size, text)` y obtengamos un lote de textos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=1\n",
    "\n",
    "batch_of_tokens=[]\n",
    "\n",
    "for i in range(BATCH_SIZE):\n",
    "  _,text =next(iter(train_iter))\n",
    "  batch_of_tokens.append(tokenizer(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos la primera muestra de texto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=batch_of_tokens[0][0:100]\n",
    "text[0:100]\n",
    "batch_of_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para probar la función `get_sample` con un tamaño de bloque de 100, donde la salida incluye tanto la secuencia fuente como la secuencia objetivo, siendo la secuencia objetivo la secuencia fuente desplazada en un carácter, podemos usar el siguiente código como ejemplo:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size=10\n",
    "src_sequences, tgt_sequence=get_sample( block_size, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificamos si la secuencia está desplazada.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"src: \",src_sequences)\n",
    "print(\"tgt: \",tgt_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código crea lotes de secuencias fuente (`src_batch`) y objetivo (`tgt_batch`) a partir de un conjunto de datos para entrenar modelos de NLP. Recorre el conjunto de datos para extraer muestras de texto, genera las secuencias fuente y objetivo correspondientes usando la función `get_sample`, las convierte en índices del vocabulario y luego en tensores de PyTorch. Cada iteración agrega estas secuencias a sus respectivas listas de lotes y muestra sus detalles, incluyendo el texto, los índices y las dimensiones de los tensores, para dos muestras por lote.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa listas vacías para almacenar las secuencias fuente y objetivo\n",
    "src_batch, tgt_batch = [], []\n",
    "\n",
    "# Define el tamaño del lote\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "# Bucle para crear lotes de secuencias fuente y objetivo\n",
    "for i in range(BATCH_SIZE):\n",
    "    # Obtiene el siguiente dato del iterador de entrenamiento\n",
    "    _, text = next(iter(train_iter))\n",
    "\n",
    "    # Genera las secuencias fuente y objetivo usando la función get_sample\n",
    "    src_sequence_text, tgt_sequence_text = get_sample(block_size, tokenizer(text))\n",
    "\n",
    "    # Convierte las secuencias fuente y objetivo a índices del vocabulario tokenizado\n",
    "    src_sequence_indices = vocab(src_sequence_text)\n",
    "    tgt_sequence_indices = vocab(tgt_sequence_text)\n",
    "\n",
    "    # Convierte las secuencias a tensores de PyTorch con tipo de dato int64\n",
    "    src_sequence = torch.tensor(src_sequence_indices, dtype=torch.int64)\n",
    "    tgt_sequence = torch.tensor(tgt_sequence_indices, dtype=torch.int64)\n",
    "\n",
    "    # Agrega las secuencias fuente y objetivo a sus respectivos lotes\n",
    "    src_batch.append(src_sequence)\n",
    "    tgt_batch.append(tgt_sequence)\n",
    "\n",
    "    # Imprime la salida para cada muestra (en este caso, cada segunda muestra)\n",
    "    print(f\"Muestra {i}:\")\n",
    "    print(\"Secuencia fuente (texto):\", src_sequence_text)\n",
    "    print(\"Secuencia fuente (índices):\", src_sequence_indices)\n",
    "    print(\"Secuencia fuente (forma):\", src_sequence.shape)\n",
    "    print(\"Secuencia objetivo (texto):\", tgt_sequence_text)\n",
    "    print(\"Secuencia objetivo (índices):\", tgt_sequence_indices)\n",
    "    print(\"Secuencia objetivo (forma):\", tgt_sequence.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `collate_batch` prepara lotes de secuencias fuente y objetivo para el entrenamiento procesando cada muestra de texto en un lote dado. Genera las secuencias fuente y objetivo utilizando la función `get_sample` con un tamaño de bloque especificado, convierte estas secuencias en índices usando un vocabulario y las transforma en tensores de PyTorch. Luego, las secuencias se rellenan (padding) para asegurar una longitud uniforme en todo el lote. Finalmente, devuelve los lotes fuente y objetivo ya rellenados, listos para el entrenamiento en el dispositivo especificado (`DEVICE`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE=30\n",
    "def collate_batch(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for _,_textt in batch:\n",
    "      src_sequence,tgt_sequence=get_sample(BLOCK_SIZE,tokenizer(_textt))\n",
    "      src_sequence=vocab(src_sequence)\n",
    "      tgt_sequence=vocab(tgt_sequence)\n",
    "      src_sequence= torch.tensor(src_sequence, dtype=torch.int64)\n",
    "      tgt_sequence = torch.tensor(tgt_sequence, dtype=torch.int64)\n",
    "      src_batch.append(src_sequence)\n",
    "      tgt_batch.append(tgt_sequence)\n",
    "\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "\n",
    "    return src_batch.to(DEVICE), tgt_batch.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código configura cargadores de datos para los conjuntos de entrenamiento, validación y prueba utilizando la clase `DataLoader`, donde cada conjunto utiliza una función personalizada `collate_batch` para el procesamiento por lotes. Los cargadores de datos manejan lotes de tamaño 1 por simplicidad y mezclan aleatoriamente los datos para un acceso aleatorio. Después de inicializar el cargador de datos de entrenamiento, se obtiene el primer lote de secuencias fuente (`src`) y objetivo (`tgt`). Luego, itera sobre cada token en la secuencia fuente, los convierte nuevamente a texto usando la función `index_to_en` y muestra las oraciones resultantes, demostrando cómo acceder y visualizar los datos preprocesados que están listos para el entrenamiento del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=1\n",
    "dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "val_dataloader= DataLoader(val_iter , batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Iterando a través de muestra de datos**\n",
    "El código proporcionado itera a través de lotes de pares fuente-objetivo desde un cargador de datos. Demuestra cómo acceder y mostrar algunas muestras del conjunto de datos:\n",
    "\n",
    "* Se inicializa un iterador sobre el cargador de datos llamado `dataset`.\n",
    "* Un bucle se ejecuta durante 10 iteraciones para obtener y mostrar los primeros 10 pares fuente-objetivo. Para cada par:\n",
    "\n",
    "  * `src` y `trt` (abreviatura de target) contienen el lote de secuencias fuente y objetivo respectivamente.\n",
    "  * La función `index_to_en` se utiliza para convertir estas secuencias de índices numéricos a texto legible.\n",
    "  * Se imprime el número de `sample` y los textos fuente y objetivo correspondientes.\n",
    "\n",
    "Después de imprimir las primeras 10 muestras, el código continúa iterando a través del conjunto de datos:\n",
    "\n",
    "* Se imprime la forma (shape) de los tensores objetivo y fuente del siguiente lote, lo que proporciona información sobre el número de tokens y el tamaño del lote.\n",
    "* Nuevamente se utiliza la función `index_to_en` para convertir la primera secuencia del lote de índices a texto tanto para la fuente como para el objetivo.\n",
    "* Solo se imprime el primer par de los lotes restantes y luego el bucle se rompe.\n",
    "\n",
    "Este proceso es útil para verificar que el cargador de datos esté funcionando correctamente y que las secuencias se estén transformando adecuadamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=iter(dataloader)\n",
    "for sample in range(10):\n",
    "  src,trt=next(dataset)\n",
    "  print(\"Muestra\",sample)\n",
    "  print(\"Fuente:\",index_to_en(src))\n",
    "  print(\"\\n\")\n",
    "  print(\"Objetivo:\",index_to_en(trt))\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for  src,trt in dataset:\n",
    "    print(trt.shape)\n",
    "    print(src.shape)\n",
    "    print(index_to_en(src[0,:]))\n",
    "    print(index_to_en(trt[0,:]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asegúrate de que la secuencia fuente y la secuencia objetivo estén desplazadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fuente:\",index_to_en(src))\n",
    "print(\"Objetivo:\",index_to_en(trt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que hemos cubierto la preparación de los datos, pasemos a comprender los componentes clave del modelo Transformer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Enmascaramiento**\n",
    "\n",
    "En los transformers, el enmascaramiento es crucial para asegurar que ciertas posiciones no sean atendidas. La función `generate_square_subsequent_mask` produce una matriz triangular superior, lo que garantiza que durante la decodificación, un token no pueda atender a tokens futuros del objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz,device=DEVICE):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `create_mask`, por otro lado, genera máscaras para la secuencia fuente, basándose en la secuencia fuente proporcionada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(src,device=DEVICE):\n",
    "    src_seq_len = src.shape[0]\n",
    "    src_mask = generate_square_subsequent_mask(src_seq_len)\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask,src_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos un ejemplo de un tensor fuente y sus máscaras asociadas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplaza los primeros cuatro tokens con el token PAD para verificar cómo los tokens de relleno (padding) son enmascarados usando padding_mask\n",
    "src[0:4] = PAD_IDX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask,padding_mask = create_mask(src)\n",
    "src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Codificación posicional**\n",
    "\n",
    "El modelo Transformer no tiene conocimiento incorporado del orden de los tokens en la secuencia. Para proporcionarle esta información al modelo, se agregan codificaciones posicionales a los embeddings de los tokens. Estas codificaciones siguen un patrón fijo basado en su posición dentro de la secuencia.\n",
    "\n",
    "GPT utiliza codificaciones posicionales entrenables. A diferencia de las codificaciones posicionales fijas (como las codificaciones sinusoidales utilizadas en el artículo original de Transformer), las codificaciones posicionales entrenables se aprenden durante el proceso de entrenamiento del modelo.\n",
    "\n",
    "Las codificaciones posicionales entrenables se implementan como un conjunto de parámetros aprendibles, uno para cada posición en la secuencia de entrada. Estos parámetros tienen la misma dimensionalidad que los embeddings de los tokens. Durante el entrenamiento, el modelo actualiza los parámetros de codificación posicional junto con los demás parámetros del modelo para capturar la información posicional de manera más efectiva.\n",
    "\n",
    "El uso de codificaciones posicionales entrenables en GPT permite al modelo aprender representaciones posicionales más flexibles y específicas para la tarea, lo que potencialmente mejora su rendimiento en diversas tareas de procesamiento de lenguaje natural.\n",
    "\n",
    "En el contexto de este cuadernos, nos mantenemos con la codificación posicional fija por simplicidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agrega información posicional a los tokens de entrada\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Embeddings de tokens**\n",
    "\n",
    "Los embeddings de tokens, o embeddings de palabras o representación de palabras, es una forma de convertir palabras o tokens de un corpus de texto en vectores numéricos dentro de un espacio vectorial continuo. A cada palabra o token único en el corpus se le asigna un vector de longitud fija, donde los valores numéricos representan diversas propiedades lingüísticas de la palabra, como su significado, contexto o relaciones con otras palabras.\n",
    "\n",
    "La clase `TokenEmbedding` que se muestra a continuación convierte tokens numéricos en embeddings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Arquitectura personalizada GPT**\n",
    "\n",
    "La clase `CustomGPTModel` define una arquitectura de modelo basada en transformer para modelos generativos preentrenados. Este modelo tiene como objetivo generar texto y realizar diversas tareas de NLP. A continuación se presenta una explicación de los principales componentes de la clase:\n",
    "\n",
    "* **Initialización (`__init__`)**: El constructor toma varios parámetros incluyendo `embed_size`, `vocab_size`, `num_heads`, `num_layers`, `max_seq_len` y `dropout`. Inicializa la capa embedding, la codificación posicional, las capas codificadoras del transformer y una capa lineal (`lm_head`) para generar los logits sobre el vocabulario.\n",
    "\n",
    "* **Inicialización de pesos (`init_weights`)**: Este método inicializa los pesos del modelo para una mejor convergencia durante el entrenamiento. Se utiliza la inicialización uniforme de Xavier, que es una práctica común para inicializar pesos en aprendizaje profundo.\n",
    "\n",
    "* **Decoder (`decoder`)**: Aunque se llama `decoder`, este método actualmente funciona como el paso hacia adelante (forward pass) a través de las capas codificadoras del transformer, seguido por la generación de logits para la tarea de modelado del lenguaje. Se encarga de añadir las codificaciones posicionales a los embeddings y aplica una máscara si es necesario.\n",
    "\n",
    "* **Forward pass (`forward`)**: Este método es similar al método `decoder` y define el cálculo hacia adelante del modelo. Procesa la entrada a través de las capas de embedding, codificación posicional, capas codificadoras del transformer, y produce la salida final usando `lm_head`.\n",
    "\n",
    "* **Generación de máscaras**: Tanto los métodos `decoder` como `forward` contienen lógica para generar una máscara causal cuadrada si no se proporciona una máscara fuente. Esta máscara asegura que la predicción para una posición no dependa de los tokens futuros en la secuencia, lo cual es importante para la naturaleza autoregresiva de los modelos GPT.\n",
    "\n",
    "Una sección del código está comentada, lo que sugiere un diseño inicial en el que se consideró una capa decodificadora de transformer. Sin embargo, la implementación final utiliza solo capas codificadoras, lo cual es una simplificación común para modelos enfocados en modelado y generación de lenguaje.\n",
    "\n",
    "Esta clase encapsula eficazmente los componentes necesarios para crear un modelo tipo GPT, permitiendo su entrenamiento en tareas de modelado de lenguaje y aplicaciones de generación de texto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGPTModel(nn.Module):\n",
    "    def __init__(self, embed_size, vocab_size, num_heads, num_layers, max_seq_len=500, dropout=0.1):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Inicializa los pesos del modelo\n",
    "        self.init_weights()\n",
    "\n",
    "        # Capa de embeddings de tokens\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        # Codificación posicional para proporcionar información de orden en la secuencia\n",
    "        self.positional_encoding = PositionalEncoding(embed_size, dropout=dropout)\n",
    "\n",
    "        print(embed_size)\n",
    "\n",
    "        # Las capas restantes forman parte del codificador Transformer\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=embed_size, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        # Capa lineal final para proyectar a logits sobre el vocabulario\n",
    "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def init_weights(self):\n",
    "        # Inicialización de pesos con Xavier para mejor convergencia\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def create_mask(src, device=DEVICE):\n",
    "        # Crea una máscara causal y una máscara de padding para la entrada\n",
    "        src_seq_len = src.shape[0]\n",
    "        src_mask = nn.Transformer.generate_square_subsequent_mask(src_seq_len)\n",
    "        src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "        return src_mask, src_padding_mask\n",
    "\n",
    "    def decoder(self, x, src_mask):\n",
    "        seq_length = x.size(0)\n",
    "\n",
    "        # Agrega codificaciones posicionales a los embeddings de entrada\n",
    "        x = self.embed(x) * math.sqrt(self.embed_size)\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        # Si no se proporciona una máscara fuente, generar una máscara causal\n",
    "        if src_mask is None:\n",
    "            \"\"\"Genera una máscara causal cuadrada para la secuencia. Las posiciones enmascaradas se llenan con float('-inf').\n",
    "            Las posiciones no enmascaradas se llenan con float(0.0).\n",
    "            \"\"\"\n",
    "            src_mask, src_padding_mask = create_mask(x)\n",
    "\n",
    "        # Pasa por las capas del codificador Transformer\n",
    "        output = self.transformer_encoder(x, src_mask)\n",
    "\n",
    "        # Proyecta las salidas a logits del vocabulario\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "    def forward(self, x, src_mask=None, key_padding_mask=None):\n",
    "        seq_length = x.size(0)\n",
    "\n",
    "        # Agrega codificaciones posicionales a las embedding de entrada\n",
    "        x = self.embed(x) * math.sqrt(self.embed_size)  # src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        # Si no se proporciona una máscara fuente, generar una máscara causal\n",
    "        if src_mask is None:\n",
    "            \"\"\"Genera una máscara causal cuadrada para la secuencia. Las posiciones enmascaradas se llenan con float('-inf').\n",
    "            Las posiciones no enmascaradas se llenan con float(0.0).\n",
    "            \"\"\"\n",
    "            src_mask, src_padding_mask = create_mask(x)\n",
    "\n",
    "        # Pasa por el codificador Transformer\n",
    "        output = self.transformer_encoder(x, src_mask, key_padding_mask)\n",
    "\n",
    "        # Proyecta las salidas a logits del vocabulario\n",
    "        x = self.lm_head(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuración e inicialización del modelo\n",
    "\n",
    "Aquí configuramos e instanciamos un modelo GPT personalizado con las siguientes especificaciones:\n",
    "\n",
    "* `ntokens`: El número total de tokens únicos en el vocabulario, que el modelo utilizará para representar palabras.\n",
    "* `emsize`: El tamaño de cada vector de embeddings. En este modelo, cada palabra será representada por un vector de 200 dimensiones.\n",
    "* `nlayers`: El número de capas codificadoras del transformer en el modelo. Estamos utilizando dos capas en esta configuración.\n",
    "* `nhead`: El número de cabeceras de atención en el mecanismo de atención multi-cabecera. El modelo usará dos cabeceras de atención.\n",
    "* `dropout`: Una técnica de regularización en la que se ignoran aleatoriamente algunas neuronas durante el entrenamiento para prevenir el sobreajuste. Aquí, establecemos la probabilidad de dropout en 0.2.\n",
    "\n",
    "Después de definir estos hiperparámetros, creamos una instancia de `CustomGPTModel` pasando el tamaño de los embeddings, el número de cabeceras de atención, el número de capas, el tamaño del vocabulario y la probabilidad de dropout. Luego, el modelo se mueve al `DEVICE` especificado, que puede ser una CPU o una GPU, para su entrenamiento o inferencia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab)  # tamaño del vocabulario\n",
    "emsize = 200  # dimensión de los embeddings\n",
    "nlayers = 2  # número de capas ``nn.TransformerEncoderLayer`` en ``nn.TransformerEncoder``\n",
    "nhead = 2  # número de cabeceras en ``nn.MultiheadAttention``\n",
    "dropout = 0.2  # probabilidad de dropout\n",
    "\n",
    "# Crea el modelo personalizado GPT y moverlo al dispositivo (CPU o GPU)\n",
    "modelo = CustomGPTModel(embed_size=emsize, num_heads=nhead, num_layers=nlayers, vocab_size=ntokens, dropout=dropout).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Generación con prompt (Prompting)**\n",
    "Para que el modelo genere texto (el siguiente token), necesitas crear un punto de inicio, al que llamamos *prompt*, para que el modelo agregue tokens y genere texto a partir de él. \n",
    "\n",
    "Debes verificar que el *prompt* no sea `None` ni demasiado largo, luego procede a tokenizarlo, convertirlo en índices y reestructurarlo según sea necesario.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_prompt(prompt, block_size=BLOCK_SIZE):\n",
    "    # Maneja el caso en que el prompt sea None\n",
    "    while prompt is None:\n",
    "        prompt = input(\"Lo siento, el prompt no puede estar vacío. Por favor, ingresa un prompt válido: \")\n",
    "\n",
    "    # Tokeniza el prompt\n",
    "    tokens = tokenizer(prompt)\n",
    "    number_of_tokens = len(tokens)\n",
    "\n",
    "    # Maneja el caso de prompts muy largos\n",
    "    if number_of_tokens > block_size:\n",
    "        tokens = tokens[-block_size:]  # Conserva solo los últimos block_size tokens\n",
    "\n",
    "    # Convierte los tokens a índices del vocabulario\n",
    "    prompt_indices = vocab(tokens)\n",
    "\n",
    "    # Convierte los índices a un tensor de PyTorch y reestructurarlo\n",
    "    prompt_encoded = torch.tensor(prompt_indices, dtype=torch.int64).reshape(-1, 1)\n",
    "\n",
    "    return prompt_encoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos algunos ejemplos diferentes donde la entrada es `None` o más larga que el tamaño del bloque (`block size`):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(index_to_en(encode_prompt(None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(index_to_en(encode_prompt(\"Este es un prompt para que el modelo genere las siguientes palabras.\" ) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, codifiquemos un *prompt* de texto y ejecutémoslo a través de la parte decodificadora del modelo:\n",
    "\n",
    "* Se llama al método `decoder` de la instancia `modelo` de `CustomGPTModel` con el *prompt* codificado y sin una máscara de entrada (`src_mask=None`), lo que indica que no se enmascarará ninguna parte de la secuencia durante el procesamiento. El decodificador se encargará de crear una máscara causal internamente si es necesario.\n",
    "* La salida `logits` representa las predicciones en bruto del modelo para cada posición del token, las cuales pueden procesarse posteriormente (por ejemplo, aplicando una función *softmax*) para obtener las probabilidades del siguiente token en la secuencia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_encoded=encode_prompt(\"Este es un prompt para que el modelo genere las siguientes palabras.\").to(DEVICE)\n",
    "prompt_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = modelo.decoder(prompt_encoded,src_mask=None).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos 11 tokens por salida, una dimensión adicional de lote (*batch*), junto con los valores de *logits* correspondientes para cada palabra en el vocabulario.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reestructuramos de manera que la dimensión del lote (*batch*) sea cinco.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = logits.transpose(0, 1)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logits contiene los logits para cada token en la secuencia generada por el decodificador, solo necesitamos el último para la siguiente palabra.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_preiction =logits[:,-1]\n",
    "logit_preiction.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtiene el índice de la siguiente palabra.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " _, next_word_index = torch.max(logit_preiction, dim=1)\n",
    " next_word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Próxima palabra\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_en(next_word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Generación autoregresiva de texto**\n",
    "\n",
    "En los modelos de decodificador, simplemente se añade la salida a la entrada para generar la siguiente respuesta. Este proceso se detiene cuando se encuentra la etiqueta de fin de secuencia `<|endoftext|>` o si la entrada se vuelve demasiado grande. Más adelante en este notebook, implementaremos esto como una función.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"este es el inicio de\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asegurate de que el *prompt* tenga el tamaño máximo de entrada y realizar una predicción.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_encoded = encode_prompt(prompt).to(DEVICE)\n",
    "print(\"Dispositivo para prompt_encoded:\", prompt_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(max_new_tokens):\n",
    "    # Obtiene los logits del modelo a partir del prompt codificado\n",
    "    logits = modelo.decoder(prompt_encoded, src_mask=None)\n",
    "    logits = logits.transpose(0, 1)\n",
    "\n",
    "    print(\" \")\n",
    "    print(f\"Forma (shape) de los logits en el paso {i}: {logits.shape}\")\n",
    "\n",
    "    # Obtiene los logits de la última posición (último token)\n",
    "    logit_preiction = logits[:, -1]\n",
    "    print(f\"Forma de logit_prediction en el paso {i}: {logit_preiction.shape}\")\n",
    "\n",
    "    # Obtiene el índice del token con mayor probabilidad (token siguiente)\n",
    "    next_token_encoded = torch.argmax(logit_preiction, dim=-1).reshape(-1, 1)\n",
    "    print(f\"Forma de next_token_encoded en el paso {i}: {next_token_encoded.shape}\")\n",
    "\n",
    "    # Añade el nuevo token predicho al final de la secuencia\n",
    "    prompt_encoded = torch.cat((prompt_encoded, next_token_encoded), dim=0).to(DEVICE)\n",
    "    print(f\"Secuencia en el paso {i}: {[index_to_en(j) for j in prompt_encoded]}\")\n",
    "    print(f\"Forma de prompt_encoded después de la concatenación en el paso {i}: {prompt_encoded.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a implementarlo como una función.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define los símbolos especiales y sus índices\n",
    "UNK_IDX, PAD_IDX, EOS_IDX = 0, 1, 2\n",
    "\n",
    "# Asegurate de que los tokens estén en el orden de sus índices para insertarlos correctamente en el vocabulario\n",
    "special_symbols = ['<unk>', '<pad>', '<|endoftext|>']\n",
    "\n",
    "BLOCK_SIZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generación de texto autoregresiva con un modelo de lenguaje\n",
    "def generate(modelo, prompt=None, max_new_tokens=500, block_size=BLOCK_SIZE, vocab=vocab, tokenizer=tokenizer):\n",
    "    # Mueve el modelo al dispositivo especificado (por ejemplo, GPU o CPU)\n",
    "    modelo.to(DEVICE)\n",
    "\n",
    "    # Codifica el prompt de entrada usando la función encode_prompt\n",
    "    prompt_encoded = encode_prompt(prompt).to(DEVICE)\n",
    "    tokens = []\n",
    "\n",
    "    # Genera nuevos tokens hasta alcanzar el máximo especificado\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Decodifica el prompt codificado utilizando el decodificador del modelo\n",
    "        logits = modelo(prompt_encoded, src_mask=None, key_padding_mask=None)\n",
    "\n",
    "        # Transpone los logits para poner la longitud de la secuencia como la primera dimensión\n",
    "        logits = logits.transpose(0, 1)\n",
    "\n",
    "        # Selecciona los logits del último token de la secuencia\n",
    "        logit_prediction = logits[:, -1]\n",
    "\n",
    "        # Escoge el token más probable a partir de los logits (decodificación codiciosa)\n",
    "        next_token_encoded = torch.argmax(logit_prediction, dim=-1).reshape(-1, 1)\n",
    "\n",
    "        # Si el siguiente token es el token de fin de secuencia (EOS), detener la generación\n",
    "        if next_token_encoded.item() == EOS_IDX:\n",
    "            break\n",
    "\n",
    "        # Añade el siguiente token al prompt codificado y conservar solo los últimos 'block_size' tokens\n",
    "        prompt_encoded = torch.cat((prompt_encoded, next_token_encoded), dim=0)[-block_size:]\n",
    "\n",
    "        # Convierte el índice del siguiente token en una cadena usando el vocabulario\n",
    "        # Mueve el tensor de nuevo a la CPU para la búsqueda en el vocabulario si es necesario\n",
    "        token_id = next_token_encoded.to('cpu').item()\n",
    "        tokens.append(vocab.get_itos()[token_id])\n",
    "\n",
    "    # Une los tokens generados en una sola cadena y devolverla\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(modelo,prompt=\"este es el inicio de \",max_new_tokens=30,vocab=vocab,tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Decodificando las diferencias: Entrenamiento vs. inferencia**\n",
    "\n",
    "La diferencia clave entre las etapas de entrenamiento e inferencia radica en las entradas al decodificador. Durante el entrenamiento, el decodificador se beneficia de tener acceso a los *ground truth*, recibiendo los tokens exactos de la secuencia objetivo de forma incremental a través de una técnica conocida como **\"teacher forcing\"**. Este enfoque contrasta notablemente con otras arquitecturas de redes neuronales que dependen de las predicciones previas de la red como entradas durante el entrenamiento. Una vez concluido el entrenamiento, los conjuntos de datos utilizados se asemejan a los empleados en modelos de redes neuronales más convencionales, proporcionando una base familiar para la comparación y evaluación.\n",
    "\n",
    "Para iniciar el entrenamiento, primero se debe crear un objeto de pérdida de entropía cruzada (*Cross Entropy Loss*). La función de pérdida no tomará en cuenta los tokens de relleno (*PAD*).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "loss_fn = CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos las máscaras requeridas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src,tgt=next(iter(dataloader))\n",
    "\n",
    "mask,padding_mask = create_mask(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando llamas a `modelo(src, src_mask, key_padding_mask)`, el método `forward` de la clase `CustomGPTModel` genera los *logits* para la secuencia objetivo, los cuales luego pueden traducirse en tokens reales tomando la predicción con la mayor probabilidad en cada paso de la secuencia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = modelo(src,src_mask=mask,key_padding_mask=padding_mask)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"forma de salida (output shape)\", logits.shape)\n",
    "print(\"forma de la secuencia fuente (source shape)\", src)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durante el entrenamiento, al decodificador del transformer se le proporciona toda la secuencia objetivo de una sola vez. Esto permite el procesamiento paralelo de la secuencia, a diferencia de la generación de un token a la vez. En consecuencia, la secuencia de salida se produce en su totalidad, coincidiendo con la forma (shape) de la secuencia objetivo de entrada. Esta generación paralela es eficiente y aprovecha la capacidad del modelo para manejar secuencias de manera integral. Al examinar las dimensiones de la salida, podemos confirmar que se alinean con la secuencia objetivo de entrada, lo que indica que toda la secuencia ha sido procesada simultáneamente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos la primera muestra de la secuencia objetivo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt\n",
    "print(tgt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logits.reshape(-1, logits.shape[-1]).shape)\n",
    "print(tgt.reshape(-1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora calculamos la pérdida, ya que la salida del decodificador del transformer se proporciona como entrada a la función de pérdida de entropía cruzada junto con los valores de la secuencia objetivo. Dado que la salida del transformer tiene las dimensiones de longitud de secuencia, tamaño del lote (*batch size*) y características (*features*), es necesario reorganizar (*reshape*) esta salida para que coincida con el formato estándar requerido por la función de pérdida de entropía cruzada. Este paso asegura que la pérdida se calcule correctamente, comparando la secuencia predicha con las *ground truth* en cada paso temporal a lo largo del lote, utilizando el método `reshape`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt.reshape(-1))\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siguiendo los procedimientos mencionados anteriormente, podemos desarrollar una función capaz de realizar predicciones y, posteriormente, calcular la pérdida correspondiente sobre los datos de validación. Utilizaremos esta función más adelante.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(modelo: nn.Module, eval_data) -> float:\n",
    "    modelo.eval()  \n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for src,tgt in eval_data:\n",
    "            tgt = tgt.to(DEVICE)\n",
    "            #seq_len = src.size(0)\n",
    "            logits = modelo(src,src_mask=None,key_padding_mask=None)\n",
    "            total_loss +=  loss_fn(logits.reshape(-1, logits.shape[-1]), tgt.reshape(-1)).item()\n",
    "    return total_loss / (len(list(eval_data)) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(modelo,val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento del modelo\n",
    "\n",
    "Incorporando los pasos descritos anteriormente, procedemos a entrenar el modelo. Aparte de estos procedimientos específicos, el proceso general de entrenamiento sigue los métodos convencionales empleados en el entrenamiento de redes neuronales.\n",
    "\n",
    "**Ten en cuenta que entrenar el modelo utilizando CPUs puede ser un proceso que consume mucho tiempo. Si no tienes acceso a GPUs, puedes saltar y continuar cargando el modelo preentrenado usando el código proporcionado. Hemos entrenado el modelo durante 30 épocas y lo hemos guardado para tu conveniencia.**\n",
    "\n",
    "La función `train` está definida para ajustar finamente el `CustomGPTModel` sobre un conjunto de datos de entrenamiento dado. Está estructurada de la siguiente manera:\n",
    "\n",
    "* **Optimizador**: Se inicializa un optimizador ADAM.\n",
    "\n",
    "Dentro de la función `train`:\n",
    "\n",
    "* El modelo se establece en modo entrenamiento, lo que habilita las capas de *dropout* y de *batch normalization*.\n",
    "* Un bucle itera sobre los datos de entrenamiento, que se cargan en lotes. Para cada lote:\n",
    "\n",
    "  * Se extraen las secuencias fuente (`src`) y objetivo (`tgt`).\n",
    "  * El modelo realiza una pasada hacia adelante (*forward pass*) para obtener los *logits*.\n",
    "  * Los *logits* se reestructuran para el cálculo de la pérdida.\n",
    "  * La pérdida se calcula utilizando `loss_fn`, que probablemente se refiere a una función de pérdida como la entropía cruzada, que mide la diferencia entre los *logits* predichos y las secuencias objetivo.\n",
    "* Se aplica recorte de gradientes (*gradient clipping*) para prevenir gradientes explosivos, lo cual es común en el entrenamiento de redes neuronales profundas.\n",
    "* El optimizador actualiza los parámetros del modelo basándose en los gradientes calculados.\n",
    "\n",
    "El registro (*logging*) ocurre cada `10000` pasos, o al alcanzar un lote específico (el lote `42060` está codificado como ejemplo). Durante el registro:\n",
    "\n",
    "* Se calculan e imprimen la pérdida promedio y la *perplejidad* (una medida de qué tan bien el modelo de probabilidad predice una muestra), lo que proporciona información sobre el rendimiento del modelo.\n",
    "* Se mide e informa el tiempo transcurrido por lote desde el último intervalo de registro, lo que da una indicación de la eficiencia del entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(modelo.parameters(), lr=1e-2, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10000, gamma=0.9)\n",
    "\n",
    "def train(modelo: nn.Module,train_data) -> None:\n",
    "    modelo.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 10000\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(list(train_data)) // block_size\n",
    "    for batch,srctgt in enumerate(train_data):\n",
    "        src= srctgt[0]\n",
    "        tgt= srctgt[1]\n",
    "        logits = modelo(src,src_mask=None)\n",
    "        logits_flat = logits.reshape(-1, logits.shape[-1])\n",
    "        loss = loss_fn(logits_flat, tgt.reshape(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(modelo.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (batch % log_interval == 0 and batch > 0) or batch==42060:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            #cur_loss = total_loss / log_interval\n",
    "            cur_loss = total_loss / batch\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoca {epoch:3d} | {batch//block_size:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.4f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            start_time = time.time()\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos listas de pérdidas para hacer un seguimiento de la pérdida de entrenamiento y validación.\n",
    "\n",
    "El modelo recorrerá los datos de entrenamiento 30 veces (épocas). Este paso de entrenamiento utiliza funciones que hemos definido anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 30\n",
    "Train_losses= []\n",
    "Val_losses = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_loss = train(modelo,dataloader)\n",
    "    val_loss = evaluate(modelo, val_dataloader)\n",
    "    val_ppl = math.exp(val_loss)\n",
    "    Train_losses.append(train_loss)\n",
    "    Val_losses.append(val_loss)\n",
    "\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print('-' * 89)\n",
    "    print(f'| Fin de epoca {epoch:3d} | tiempo: {elapsed:5.2f}s | '\n",
    "        f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(modelo.state_dict(), 'model_best_val_loss.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a graficar las pérdidas de entrenamiento y validación:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula el número de épocas (suponiendo que las longitudes de Train_losses y Val_losses sean iguales)\n",
    "num_epochs = len(Train_losses)\n",
    "\n",
    "# Crea una figura y un conjunto de subgráficas\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Grafica las pérdidas de entrenamiento\n",
    "ax.plot(range(num_epochs), Train_losses, label='Pérdida de entrenamiento', color='blue')\n",
    "\n",
    "# Grafica las pérdidas de validación\n",
    "ax.plot(range(num_epochs), Val_losses, label='Pérdida de validación', color='orange')\n",
    "\n",
    "# Establece la etiqueta del eje x\n",
    "ax.set_xlabel('Época')\n",
    "\n",
    "# Establece la etiqueta del eje y\n",
    "ax.set_ylabel('Pérdida')\n",
    "\n",
    "# Establece el título del gráfico\n",
    "ax.set_title('Pérdidas de entrenamiento y validación')\n",
    "\n",
    "# Añade una leyenda al gráfico\n",
    "ax.legend()\n",
    "\n",
    "# Muestra el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargando el modelo guardado\n",
    "\n",
    "Si deseas omitir el entrenamiento y cargar un modelo entrenado que hemos proporcionado, adelante, descomenta la siguiente celda:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/kyn1_OsXrzjef0xihlsXmg.pt'\n",
    "#modelo.load_state_dict(torch.load('kyn1_OsXrzjef0xihlsXmg.pt',map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(modelo,prompt=\"the movie was\",max_new_tokens=10,vocab=vocab,tokenizer=tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes ver que el resultado no es satisfactorio, lo cual se debe al hecho de que los LLMs necesitan ser entrenados con grandes volúmenes de datos durante varias épocas para ser precisos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cargando el modelo GPT2 desde HuggingFace**\n",
    "\n",
    "Ahora carguemos el modelo GPT2 desde HuggingFace para comprobar cómo se desempeña en la generación de texto:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga el tokenizador y el modelo\n",
    "tokenizer1 = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "modelo = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Define el texto de entrada (prompt)\n",
    "#input_text = \"Once upon a time in a faraway land,\"\n",
    "input_text = \"the movie was\"\n",
    "\n",
    "# Tokeniza el texto de entrada y prepararlo para el modelo\n",
    "input_ids = tokenizer1.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Genera texto usando el modelo\n",
    "# Establece la longitud deseada del texto generado (max_length),\n",
    "# y otros parámetros de generación como temperature, top_k y top_p\n",
    "max_length = 15\n",
    "temperature = 0.7\n",
    "top_k = 50\n",
    "top_p = 0.95\n",
    "\n",
    "generated_ids = modelo.generate(\n",
    "    input_ids,\n",
    "    max_length=max_length,\n",
    "    temperature=temperature,\n",
    "    top_k=top_k,\n",
    "    top_p=top_p,\n",
    "    pad_token_id=tokenizer1.eos_token_id,\n",
    ")\n",
    "\n",
    "# Decodifica el texto generado\n",
    "generated_text = tokenizer1.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Imprimir el texto de entrada y el texto generado\n",
    "print(f\"Entrada: {input_text}\")\n",
    "print(f\"Texto generado: {generated_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Preguntas**\n",
    "\n",
    "\n",
    "1. ¿Qué papel juegan `UNK_IDX`, `PAD_IDX` y `EOS_IDX` en el vocabulario y por qué es importante definirlos antes de construirlo?\n",
    "2. ¿Cómo funciona la función `yield_tokens` y por qué usamos un generador para construir el vocabulario?\n",
    "3. ¿Qué efecto tiene `special_first=True` al llamar a `build_vocab_from_iterator`?\n",
    "4. Explica paso a paso qué hace `text_to_index` y cómo se relaciona con `index_to_en`.\n",
    "5. En `get_sample`, ¿cómo se determina `random_sample_stop` y qué casos cubre cada rama del `if`?\n",
    "6. ¿Por qué en el caso de `random_sample_stop <= 0` se añade explícitamente `'<|endoftext|>'` al final de `tgt_sequence`?\n",
    "7. ¿Qué resultado obtenemos al concatenar varias cadenas de tokens con `pad_sequence` en el `collate_batch`?\n",
    "8. Explica la diferencia entre usar `batch_first=False` y `batch_first=True` en `pad_sequence`.\n",
    "9. ¿Cómo genera `generate_square_subsequent_mask` una máscara causal y por qué la transposición de índices es necesaria?\n",
    "10. ¿Qué información codifica `padding_mask` y cómo mejora la atención en el Transformer?\n",
    "11. En la clase `PositionalEncoding`, ¿por qué se usan funciones seno y coseno con frecuencias crecientes?\n",
    "12. ¿Qué ventaja aporta multiplicar el embedding por `sqrt(self.emb_size)` en `TokenEmbedding`?\n",
    "13. ¿Cuál es la función de `init_weights` y por qué se prefiere la inicialización de Xavier para pesos de más de una dimensión?\n",
    "14. Analiza el método `decoder` de `CustomGPTModel`: ¿qué pasos realiza desde la entrada hasta la proyección final en `lm_head`?\n",
    "15. ¿Por qué el método `forward` de `CustomGPTModel` requiere tanto `src_mask` como `key_padding_mask`?\n",
    "16. En la función `encode_prompt`, ¿cómo se manejan los prompts más largos que `block_size` y por qué?\n",
    "17. Describe cómo funciona la generación autoregresiva en el bucle de `generate`, especialmente la lógica de truncar a los últimos `block_size` tokens.\n",
    "18. ¿Cómo se computa la pérdida con `CrossEntropyLoss(ignore_index=PAD_IDX)` y qué papel juega el parámetro `ignore_index`?\n",
    "19. En la rutina de entrenamiento (`train`), ¿por qué se llama a `clip_grad_norm_` y qué problema evita?\n",
    "20. ¿Qué métrica de evaluación representa `ppl` (perplejidad) y cómo se calcula a partir de la pérdida `cur_loss`?\n",
    "21. ¿Cómo se implementa el scheduler de tasa de aprendizaje (`StepLR`) y qué efecto tiene `gamma=0.9` cada 10000 pasos?\n",
    "22. Al graficar las listas `Train_losses` y `Val_losses`, ¿qué información visual obtenemos y cómo la interpretarías para decidir si el modelo está sobreajustando?\n",
    "23. Compara brevemente la generación de texto con el modelo GPT2 preentrenado (último bloque) frente al modelo `CustomGPTModel`: ¿en qué difieren los enfoques de tokenización y generación?\n",
    "24. Identifica al menos dos puntos en los que podrías modularizar o refactorizar este código para mejorar su mantenibilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "prev_pub_hash": "e1b85295509065c068b2e670cf2ca79050c48a451b3cf21a0f6a0bb73a97d7f6"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
