{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Adaptadores en PyTorch**\n",
    "\n",
    "En PyTorch, afinar un modelo preentrenado puede hacerse modificando únicamente la capa final o ajustando todas las capas: el primero suele resultar en un rendimiento limitado; el segundo, en un gran consumo de recursos. \n",
    "\n",
    "Los **adaptadores** (adapters) ofrecen una alternativa más ligera: se insertan pequeños módulos entrenables dentro de la arquitectura original y solo éstos se actualizan durante el fine-tuning, manteniendo intactos los pesos preentrenados. De este modo, se ahorra tiempo y memoria, se reduce el riesgo de sobreajuste y se facilita la reutilización del mismo backbone para múltiples tareas. No obstante, al trabajar con adaptadores, es posible que la precisión final quede algo por debajo de la de un ajuste completo, especialmente en tareas que requieren modificaciones profundas del modelo. \n",
    "\n",
    "En el cuaderno práctico que sigue, aplicaremos un adaptador a un transformer entrenado sobre AG News para transferirlo al conjunto IMDB y compararemos su desempeño frente a un fine-tuning parcial (solo capa final) y uno completo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuración"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instalar las librerías necesarias\n",
    "\n",
    "Para este cuaderno necesitaremos las siguientes librerías, que **no** vienen preinstaladas.**Debes ejecutar la siguiente celda** para instalarlas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade portalocker==2.8.2 torchtext==0.17.0 torchdata==0.7.1 pandas==2.2.2 matplotlib==3.9.0 scikit-learn==1.5.0 torch==2.2.0 numpy==1.26.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importar las librerías requeridas\n",
    "\n",
    "El siguiente bloque importa todas las librerías necesarias:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import accumulate\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "torch.set_num_threads(1)\n",
    "from torch import nn\n",
    "import os\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator, GloVe, Vectors\n",
    "from torchtext.datasets import IMDB\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "import pickle\n",
    "\n",
    "from urllib.request import urlopen\n",
    "import io\n",
    "\n",
    "import tarfile\n",
    "import tempfile\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# También puedes usar esta sección para suprimir las advertencias generadas por tu código:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Funciones auxiliares**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(COST,ACC):\n",
    "    \"\"\"\n",
    "    Grafica la evolución de la pérdida total y la precisión por época.\n",
    "\n",
    "    Parámetros:\n",
    "        COST (list): Lista con el valor de la pérdida total en cada época.\n",
    "        ACC (list): Lista con la precisión en cada época.\n",
    "    \"\"\"\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    color = 'tab:red'\n",
    "    ax1.plot(COST, color=color)\n",
    "    ax1.set_xlabel('epoca', color=color)\n",
    "    ax1.set_ylabel('Perdida total', color=color)\n",
    "    ax1.tick_params(axis='y', color=color)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('accuracy', color=color)  \n",
    "    ax2.plot(ACC, color=color)\n",
    "    ax2.tick_params(axis='y', color=color)\n",
    "    fig.tight_layout() \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list_to_file(lst, filename):\n",
    "    \"\"\"\n",
    "    Guarda una lista en un archivo usando serialización con pickle.\n",
    "\n",
    "    Parámetros:\n",
    "        lst (list): La lista que se guardará.\n",
    "        filename (str): El nombre del archivo donde se guardará la lista.\n",
    "\n",
    "    Retorna:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(lst, file)\n",
    "\n",
    "def load_list_from_file(filename):\n",
    "    \"\"\"\n",
    "    Carga una lista desde un archivo usando deserialización con pickle.\n",
    "\n",
    "    Parámetros:\n",
    "        filename (str): El nombre del archivo desde el cual se cargará la lista.\n",
    "\n",
    "    Retorna:\n",
    "        list: La lista cargada.\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as file:\n",
    "        loaded_list = pickle.load(file)\n",
    "    return loaded_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Codificaciones posicionales**\n",
    "\n",
    "Las codificaciones posicionales desempeñan un papel fundamental en los transformers y en diversos modelos de secuencia a secuencia, ya que ayudan a transmitir información crítica sobre las posiciones o el orden de los elementos dentro de una secuencia.\n",
    "\n",
    "A pesar de sus significados distintos, cabe destacar que los *embeddings* de estas oraciones permanecen idénticos en ausencia de codificaciones posicionales. La siguiente clase define las codificaciones posicionales heredando de la clase `Module` de PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, vocab_size=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(vocab_size, d_model)\n",
    "        position = torch.arange(0, vocab_size, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float()\n",
    "            * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Importar el conjunto de datos IMDB**\n",
    "\n",
    "El siguiente código carga el conjunto de datos IMDB:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/35t-FeC-2uN1ozOwPs7wFg.gz')\n",
    "tar = tarfile.open(fileobj=io.BytesIO(urlopened.read()))\n",
    "tempdir = tempfile.TemporaryDirectory()\n",
    "tar.extractall(tempdir.name)\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Descripción general del conjunto de datos IMDB**\n",
    "\n",
    "El **conjunto de datos IMDB** contiene reseñas de películas extraídas de la Internet Movie Database (IMDB) y se utiliza habitualmente para tareas de clasificación de sentimientos binarios. Es un conjunto de datos de referencia para entrenar y evaluar modelos de procesamiento de lenguaje natural (NLP), en particular en análisis de sentimientos.\n",
    "\n",
    "**Composición del conjunto de datos**\n",
    "\n",
    "* **Reseñas**: El conjunto de datos consta de 50 000 reseñas de películas, divididas equitativamente en 25 000 muestras de entrenamiento y 25 000 de prueba.\n",
    "* **Etiquetas de sentimiento**: Cada reseña está etiquetada como positiva o negativa, indicando el sentimiento expresado. El conjunto está balanceado, con igual número de reseñas positivas y negativas en ambos subconjuntos.\n",
    "* **Contenido de texto**: Las reseñas se presentan en texto plano y han sido preprocesadas hasta cierto punto (por ejemplo, se han eliminado etiquetas HTML), aunque conservan puntuación y mayúsculas originales.\n",
    "* **Uso**: Se emplea comúnmente para entrenar modelos de clasificación binaria de sentimientos, donde el objetivo es predecir si una reseña dada es positiva o negativa en función de su contenido textual.\n",
    "\n",
    "**Aplicaciones**\n",
    "\n",
    "* **Análisis de sentimientos**: Principal uso del conjunto IMDB como benchmark para distintos algoritmos de clasificación de texto.\n",
    "* **Procesamiento de lenguaje natural**: Ampliamente utilizado en investigación y aplicaciones de NLP para comprobar la eficacia de modelos y enfoques en la comprensión del lenguaje humano.\n",
    "\n",
    "#### **Desafíos**\n",
    "\n",
    "* **Tamaño del conjunto**: Al ser relativamente pequeño, resulta difícil entrenar un modelo eficaz desde cero sin recurrir a técnicas de transferencia o regularización.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, root_dir, train=True):\n",
    "        \"\"\"\n",
    "        root_dir: El directorio base del conjunto de datos IMDB.\n",
    "        train: Indicado booleano que indica si se debe usar datos de entrenamiento o de prueba.\n",
    "        \"\"\"\n",
    "        self.root_dir = os.path.join(root_dir, \"train\" if train else \"test\")\n",
    "        self.neg_files = [os.path.join(self.root_dir, \"neg\", f) for f in os.listdir(os.path.join(self.root_dir, \"neg\")) if f.endswith('.txt')]\n",
    "        self.pos_files = [os.path.join(self.root_dir, \"pos\", f) for f in os.listdir(os.path.join(self.root_dir, \"pos\")) if f.endswith('.txt')]\n",
    "        self.files = self.neg_files + self.pos_files\n",
    "        self.labels = [0] * len(self.neg_files) + [1] * len(self.pos_files)\n",
    "        self.pos_inx=len(self.pos_files)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.files[idx]\n",
    "        label = self.labels[idx]\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        return label, content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código utiliza la clase `IMDBDataset` definida anteriormente para crear iteradores de los conjuntos de datos de entrenamiento y prueba. Luego, muestra 20 ejemplos del conjunto de entrenamiento:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = tempdir.name + '/' + 'imdb_dataset'\n",
    "train_iter = IMDBDataset(root_dir=root_dir, train=True)  # Para datos de entrenamiento\n",
    "test_iter = IMDBDataset(root_dir=root_dir, train=False)  # Para datos de prueba\n",
    "\n",
    "start=train_iter.pos_inx\n",
    "for i in range(-10,10):\n",
    "    print(train_iter[start+i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente fragmento define el mapeo de etiquetas numéricas a reseñas negativas y positivas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_label = {0: \" negative review\", 1: \"positive review\"}\n",
    "imdb_label[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, este código verifica que en el conjunto de entrenamiento existan exactamente dos clases:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "num_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código carga un tokenizador básico en inglés y define una función llamada `yield_tokens` que utiliza el tokenizador para descomponer datos de texto proporcionados por un iterador en tokens:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    \"\"\"Devuelve tokens para cada muestra de datos.\"\"\"\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código carga un modelo de embeddings de palabras preentrenado llamado GloVe en una variable llamada `glove_embedding`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nota: los embeddings de GloVe normalmente se descargan usando:\n",
    "# glove_embedding = GloVe(name=\"6B\", dim=100)\n",
    "# Sin embargo, el servidor de GloVe frecuentemente está inactivo.\n",
    "# El código siguiente ofrece una solución alternativa.\n",
    "\n",
    "class GloVe_override(Vectors):\n",
    "    url = {\n",
    "        \"6B\": \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/tQdezXocAJMBMPfUJx_iUg/glove-6B.zip\",\n",
    "    }\n",
    "\n",
    "    def __init__(self, name=\"6B\", dim=100, **kwargs) -> None:\n",
    "        url = self.url[name]\n",
    "        name = \"glove.{}.{}d.txt\".format(name, str(dim))\n",
    "        #name = \"glove.{}/glove.{}.{}d.txt\".format(name, name, str(dim))\n",
    "        super(GloVe_override, self).__init__(name, url=url, **kwargs)\n",
    "\n",
    "class GloVe_override2(Vectors):\n",
    "    url = {\n",
    "        \"6B\": \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/tQdezXocAJMBMPfUJx_iUg/glove-6B.zip\",\n",
    "    }\n",
    "\n",
    "    def __init__(self, name=\"6B\", dim=100, **kwargs) -> None:\n",
    "        url = self.url[name]\n",
    "        #name = \"glove.{}.{}d.txt\".format(name, str(dim))\n",
    "        name = \"glove.{}/glove.{}.{}d.txt\".format(name, name, str(dim))\n",
    "        super(GloVe_override2, self).__init__(name, url=url, **kwargs)\n",
    "\n",
    "try:\n",
    "    glove_embedding = GloVe_override(name=\"6B\", dim=100)\n",
    "except:\n",
    "    try:\n",
    "        glove_embedding = GloVe_override2(name=\"6B\", dim=100)\n",
    "    except:\n",
    "        glove_embedding = GloVe(name=\"6B\", dim=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código construye un objeto vocabulario a partir de un modelo de embeddings de palabras GloVe preentrenado y establece el índice predeterminado en el token `<unk>`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe,vocab\n",
    "# Construye el vocabulario a partir de glove_embedding.stoi\n",
    "vocab = vocab(glove_embedding .stoi, 0,specials=('<unk>', '<pad>'))\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contemos el número de palabras en el vocabulario:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probemos la función `vocab`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab(['he'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **División del conjunto de datos**\n",
    "\n",
    "Se toman los iteradores de entrenamiento y de prueba y se convierten en conjuntos de datos del tipo \"map\".  A continuación, se aplica una partición aleatoria sobre el conjunto de entrenamiento original, reservando el 95 % de las muestras para entrenar y el 5 % restante para validar. \n",
    "\n",
    "De este modo obtenemos dos subconjuntos, uno de entrenamiento y otro de validación que se usan para ajustar y supervisar el modelo de clasificación de texto en IMDB. Finalmente, el rendimiento definitivo se mide sobre el conjunto de prueba independiente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convierte los iteradores de entrenamiento y prueba en conjuntos de datos de estilo map.\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "# Determina el número de muestras para entrenamiento y validación (5 % para validación).\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "\n",
    "# Divide aleatoriamente el conjunto de entrenamiento en entrenamiento y validación.\n",
    "# El conjunto de entrenamiento tendrá el 95 % de las muestras y el de validación el 5 % restante.\n",
    "split_train_, split_valid_ = random_split(\n",
    "    train_dataset,\n",
    "    [num_train, len(train_dataset) - num_train]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para simular el proceso como si se tuviera GPU, reducimos aún más el tamaño del conjunto de entrenamiento. Si deseas usar el conjunto IMDB completo, comenta o elimina las dos líneas siguientes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = int(len(train_dataset) * 0.05)\n",
    "split_train_, _ = random_split(split_train_, [num_train, len(split_train_) - num_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código verifica si hay una GPU compatible con CUDA disponible usando PyTorch. Si existe, asigna `device = \"cuda\"`, de lo contrario `device = \"cpu\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cargador de datos**\n",
    "\n",
    "El siguiente código prepara el pipeline de procesamiento de texto con el tokenizador y el vocabulario. \n",
    "\n",
    "La función `text_pipeline` primero tokeniza el texto de entrada y luego aplica `vocab` para obtener los índices de los tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_pipeline(x):\n",
    "    return vocab(tokenizer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En PyTorch, la función **`collate_fn`** se utiliza junto con los data loaders para personalizar la forma en que se crean los lotes a partir de muestras individuales. El código proporcionado define una función `collate_batch` en PyTorch, que se emplea con los data loaders para ajustar la creación de lotes a partir de muestras individuales. Esta función procesa un lote de datos, incluyendo etiquetas y secuencias de texto. \n",
    "\n",
    "Aplica la función `text_pipeline` para preprocesar el texto. Los datos resultantes se convierten en tensores de PyTorch y se devuelven como una tupla que contiene el tensor de etiquetas, el tensor de texto y un tensor de offsets que representa las posiciones iniciales de cada secuencia de texto dentro del tensor combinado.\n",
    "\n",
    "Además, la función se asegura de que los tensores generados se muevan al dispositivo especificado (por ejemplo, GPU) para un cálculo más eficiente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for _label, _text in batch:\n",
    "\n",
    "        label_list.append(_label)\n",
    "        text_list.append(torch.tensor(text_pipeline(_text), dtype=torch.int64))\n",
    "\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = pad_sequence(text_list, batch_first=True)\n",
    "\n",
    "    return label_list.to(device), text_list.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes convertir estos conjuntos de datos en data loaders aplicando `collate_fn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Comprobemos qué generan estos data loaders:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label,seqence=next(iter(valid_dataloader))\n",
    "label,seqence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Red neuronal**\n",
    "\n",
    "Este código define una clase llamada `Net` que representa un clasificador de texto basado en un `TransformerEncoder` de PyTorch.\n",
    "\n",
    "El constructor recibe los siguientes argumentos:\n",
    "\n",
    "* `num_class`: Número de clases a clasificar.\n",
    "* `vocab_size`: Tamaño del vocabulario.\n",
    "* `freeze`: Indica si se debe congelar la capa de embedding.\n",
    "* `nhead`: Número de cabezas en el codificador Transformer.\n",
    "* `dim_feedforward`: Dimensión de la capa feedforward en el codificador Transformer.\n",
    "* `num_layers`: Número de capas del codificador Transformer.\n",
    "* `dropout`: Tasa de dropout.\n",
    "* `activation`: Función de activación que se usará en el codificador Transformer.\n",
    "* `classifier_dropout`: Tasa de dropout para el clasificador.\n",
    "\n",
    "**Atributos:**\n",
    "\n",
    "* `emb`: Capa de embedding que mapea cada palabra del vocabulario a un vector denso.\n",
    "* `pos_encoder`: Capa de codificación posicional que añade información de posición a los vectores de palabra.\n",
    "* `transformer_encoder`: Codificador Transformer que procesa la secuencia de vectores de palabra y extrae características de alto nivel.\n",
    "* `classifier`: Capa lineal que mapea la salida del codificador Transformer al número deseado de clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Clasificador de texto basado en un TransformerEncoder de PyTorch.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "\n",
    "        self,\n",
    "        num_class,vocab_size,\n",
    "        freeze=True,\n",
    "        nhead=2,\n",
    "        dim_feedforward=128,\n",
    "        num_layers=2,\n",
    "        dropout=0.1,\n",
    "        activation=\"relu\",\n",
    "        classifier_dropout=0.1):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        #self.emb = embedding=nn.Embedding.from_pretrained(glove_embedding.vectors,freeze=freeze)\n",
    "        self.emb = nn.Embedding.from_pretrained(glove_embedding.vectors,freeze=freeze)\n",
    "        embedding_dim = self.emb.embedding_dim\n",
    "\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(\n",
    "            d_model=embedding_dim,\n",
    "            dropout=dropout,\n",
    "            vocab_size=vocab_size,\n",
    "        )\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.classifier = nn.Linear(embedding_dim, num_class)\n",
    "        self.d_model = embedding_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo puede entrenarse luego con datos etiquetados del conjunto IMDB, que tiene dos clases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "modelo = Net(num_class=2,vocab_size=vocab_size).to(device)\n",
    "modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente función **`predict`** recibe un texto, un pipeline de procesamiento de texto y el modelo, y devuelve la etiqueta predicha usando el modelo preentrenado para clasificación en IMDB:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, text_pipeline, modelo):\n",
    "    with torch.no_grad():\n",
    "        text = torch.unsqueeze(torch.tensor(text_pipeline(text)),0).to(device)\n",
    "        modelo.to(device)\n",
    "        output = modelo(text)\n",
    "        return imdb_label[output.argmax(1).item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"I like sports and stuff\", text_pipeline, modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evaluar la precisión del modelo en un conjunto de datos, definimos dos funciones casi idénticas. Una muestra una barra de progreso con `tqdm` y la otra no:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model_eval):\n",
    "    model_eval.eval()\n",
    "    total_acc, total_count= 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for label, text in tqdm(dataloader):\n",
    "            label, text = label.to(device), text.to(device)\n",
    "            output = model_eval(text)\n",
    "            predicted = torch.max(output.data, 1)[1]\n",
    "            total_acc += (predicted == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código evalúa el rendimiento del modelo (puede tardar unos 4 minutos en CPU). **Para mayor eficiencia, no ejecutaremos esta celda ahora**, pero puedes descomentar la celda si se desea comprobar que el modelo sin entrenar no rinde mejor que el azar:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_no_tqdm(dataloader, model_eval):\n",
    "    model_eval.eval()\n",
    "    total_acc, total_count= 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for label, text in dataloader:\n",
    "            label, text = label.to(device), text.to(device)\n",
    "            output = model_eval(text)\n",
    "            predicted = torch.max(output.data, 1)[1]\n",
    "            total_acc += (predicted == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate(test_dataloader, modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten en cuenta que el rendimiento actual del modelo no es mejor que el promedio. Este resultado es esperado, considerando que el modelo aún no ha recibido ningún entrenamiento.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Entrenamiento**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código define la función de entrenamiento que se utiliza para entrenar el modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(modelo, optimizer, criterion, train_dataloader, valid_dataloader,  epochs=1000, save_dir=\"\", file_name=None):\n",
    "    cum_loss_list = []\n",
    "    acc_epoch = []\n",
    "    acc_old = 0\n",
    "    model_path = os.path.join(save_dir, file_name)\n",
    "    acc_dir = os.path.join(save_dir, os.path.splitext(file_name)[0] + \"_acc\")\n",
    "    loss_dir = os.path.join(save_dir, os.path.splitext(file_name)[0] + \"_loss\")\n",
    "    time_start = time.time()\n",
    "\n",
    "    for epoch in tqdm(range(1, epochs + 1)):\n",
    "        modelo.train()\n",
    "        #print(modelo)\n",
    "        #for parm in modelo.parameters():\n",
    "        #    print(parm.requires_grad)\n",
    "        \n",
    "        cum_loss = 0\n",
    "        for idx, (label, text) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            label, text = label.to(device), text.to(device)\n",
    "\n",
    "            predicted_label = modelo(text)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            loss.backward()\n",
    "            #print(loss)\n",
    "            torch.nn.utils.clip_grad_norm_(modelo.parameters(), 0.1)\n",
    "            optimizer.step()\n",
    "            cum_loss += loss.item()\n",
    "        print(f\"Epoca {epoch}/{epochs} - Pérdida: {cum_loss}\")\n",
    "\n",
    "        cum_loss_list.append(cum_loss)\n",
    "        accu_val = evaluate_no_tqdm(valid_dataloader,modelo)\n",
    "        acc_epoch.append(accu_val)\n",
    "\n",
    "        if model_path and accu_val > acc_old:\n",
    "            print(accu_val)\n",
    "            acc_old = accu_val\n",
    "            if save_dir is not None:\n",
    "                pass\n",
    "                #print(\"save model epoch\",epoch)\n",
    "                #torch.save(modelo.state_dict(), model_path)\n",
    "                #save_list_to_file(lst=acc_epoch, filename=acc_dir)\n",
    "                #save_list_to_file(lst=cum_loss_list, filename=loss_dir)\n",
    "\n",
    "    time_end = time.time()\n",
    "    print(f\"Tiempo de entrenamiento: {time_end - time_start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Entrenamiento en IMDB**\n",
    "\n",
    "El siguiente código establece la tasa de aprendizaje (LR) en 1, que determina el tamaño del paso con el que el optimizador actualiza los parámetros del modelo durante el entrenamiento. El criterio `CrossEntropyLoss` se utiliza para calcular la pérdida entre las salidas predichas por el modelo y las etiquetas reales. Esta función de pérdida se emplea comúnmente en tareas de clasificación multiclase.\n",
    "\n",
    "El optimizador elegido es `Stochastic Gradient Descent` (SGD), que ajusta los parámetros del modelo en función de los gradientes calculados respecto a la función de pérdida. El optimizador SGD utiliza la tasa de aprendizaje especificada para controlar el tamaño de las actualizaciones de los pesos.\n",
    "\n",
    "Además, se define un programador (scheduler) de tasa de aprendizaje mediante `StepLR`. Este programador ajusta la tasa de aprendizaje durante el entrenamiento, reduciéndola en un factor (`gamma`) de 0.1 después de cada época (paso) para mejorar la convergencia y afinar el rendimiento del modelo. Estos componentes forman, en conjunto, la configuración esencial para entrenar una red neuronal usando la tasa de aprendizaje, el criterio de pérdida, el optimizador y el programador de tasa de aprendizaje especificados.\n",
    "\n",
    "Por motivos de eficiencia de tiempo, **las siguientes líneas están comentadas y el modelo no se entrena realmente**. Si deseas echar un vistazo de cómo sería el proceso de entrenamiento, descomenta el siguiente bloque de código para entrenar el modelo durante 2 épocas. Si entrenaras este modelo en un escenario real, lo más probable es que aumentarías el número de épocas a una cifra mayor, como 100 o más. Dado el conjunto de entrenamiento reducido definido anteriormente, tarda aproximadamente 2 minutos completar 2 épocas de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "LR=1\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "save_dir = \"\"\n",
    "file_name = \"model_IMDB dataset small2.pth\"\n",
    "train_model(model=modelo, \n",
    "            optimizer=optimizer, \n",
    "            criterion=criterion, \n",
    "            train_dataloader=train_dataloader, \n",
    "            valid_dataloader=valid_dataloader, \n",
    "            epochs=2, \n",
    "            save_dir=save_dir, \n",
    "            file_name=file_name\n",
    "           )\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carguemos un modelo que ha sido preentrenado usando el mismo método pero con el conjunto de datos completo y durante 100 épocas.\n",
    "\n",
    "El siguiente código traza el costo y la precisión de los datos de validación para cada época del modelo preentrenado hasta e incluyendo aquella época que obtuvo la mayor precisión. \n",
    "\n",
    "> Comprueba que el modelo preentrenado alcanzó una precisión de más del 85% en el conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/sybqacL5p1qeEO8d4xRZNg/model-IMDB%20dataset%20small2-acc')\n",
    "loss_urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/eOt6woGoaOB565T0RLH5WA/model-IMDB%20dataset%20small2-loss')\n",
    "acc_epoch = pickle.load(acc_urlopened)\n",
    "cum_loss_list = pickle.load(loss_urlopened)\n",
    "plot(cum_loss_list,acc_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código carga tu modelo preentrenado y evalúa su rendimiento en el conjunto de prueba. \n",
    "\n",
    "**Para mayor eficiencia, no ejecutaremos la evaluación porque puede tardar aproximadamente 4 minutos. En su lugar, muestra el resultado debajo de la celda. Si deseas confirmar el resultado por ti mismo, eres libre de descomentar la última línea en el siguiente bloque de código.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/q66IH6a7lglkZ4haM6hB1w/model-IMDB%20dataset%20small2.pth')\n",
    "model_ = Net(vocab_size=vocab_size, num_class=2).to(device)\n",
    "model_.load_state_dict(torch.load(io.BytesIO(urlopened.read()), map_location=device))\n",
    "#evaluate(test_dataloader, model_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Comprueba que el modelo preentrenado alcanzó una exactitud de aproximadamente el 83% en los datos de prueba.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ajuste fino de un modelo preentrenado en el conjunto de datos AG News**\n",
    "\n",
    "En lugar de entrenar un modelo en el conjunto de datos IMDB como hicimos anteriormente, puedes ajustar finamente (fine-tune) un modelo que ha sido preentrenado en el conjunto de datos AG News, que es una colección de artículos de noticias.  El objetivo del conjunto AG News es categorizar los artículos en una de cuatro categorías: deportes, negocios, ciencia/tecnología o mundo. \n",
    "\n",
    "Comenzaremos entrenando un modelo desde cero en el conjunto AG News. Para ahorrar tiempo, puedes hacerlo en una sola celda. \n",
    "\n",
    "Además, por eficiencia, **comenta la parte de entrenamiento**. Si deseas entrenar el modelo durante 2 épocas en un conjunto de datos reducido para demostrar cómo sería el proceso de entrenamiento, descomenta la sección que dice `### Descomenta para entrenar ###` antes de ejecutar la celda. \n",
    "\n",
    "Entrenar durante 2 épocas en el conjunto reducido puede llevar aproximadamente 3 minutos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga los datos de entrenamiento de AG News\n",
    "train_iter_ag_news = AG_NEWS(split=\"train\")\n",
    "\n",
    "# Calcula el número de clases distintas en AG News\n",
    "num_class_ag_news = len(set([label for (label, text) in train_iter_ag_news ]))\n",
    "num_class_ag_news\n",
    "\n",
    "# Divide el conjunto de datos en iteradores de entrenamiento y prueba\n",
    "train_iter_ag_news, test_iter_ag_news = AG_NEWS()\n",
    "\n",
    "# Convierte los iteradores de entrenamiento y prueba en datasets de tipo mapa\n",
    "train_dataset_ag_news = to_map_style_dataset(train_iter_ag_news)\n",
    "test_dataset_ag_news = to_map_style_dataset(test_iter_ag_news)\n",
    "\n",
    "# Determina el número de muestras para entrenamiento y validación (5% para validación)\n",
    "num_train_ag_news = int(len(train_dataset_ag_news) * 0.95)\n",
    "\n",
    "# Divide aleatoriamente el dataset de entrenamiento en entrenamiento y validación\n",
    "# El 95% de las muestras irán a entrenamiento y el 5% restante a validación\n",
    "split_train_ag_news_, split_valid_ag_news_ = random_split(\n",
    "    train_dataset_ag_news,\n",
    "    [num_train_ag_news, len(train_dataset_ag_news) - num_train_ag_news]\n",
    ")\n",
    "\n",
    "# Reduce el conjunto de entrenamiento para que se ejecute rápidamente como ejemplo\n",
    "# SI DESEAS ENTRENAR EN EL DATASET AG_NEWS, COMENTA LAS 2 LÍNEAS DE ABAJO.\n",
    "# SIN EMBARGO, TEN EN CUENTA QUE EL ENTRENAMIENTO TOMARÁ MUCHO TIEMPO\n",
    "num_train_ag_news = int(len(train_dataset_ag_news) * 0.05)\n",
    "split_train_ag_news_, _ = random_split(\n",
    "    split_train_ag_news_,\n",
    "    [num_train_ag_news, len(split_train_ag_news_) - num_train_ag_news]\n",
    ")\n",
    "\n",
    "# Define el dispositivo: usa GPU si está disponible, de lo contrario CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "# Función para convertir la etiqueta (1–4) a un índice en 0–3\n",
    "def label_pipeline(x):\n",
    "    return int(x) - 1\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Función collate para procesar un lote: convierte etiquetas y secuencias a tensores y aplica padding\n",
    "def collate_batch_ag_news(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for _label, _text in batch:\n",
    "        # Convierte y almacena cada etiqueta\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        # Tokeniza y almacena cada texto como tensor de enteros\n",
    "        text_list.append(torch.tensor(text_pipeline(_text), dtype=torch.int64))\n",
    "\n",
    "    # Crea tensor de etiquetas\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    # Aplica padding para igualar la longitud de las secuencias\n",
    "    text_list = pad_sequence(text_list, batch_first=True)\n",
    "\n",
    "    # Devuelve los tensores en el dispositivo correspondiente\n",
    "    return label_list.to(device), text_list.to(device)\n",
    "\n",
    "# Tamaño de lote\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Crea DataLoaders para entrenamiento, validación y prueba\n",
    "train_dataloader_ag_news = DataLoader(\n",
    "    split_train_ag_news_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch_ag_news\n",
    ")\n",
    "valid_dataloader_ag_news = DataLoader(\n",
    "    split_valid_ag_news_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch_ag_news\n",
    ")\n",
    "test_dataloader_ag_news = DataLoader(\n",
    "    test_dataset_ag_news, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch_ag_news\n",
    ")\n",
    "\n",
    "# Crea la instancia del modelo y muévela al dispositivo\n",
    "model_ag_news = Net(num_class=4, vocab_size=vocab_size).to(device)\n",
    "model_ag_news.to(device)\n",
    "\n",
    "'''\n",
    "### Descomenta para entrenar ###\n",
    "LR = 1\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_ag_news.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "save_dir = \"\"\n",
    "file_name = \"model_AG News small1.pth\"\n",
    "train_model(\n",
    "    model=model_ag_news,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    train_dataloader=train_dataloader_ag_news,\n",
    "    valid_dataloader=valid_dataloader_ag_news,\n",
    "    epochs=2,\n",
    "    save_dir=save_dir,\n",
    "    file_name=file_name\n",
    ")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carguemos un modelo que ha sido preentrenado usando el mismo método pero con el conjunto completo de AG News durante 100 épocas.\n",
    "\n",
    "El siguiente código traza el costo y la precisión de los datos de validación para cada época del modelo preentrenado hasta e incluyendo la época que obtuvo la mayor precisión. \n",
    "\n",
    "> Puedes verificar que el modelo preentrenado alcanzó una precisión muy alta de más del 90% en el conjunto de validación de AG News.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/bQk8mJu3Uct3I4JEsEtRnw/model-AG%20News%20small1-acc')\n",
    "loss_urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/KNQkqJWWwY_XfbFBRFhZNA/model-AG%20News%20small1-loss')\n",
    "acc_epoch = pickle.load(acc_urlopened)\n",
    "cum_loss_list = pickle.load(loss_urlopened)\n",
    "plot(cum_loss_list,acc_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código carga el modelo preentrenado y evalúa su rendimiento en el conjunto de prueba de AG News. \n",
    "\n",
    "**Por eficiencia, no ejecutemos la evaluación porque puede tardar unos minutos. En su lugar, indica que el modelo preentrenado funciona bien en el conjunto de AG News. Si deseas confirmar el resultado, siéntete libre de descomentar la última línea en el siguiente bloque de código.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/9c3Dh2O_jsYBShBuchUNlg/model-AG%20News%20small1.pth')\n",
    "model_ag_news_ = Net(vocab_size=vocab_size, num_class=4).to(device)\n",
    "model_ag_news_.load_state_dict(torch.load(io.BytesIO(urlopened.read()), map_location=device))\n",
    "#evaluate(test_dataloader_ag_news, model_ag_news_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ¿El modelo preentrenado funcionó extremadamente bien en el conjunto de datos AG News?, ¿se puede ajustar este modelo para que también funcione bien en el conjunto IMDB?.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/9c3Dh2O_jsYBShBuchUNlg/model-AG%20News%20small1.pth')\n",
    "model_fine1 = Net(vocab_size=vocab_size, num_class=4).to(device)\n",
    "model_fine1.load_state_dict(torch.load(io.BytesIO(urlopened.read()), map_location=device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El conjunto de datos IMDB es una tarea de clasificación binaria con solo dos clases (reseñas positivas y negativas). Por lo tanto, la capa de salida del modelo AG News debe ajustarse para tener solo dos neuronas de salida, a fin de reflejar la naturaleza binaria del conjunto IMDB. \n",
    "\n",
    "Este ajuste es esencial para que el modelo aprenda y prediga con precisión el sentimiento de las reseñas de películas en el conjunto de datos IMDB.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fine1.classifier\n",
    "in_features = model_fine1.classifier.in_features\n",
    "print(\"Capa final original:\", model_fine1.classifier)\n",
    "print(\"Dimensión de entrada de la capa final:\", in_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reemplaza la capa final para resolver un problema de dos clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fine1.classifier = nn.Linear(in_features, 2)\n",
    "model_fine1.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código muestra las capas que están congeladas (`requires_grad == False`) y descongeladas (`requires_grad == True`) en el modelo. \n",
    "\n",
    "Las capas descongeladas tendrán sus pesos actualizados durante el ajuste fino.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model_fine1.named_parameters():\n",
    "    print(f\"{name} requires_grad: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente bloque de código simula el ajuste fino en el conjunto de entrenamiento reducido durante solo 2 épocas. \n",
    "\n",
    "**Por motivos de eficiencia de tiempo, este bloque de código está comentado**. Si quieres ver cómo sería el entrenamiento, descomenta el siguiente bloque de código, pero recuerda que este proceso podría tardar aproximadamente 2 minutos en ejecutarse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "LR=1\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_fine1.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "save_dir = \"\"\n",
    "file_name = \"model_fine1.pth\"\n",
    "train_model(model=model_fine1, optimizer=optimizer, criterion=criterion, train_dataloader=train_dataloader, valid_dataloader=valid_dataloader,  epochs=2,  save_dir=save_dir ,file_name=file_name )\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código muestra el progreso del ajuste fino completo de todo el conjunto de entrenamiento de IMDB durante 100 épocas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/3LEJw8BRgJJFGqlLxaETxA/model-fine1-acc')\n",
    "loss_urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/-CT1h97vjv0TolY82Nw29g/model-fine1-loss')\n",
    "acc_epoch = pickle.load(acc_urlopened)\n",
    "cum_loss_list = pickle.load(loss_urlopened)\n",
    "plot(cum_loss_list,acc_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente línea carga un modelo preajustado que fue entrenado durante 100 épocas con el conjunto completo de IMDB y evalúa su rendimiento en el conjunto de prueba de IMDB. \n",
    "\n",
    "**Por motivos de eficiencia, no ejecutemos la evaluación ya que puede tardar varios minutos. En su lugar, muestra el resultado debajo de la celda. Si deseas comprobar el resultado por ti mismo, siéntete libre de descomentar la última línea del bloque de código.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/e0WOHKh5dnrbC2lGhpsMMw/model-fine1.pth')\n",
    "model_fine1_ = Net(vocab_size=vocab_size, num_class=2).to(device)\n",
    "model_fine1_.load_state_dict(torch.load(io.BytesIO(urlopened.read()), map_location=device))\n",
    "#evaluate(test_dataloader, model_fine1_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este modelo debe demostrar una mejora notable, logrando un rendimiento excepcional con una exactitud del 86 % en los datos de prueba. Esto es superior al 83 % conseguido por el modelo entrenado desde cero con el conjunto de datos IMDB. Aunque el proceso de entrenamiento fue intensivo en tiempo (el ajuste fino fue tan costoso en tiempo como entrenar el modelo desde cero), el rendimiento mejorado subraya la eficacia y superioridad del modelo ajustado frente al modelo entrenado desde cero. \n",
    "\n",
    "Gran parte del esfuerzo computacional se dedicó a actualizar las capas del transformer. Para acelerar el proceso de entrenamiento, una estrategia viable es centrarse únicamente en entrenar la capa final, lo que puede reducir significativamente la carga computacional, aunque podría comprometer la precisión del modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ajuste fino solo de la capa final**\n",
    "\n",
    "El ajuste fino de la capa de salida final de una red neuronal es similar al ajuste fino de todo el modelo. Puedes comenzar cargando el modelo preentrenado que deseas afinar. \n",
    "\n",
    "En este caso, se trata del mismo modelo preentrenado en el conjunto de datos AG News.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/9c3Dh2O_jsYBShBuchUNlg/model-AG%20News%20small1.pth')\n",
    "model_fine2 = Net(vocab_size=vocab_size, num_class=4).to(device)\n",
    "model_fine2.load_state_dict(torch.load(io.BytesIO(urlopened.read()), map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, la diferencia clave. Iteras a través de todos los parámetros del modelo `model_fine2` y estableces el atributo `requires_grad` de cada parámetro en `False`. Esto congela efectivamente todas las capas del modelo, lo que significa que sus pesos serán actualizados durante el entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Congela todas las capas del modelo\n",
    "for param in model_fine2.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reemplaza la capa final para reflejar que estás resolviendo un problema de dos clases. Observa que la nueva capa no estará congelada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=model_fine2.classifier.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fine2.classifier = nn.Linear(dim, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fine2.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente bloque simula el fine-tuning en el conjunto de entrenamiento reducido durante solo 2 épocas. **Para ahorrar tiempo, este bloque de código ha sido comentado**. \n",
    "\n",
    "El código debería tardar menos en entrenar que el fine-tuning completo realizado anteriormente, ya que solo la capa final no está congelada.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "LR=1\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_fine2.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "save_dir = \"\"\n",
    "file_name = \"model_fine2.pth\"\n",
    "train_model(model=model_fine2, optimizer=optimizer, criterion=criterion, train_dataloader=train_dataloader, valid_dataloader=valid_dataloader,  epochs=2,  save_dir=save_dir ,file_name=file_name )\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez más, no usarás el modelo que acabas de ajustar, sino que inspeccionarás el proceso de fine-tuning de la capa final de un modelo ajustado en el conjunto completo de entrenamiento de IMDB durante 100 épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/UdR3ApQnxSeV2mrA0CbiLg/model-fine2-acc')\n",
    "loss_urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/rWGDIF-uL2dEngWcIo9teQ/model-fine2-loss')\n",
    "acc_epoch = pickle.load(acc_urlopened)\n",
    "cum_loss_list = pickle.load(loss_urlopened)\n",
    "plot(cum_loss_list,acc_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente línea carga el modelo preentrenado y evalúa su rendimiento en el conjunto de prueba. \n",
    "\n",
    "**Para mayor eficiencia, no ejecutaremos la evaluación porque puede tardar algunos minutos. En su lugar, informa el resultado debajo de la celda. Si deseas confirmar el resultado por ti mismo, siéntete libre de descomentar la última línea del siguiente bloque de código.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/B-1H6lpDg-A0zRwpB6Ek2g/model-fine2.pth')\n",
    "model_fine2_ = Net(vocab_size=vocab_size, num_class=2).to(device)\n",
    "model_fine2_.load_state_dict(torch.load(io.BytesIO(urlopened.read()), map_location=device))\n",
    "#evaluate(test_dataloader, model_fine2_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código anterior indica que, aunque el fine-tuning de la capa final tarda significativamente menos tiempo que el fine-tuning de todo el modelo, el rendimiento del modelo con solo la última capa sin congelar es mucho peor (64 % de exactitud) que el modelo ajustado con todas las capas sin congelar (86 % de exactitud).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Adaptadores**\n",
    "\n",
    "**FeatureAdapter** es un módulo de red neuronal que introduce un cuello de botella de baja dimensión en una arquitectura Transformer para permitir fine-tuning con menos parámetros. Comprime los embeddings de alta dimensión originales a una dimensión menor, aplica una transformación no lineal y luego las expande de nuevo a la dimensión original. \n",
    "\n",
    "Este proceso va seguido de una conexión residual que añade la salida transformada a la entrada original para preservar la información y promover el flujo de gradiente.\n",
    "\n",
    "**Beneficios de usar adaptadores en redes neuronales**\n",
    "\n",
    "* **Fine-tuning eficiente**: Los adaptadores permiten actualizaciones focalizadas en partes específicas del modelo, reduciendo la necesidad de reentrenar grandes secciones de la red.\n",
    "* **Eficiencia de parámetros**: Al agregar solo unos pocos parámetros, los adaptadores hacen factible modificar modelos grandes sin una carga computacional sustancial.\n",
    "* **Preservación de las características preentrenadas**: Los adaptadores permiten la modificación de un modelo mientras se retienen las valiosas características aprendidas durante el preentrenamiento extenso.\n",
    "* **Modularidad y flexibilidad**: Mejoran la modularidad de los modelos, permitiendo una fácil adaptación a diversas tareas sin alterar la arquitectura central.\n",
    "* **Adaptación específica para la tarea**: Los adaptadores pueden diseñarse para mejorar el rendimiento en tareas particulares, optimizando la efectividad del modelo.\n",
    "* **Transferencia de aprendizaje y adaptación de dominio**: Facilitan la adaptación de modelos a nuevos dominios, cerrando brechas entre diferentes distribuciones de datos.\n",
    "* **Aprendizaje continuo**: Los adaptadores apoyan la capacidad del modelo para aprender nueva información continuamente sin olvidar conocimientos previos.\n",
    "* **Reducción del riesgo de sobreajuste**: Con menos parámetros entrenables, los adaptadores ayudan a prevenir el sobreajuste, especialmente en conjuntos de datos reducidos.\n",
    "\n",
    "El siguiente código muestra un modelo de adaptador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureAdapter(nn.Module):\n",
    "    \"\"\"\n",
    "    Atributos:\n",
    "        size (int): La dimensión del bottleneck a la cual se reducen temporalmente los embeddings.\n",
    "        model_dim (int): La dimensión original de los embeddings o características en el modelo Transformer.\n",
    "    \"\"\"\n",
    "    def __init__(self, bottleneck_size=50, model_dim=100):\n",
    "        super().__init__()\n",
    "        self.bottleneck_transform = nn.Sequential(\n",
    "            nn.Linear(model_dim, bottleneck_size),  # Proyectar a una dimensión más pequeña\n",
    "            nn.ReLU(),                              # Aplicar no linealidad\n",
    "            nn.Linear(bottleneck_size, model_dim)   # Proyectar de nuevo a la dimensión original\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Paso hacia adelante (forward) de FeatureAdapter. Aplica la transformación del bottleneck al tensor\n",
    "        de entrada y añade una conexión residual.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Tensor de entrada con forma (batch_size, seq_length, model_dim).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Tensor de salida después de aplicar la transformación del adaptador y la conexión residual,\n",
    "                    manteniendo la forma original de la entrada.\n",
    "        \"\"\"\n",
    "        transformed_features = self.bottleneck_transform(x)  # Transforma características a través del embudo\n",
    "        output_with_residual = transformed_features + x      # Añade la conexión residual\n",
    "        return output_with_residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase `adapted` envuelve esta funcionalidad de adaptador alrededor de cualquier capa lineal especificada, mejorando su salida con la no linealidad de una función de activación ReLU. \n",
    "\n",
    "Esta configuración es especialmente útil para experimentar con modificaciones arquitectónicas sutiles en modelos de deep learning, facilitando el fine-tuning y potencialmente mejorando el rendimiento del modelo en tareas complejas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adapted(nn.Module):\n",
    "    def __init__(self, linear, bottleneck_size=None):\n",
    "        super(Adapted, self).__init__()\n",
    "        self.linear = linear\n",
    "        model_dim = linear.out_features\n",
    "        if bottleneck_size is None:\n",
    "            bottleneck_size = model_dim // 2   # Define el tamaño del cuello de botella por defecto como la mitad de model_dim\n",
    "\n",
    "        # Inicializa FeatureAdapter con el bottleneck_size y model_dim calculados\n",
    "        self.adaptor = FeatureAdapter(bottleneck_size=bottleneck_size, model_dim=model_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Primero, la entrada x pasa por la capa lineal\n",
    "        x = self.linear(x)\n",
    "        # Luego se adapta usando FeatureAdapter\n",
    "        x = self.adaptor(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, cargamos el modelo transformer preentrenado que fue entrenado sobre el dataset AG News:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/9c3Dh2O_jsYBShBuchUNlg/model-AG%20News%20small1.pth')\n",
    "model_adapters = Net(vocab_size=vocab_size, num_class=4).to(device)\n",
    "model_adapters.load_state_dict(torch.load(io.BytesIO(urlopened.read()), map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego , se congela los parámetros del modelo llamado `model_adapters` para evitar que se actualicen durante el entrenamiento. Luego, obtienes el número de características de entrada del clasificador y reemplazas dicho clasificador por una nueva capa lineal que produce dos clases:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model_adapters.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "dim= model_adapters.classifier.in_features\n",
    "model_adapters.classifier = nn.Linear(dim, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos cómo aplicar el objeto `adaptado` a una capa lineal para obtener la primera salida. Primero, obtienes la capa lineal original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_example_layer=model_adapters.transformer_encoder.layers[0].linear1\n",
    "print(mi_example_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente código, se copia la capa lineal y se añade una capa adaptadora:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_adapeted_layer=Adapted(mi_example_layer)\n",
    "print(mi_adapeted_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes imprimir la capa adaptada y mostrar que los nuevos parámetros tienen su atributo `requires_grad` en `True`, indicando que se actualizarán durante el entrenamiento:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parm in mi_adapeted_layer.parameters():\n",
    "    print(parm.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podrías asignar directamente esta capa adaptada en el modelo, pero como hay muchas capas, es más sistemático recorrerlas y reemplazar aquellas que te interesan. Ten en cuenta que al fijar el tamaño del cuello de botella en 24, habrá menos parámetros que entrenar comparado con el fine-tuning completo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapta una capa específica (comentado)\n",
    "#model_adapters.transformer_encoder.layers[0].linear1=Adapted(mi_example_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Encuentra el número de capas\n",
    "N_layers=len(model_adapters.transformer_encoder.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recorre el modelo y adaptar\n",
    "for n in range(N_layers):\n",
    "    encoder = model_adapters.transformer_encoder.layers[n]\n",
    "    if encoder.linear1:\n",
    "        print(\" antes de linear1\")\n",
    "        print(encoder.linear1)\n",
    "        model_adapters.transformer_encoder.layers[n].linear1 = Adapted(encoder.linear1, bottleneck_size=24)\n",
    "        print(\" después de linear1\")\n",
    "        print(model_adapters.transformer_encoder.layers[n].linear1)\n",
    "\n",
    "    if encoder.linear2:\n",
    "        print(\" antes de linear2\")\n",
    "        print(encoder.linear2)\n",
    "        model_adapters.transformer_encoder.layers[n].linear2 = Adapted(encoder.linear2, bottleneck_size=24)\n",
    "        print(\" después de linear2\")\n",
    "        print(model_adapters.transformer_encoder.layers[n].linear2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Envía el modelo al dispositivo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Envia modelo al dispositivo\n",
    "model_adapters.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, simulamos el entrenamiento del modelo adaptado entrenándolo en un subconjunto reducido de IMDB durante 2 épocas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR=1\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_adapters.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "save_dir = \"\"\n",
    "file_name = \"model_adapters.pth\"\n",
    "train_model(modelo=model_adapters, optimizer=optimizer, criterion=criterion, train_dataloader=train_dataloader, valid_dataloader=valid_dataloader,  epochs=2,  save_dir=save_dir ,file_name=file_name )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Naturalmente, no usaremos este modelo entrenado \"a mano\". En su lugar, seguiremos el entrenamiento de un modelo adaptado fine-tuneado sobre el conjunto completo de IMDB durante 100 épocas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/D49zrrMPWO_ktwQo7PSHIQ/model-adapters-acc')\n",
    "loss_urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/RXWlmyaco695RiaoU7QsnA/model-adapters-loss')\n",
    "acc_epoch = pickle.load(acc_urlopened)\n",
    "cum_loss_list = pickle.load(loss_urlopened)\n",
    "plot(cum_loss_list,acc_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente bloque de código carga el modelo adaptado fine-tuneado durante 100 épocas en el conjunto completo de IMDB y evalúa su rendimiento sobre el conjunto de prueba:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_adapters_ = Net(vocab_size=vocab_size, num_class=2).to(device)\n",
    "for n in range(N_layers):\n",
    "    encoder = model_adapters_.transformer_encoder.layers[n]\n",
    "    if encoder.linear1:\n",
    "        print(\" antes de linear1\")\n",
    "        print(encoder.linear1)\n",
    "        model_adapters_.transformer_encoder.layers[n].linear1 = Adapted(encoder.linear1, bottleneck_size=24)\n",
    "        print(\" después de linear1\")\n",
    "        print(model_adapters_.transformer_encoder.layers[n].linear1)\n",
    "\n",
    "    if encoder.linear2:\n",
    "        print(\" antes de linear2\")\n",
    "        print(encoder.linear2)\n",
    "        model_adapters_.transformer_encoder.layers[n].linear2 = Adapted(encoder.linear2, bottleneck_size=24)\n",
    "        print(\" después de linear2\")\n",
    "        print(model_adapters_.transformer_encoder.layers[n].linear2)\n",
    "\n",
    "model_adapters_.to(device)\n",
    "for param in model_adapters_.parameters():\n",
    "    param.requires_grad = False  # Congela todos los parámetros para la evaluación\n",
    "\n",
    "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/PGhd5G_NVrWNH-_jdjwNlw/model-adapters.pth')\n",
    "model_adapters_.load_state_dict(torch.load(io.BytesIO(urlopened.read()), map_location=device))\n",
    "evaluate(test_dataloader, model_adapters_)  # Evalúa en el conjunto de prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puedes observar, el rendimiento del modelo adaptado fine-tuneado es prácticamente idéntico al del modelo completamente fine-tuneado, alcanzando ambos alrededor de un 86% de exactitud. Esto resulta especialmente sorprendente, pues se actualizaron muchos menos pesos en el modelo adaptado que en el modelo completo. Ten en cuenta que solo las capas de adaptadores con un bottleneck de 24 y la capa final del clasificador están descongeladas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Lo anterior demuestra que los adaptadores pueden usarse para un fine-tuning eficiente en parámetros (PEFT) y que el rendimiento de un modelo fine-tuneado mediante adaptadores puede ser casi tan bueno como el de un modelo fine-tuneado completamente con todas las capas descongeladas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejercicio: Adaptar capas lineales en una red diferente**\n",
    "\n",
    "El siguiente código define una red neuronal llamada `NeuralNetwork`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "exercise_model = NeuralNetwork()\n",
    "\n",
    "exercise_model.to(device)\n",
    "for param in exercise_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(exercise_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NeuralNetwork` es una red neuronal que usa el contenedor `Sequential` de PyTorch. Adapta las dos primeras capas lineales del contenedor `Sequential` usando el adaptador de cuello de botella con un tamaño de cuello de 30. Además, cambia la última capa lineal por una capa que produzca 5 salidasT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tu respuesta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "prev_pub_hash": "76d39c215db0c382687ee9cbd1add55a12e9d224c61198bd34619b63d8828b90"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
