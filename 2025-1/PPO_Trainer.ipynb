{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RLHF usando PPO**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenaremos dos modelos de lenguaje, uno optimista y otro pesimista para simular distintos tonos en atención al cliente, usando como recompensa un clasificador de sentimiento entrenado con el dataset IMDb. Emplearemos el **Aprendizaje por Refuerzo (RL)** para que el agente (el LLM) aprenda, mediante prueba y error, a generar el texto que maximice la recompensa. \n",
    "\n",
    "Para ello usaremos el **Proximal Policy Optimization (PPO)**, un algoritmo estable y eficiente desarrollado por OpenAI que regula la magnitud de las actualizaciones de la política. A lo largo del cuaderno implementaremos y entrenaremos tu agente con PPO sobre reseñas de IMDb, adquiriendo la experiencia necesaria para aplicar RL en otros dominios. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Instalando librerías requeridas**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets trl==0.11.0\n",
    "!pip install --upgrade typing_extensions\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importación de las librería requeridas\n",
    "\n",
    "*Se recomienda importar todas las bibliotecas necesarias en un mismo lugar (aquí):*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer,AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from trl.core import LengthSampler\n",
    "import os\n",
    "\n",
    "import tarfile\n",
    "import pickle\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Definiendo funciones auxiliares**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(data, file_path):\n",
    "    \"\"\"\n",
    "    Guarda un diccionario en un archivo JSON.\n",
    "\n",
    "    Args:\n",
    "        data (dict): Diccionario que se va a guardar.\n",
    "        file_path (str): Ruta al archivo JSON.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "    print(f\"Datos guardados correctamente en {file_path}\")\n",
    "    \n",
    "    \n",
    "def load_from_json(file_path):\n",
    "    \"\"\"\n",
    "    Carga datos desde un archivo JSON.\n",
    "    Args:\n",
    "        file_path (str): Ruta al archivo JSON.\n",
    "\n",
    "    Returns:\n",
    "        dict: Datos cargados desde el archivo JSON.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence_to_length(tensor, length, pad_token_id):\n",
    "    \"\"\"\n",
    "    Rellena una secuencia (tensor 1D) hasta una longitud especificada.\n",
    "\n",
    "    Args:\n",
    "        tensor (torch.Tensor): Tensor unidimensional a rellenar.\n",
    "        length (int): Longitud objetivo de la secuencia.\n",
    "        pad_token_id (int): ID del token de relleno.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor rellenado si era más corto que `length`, o el tensor original si ya era igual o más largo.\n",
    "    \"\"\"\n",
    "    padding_length = length - tensor.size(0)\n",
    "    if padding_length > 0:\n",
    "        padding = torch.full(\n",
    "            (padding_length,),\n",
    "            pad_token_id,\n",
    "            dtype=torch.long,\n",
    "            device=tensor.device\n",
    "        )\n",
    "        return torch.cat((tensor, padding))\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def pad_list_to_batch_size(tensors, batch_size, pad_token_id):\n",
    "    \"\"\"\n",
    "    Rellena una lista de tensores para formar un lote del tamaño especificado.\n",
    "\n",
    "    1. Calcula la longitud máxima de las secuencias en `tensors`.\n",
    "    2. Rellena cada secuencia hasta esa longitud.\n",
    "    3. Si hay menos tensores que `batch_size`, añade tensores de solo relleno.\n",
    "    4. Devuelve una lista de longitud exactamente `batch_size`.\n",
    "\n",
    "    Args:\n",
    "        tensors (List[torch.Tensor]): Lista de tensores unidimensionales a agrupar.\n",
    "        batch_size (int): Tamaño final del lote.\n",
    "        pad_token_id (int): ID del token de relleno.\n",
    "\n",
    "    Returns:\n",
    "        List[torch.Tensor]: Lista de tensores rellenados con longitud `batch_size`.\n",
    "    \"\"\"\n",
    "    # Determina la longitud máxima entre todas las secuencias\n",
    "    max_length = max(t.size(0) for t in tensors)\n",
    "\n",
    "    # Rellena cada secuencia hasta max_length\n",
    "    padded_tensors = [\n",
    "        pad_sequence_to_length(t, max_length, pad_token_id)\n",
    "        for t in tensors\n",
    "    ]\n",
    "\n",
    "    # Añade tensores de solo relleno hasta completar batch_size\n",
    "    while len(padded_tensors) < batch_size:\n",
    "        padded_tensors.append(\n",
    "            torch.full((max_length,), pad_token_id, dtype=torch.long, device=tensors[0].device)\n",
    "        )\n",
    "\n",
    "    # Trunca la lista si supera batch_size\n",
    "    return padded_tensors[:batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ppo_stats(stats, related_to_objective=False):\n",
    "    \"\"\"\n",
    "    Imprime estadísticas de entrenamiento PPO.\n",
    "\n",
    "    Args:\n",
    "        stats (dict): Diccionario con las estadísticas de PPO.\n",
    "        related_to_objective (bool): Si es True, muestra estadísticas relacionadas con la función objetivo.\n",
    "    \"\"\"\n",
    "    print(\"Estadísticas de entrenamiento PPO\\n\")\n",
    "\n",
    "    if related_to_objective:\n",
    "        print(\"Estadísticas del objetivo:\")\n",
    "        print(f\"  Divergencia KL (objective/kl): {stats['objective/kl']}\")\n",
    "        print(f\"  Coeficiente KL (objective/kl_coef): {stats['objective/kl_coef']}\")\n",
    "        print(f\"  Entropía (objective/entropy): {stats['objective/entropy']}\\n\")\n",
    "        \n",
    "        print(\"Pérdidas PPO (relacionadas con la minimización de la función objetivo):\")\n",
    "        print(f\"  Pérdida de política (ppo/loss/policy): {stats['ppo/loss/policy']}\")\n",
    "        print(f\"  Pérdida de valor (ppo/loss/value): {stats['ppo/loss/value']}\")\n",
    "        print(f\"  Pérdida total (ppo/loss/total): {stats['ppo/loss/total']}\\n\")\n",
    "        \n",
    "        print(\"Estadísticas de la Política PPO:\")\n",
    "        print(f\"  Entropía de política (ppo/policy/entropy): {stats['ppo/policy/entropy']}\")\n",
    "        print(f\"  KL aproximado (ppo/policy/approxkl): {stats['ppo/policy/approxkl']}\")\n",
    "        print(f\"  Fracción de clip (ppo/policy/clipfrac): {stats['ppo/policy/clipfrac']}\\n\")\n",
    "    else:\n",
    "        print(\"Recompensa y estimación de la función de valor:\")\n",
    "        print(f\"  Recompensa promedio sin puntaje (ppo/mean_non_score_reward): {stats['ppo/mean_non_score_reward']}\")\n",
    "        print(f\"  Puntajes medios (ppo/mean_scores): {stats['ppo/mean_scores']}\")\n",
    "        print(f\"  Desviación estándar de puntajes (ppo/std_scores): {stats['ppo/std_scores']}\")\n",
    "        print(f\"  Predicción de valor (ppo/val/vpred): {stats['ppo/val/vpred']}\")\n",
    "        print(f\"  Error de predicción de valor (ppo/val/error): {stats['ppo/val/error']}\")\n",
    "        print(f\"  Varianza de predicción de valor (ppo/val/var): {stats['ppo/val/var']}\")\n",
    "        print(f\"  Media de predicción de valor (ppo/val/mean): {stats['ppo/val/mean']}\")\n",
    "        print(f\"  Varianza explicada (ppo/val/var_explained): {stats['ppo/val/var_explained']}\\n\")\n",
    "    \n",
    "    print(\"Longitudes de tokens:\")\n",
    "    print(f\"  Longitud media de consultas (tokens/queries_len_mean): {stats['tokens/queries_len_mean']}\")\n",
    "    print(f\"  Longitud media de respuestas (tokens/responses_len_mean): {stats['tokens/responses_len_mean']}\\n\")\n",
    "    \n",
    "    print(\"Estadísticas de Ttempo:\")\n",
    "    print(f\"  Tiempo total (time/ppo/total): {stats['time/ppo/total']} segundos\\n\")\n",
    "\n",
    "# Ejemplo de uso con stats proporcionadas y la bandera related_to_objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Inicialización de la configuración, el modelo y el tokenizador de PPO**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "La clase `PPOConfig` se utiliza para especificar el modelo y la tasa de aprendizaje para el entrenamiento de PPO. En este caso, el modelo es `\"lvwerra/gpt2-imdb\"` y la tasa de aprendizaje se establece en `1.41e-5`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PPOConfig(\n",
    "    model_name=\"lvwerra/gpt2-imdb\",\n",
    "    learning_rate=1.41e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`config.model_name` se refiere al identificador del modelo que se usa en la configuración para cargar el modelo preentrenado. Especifica qué modelo cargar desde el repositorio de Hugging Face. En este caso, `config.model_name` está establecido como `\"lvwerra/gpt2-imdb\"`, lo que indica que se debe usar el modelo GPT-2 afinado en el conjunto de datos IMDb (por el usuario lvwerra). Este identificador es esencial para cargar la arquitectura y los pesos correctos durante el proceso de fine-tuning o inferencia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El diccionario `sent_kwargs` contiene parámetros para la canalización de análisis de sentimiento, especificando que se deben devolver todos los puntajes, que la función a aplicar es `\"none\"` y que el tamaño de lote es `2`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_kwargs = {\"top_k\":None, \"function_to_apply\": \"none\", \"batch_size\": 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase `AutoModelForCausalLMWithValueHead` se utiliza para cargar el modelo GPT-2 preentrenado con una cabeza de valor para el entrenamiento PPO. El modelo se carga a partir del nombre de modelo especificado en la configuración.\n",
    "\n",
    "La clase `AutoTokenizer` se emplea para cargar el tokenizador correspondiente al modelo preentrenado. El token de relleno del tokenizador se establece como el token de fin de secuencia (EOS).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignora la advertencia anterior, ya que la versión de `trl` que instalaste la maneja automáticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primer modelo\n",
    "modelo = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durante el entrenamiento PPO se actualiza el modelo. Además, se utiliza un modelo de referencia para estabilizar la política mediante la divergencia de Kullback–Leibler (KL) entre la política actual y la política de referencia. La divergencia KL actúa como un término de regularización.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Conjunto de datos y tokenización**\n",
    "\n",
    "**Nombre del conjunto de datos:** IMDb\n",
    "\n",
    "**Descripción:** El conjunto de datos IMDb consta de 50.000 reseñas de películas etiquetadas como \"positive2 o \"negative\", indicando el sentimiento de cada reseña. Es comúnmente usado para tareas de análisis de sentimiento.\n",
    "\n",
    "**Carga del conjunto de datos:**\n",
    "Se carga usando la función `load_dataset` de la librería `datasets`, específicamente la división \"train\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"imdb\"\n",
    "ds = load_dataset(dataset_name, split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "for sample in range(N):\n",
    "    print('texto',ds[sample]['text'])\n",
    "    print('etiqueta',ds[sample]['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Renombra la columna `\"text\"` a `\"review\"`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.rename_columns({\"text\": \"review\"})\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "El conjunto de datos se filtra para incluir solo reseñas de más de 200 caracteres:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.filter(lambda x: len(x[\"review\"]) > 200, batched=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usar un `LengthSampler` para muestrear diferentes longitudes de texto durante el procesamiento de datos introduce variabilidad, haciendo que el modelo sea más robusto y capaz de manejar entradas de longitud variable en escenarios reales. Este enfoque previene el sobreajuste al exponer al modelo a tamaños de entrada diversos, mejorando la generalización a datos nuevos. También asegura un entrenamiento eficiente al gestionar la longitud de los textos, manteniendo la practicidad y el rendimiento. En conjunto, `LengthSampler` potencia la adaptabilidad y efectividad del modelo al simular condiciones de entrenamiento realistas y variadas, con longitudes de muestra entre `input_min_text_length` e `input_max_text_length`.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_min_text_length, input_max_text_length = 2, 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear un objeto `LengthSampler`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
    "input_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este objeto `input_size`, instancia de `LengthSampler`, genera una longitud de texto aleatoria entre 2 y 8 en cada llamada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    size=input_size()\n",
    "    print(f\"La muestra {i} tiene longitud {size}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, necesitaremos muestrear tokens y obtener índices tokenizados. Verifiquemos este proceso con una muestra.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=ds[0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizamos el texto de la clave `\"review\"` en IDs, truncamos la secuencia a la longitud deseada y la asignamos a `\"input_ids\"`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[: input_size()]\n",
    "sample[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decodificamos esos IDs truncados de nuevo a texto y lo guardamos en `\"query\"`, ya que necesitaremos el texto plano para la función de recompensa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "sample[\"query\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Para aplicar esto sobre todo el dataset, definimos la función `tokenize`, que combina tokenización, truncado y decodificación:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sample):\n",
    "    sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[: input_size()]\n",
    "    sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y la usamos con `map` para procesar todo el conjunto de datos, además de darle formato PyTorch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map(tokenize, batched=False)\n",
    "ds.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Nota: puedes ignorar de forma segura la advertencia que aparece arriba.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos iterar y mostrar las primeras 5 muestras con su `'review'`, `'input_ids'` y `'query'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(ds):\n",
    "    if i >= 5:\n",
    "        break\n",
    "    print(f\"Muestra {i+1}:\")\n",
    "    print(f\"Revisión: {sample['review']}\")\n",
    "    print(f\"IDs de entradas: {sample['input_ids']}\")\n",
    "    print(f\"Consulta: {sample['query']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `build_dataset` integra todos estos pasos para crear el dataset que usará posteriormente `PPOTrainer`. Primero eliminamos cualquier variable previa y recargamos el dataset original:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(ds)\n",
    "dataset_name=\"imdb\"\n",
    "ds = load_dataset(dataset_name, split=\"train\")\n",
    "ds = ds.rename_columns({\"text\": \"review\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(config,\n",
    "                  dataset_name=\"imdb\",\n",
    "                  input_min_text_length=2,\n",
    "                  input_max_text_length=8,\n",
    "                  tokenizer=tokenizer):\n",
    "    \"\"\"\n",
    "    Construye y devuelve el dataset listo para PPOTrainer.\n",
    "\n",
    "    Args:\n",
    "        config:     Objeto con la configuración (incluye model_name).\n",
    "        dataset_name (str): Nombre del dataset a cargar.\n",
    "        input_min_text_length (int): Longitud mínima de entrada.\n",
    "        input_max_text_length (int): Longitud máxima de entrada.\n",
    "        tokenizer:  Tokenizador ya inicializado.\n",
    "\n",
    "    Returns:\n",
    "        Dataset: Objeto tokenizado y formateado para PyTorch.\n",
    "    \"\"\"\n",
    "    # (Re)inicializa el tokenizador y su token de padding\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Carga y filtra el dataset IMDb\n",
    "    ds = load_dataset(dataset_name, split=\"train\")\n",
    "    ds = ds.rename_columns({\"text\": \"review\"})\n",
    "    ds = ds.filter(lambda x: len(x[\"review\"]) > 200, batched=False)\n",
    "\n",
    "    # Crea  LengthSampler\n",
    "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
    "\n",
    "    # Función de tokenización y truncado\n",
    "    def tokenize(sample):\n",
    "        sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[: input_size()]\n",
    "        sample[\"query\"]     = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    # Aplica tokenización y dar formato PyTorch\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el objeto dataset y verificamos su contenido:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = build_dataset(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muestra la primera muestra con sus campos 'input_ids' y 'query'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Función collator**\n",
    "\n",
    "La función collator es crucial para preparar lotes de datos en un formato adecuado para el PPOTrainer. Se encarga de agrupar cada característica de las muestras de datos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función collator se entiende mejor con un ejemplo. Podemos introducir dos muestras, cada una con `'input_ids'`, `'query'` y `'review'`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {'input_ids': [1, 2, 3, 4], 'query': \"texto de ejemplo\", 'review': \"Esta es una reseña de ejemplo.\"},\n",
    "    {'input_ids': [5, 6, 7, 8], 'query': \"otro ejemplo\", 'review': \"Otra reseña de ejemplo.\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos la función collator a los datos anteriores:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = collator(data)\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, `'input_ids'`, `'query'` y `'review'` contienen sus muestras correspondientes agrupadas en listas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Inicializa PPOTrainer**\n",
    "\n",
    "Proximal Policy Optimization (PPO) es un algoritmo de aprendizaje por refuerzo especialmente adecuado para entrenar modelos generativos, incluidos los chatbots. Ayuda a resolver desafíos como mantener diálogos coherentes y contextualmente apropiados.\n",
    "\n",
    "PPO mejora los métodos de gradiente de política para chatbots usando una función objetivo recortada, lo cual garantiza actualizaciones de política graduales y estables. Esto ayuda a mantener la calidad del diálogo. Los métodos tradicionales pueden presentar alta varianza e inestabilidad, resultando en comportamientos inconsistentes del chatbot. La región de confianza de PPO equilibra la exploración de nuevas respuestas y la explotación de las ya buenas, haciéndolo más fiable para entrenar chatbots.\n",
    "\n",
    "El PPO Trainer recopila muestras de diálogo, optimiza la política del chatbot según estas muestras y gestiona los modelos de red neuronal. Esto asegura un entrenamiento estable y eficiente, llevando a respuestas de chatbot coherentes y de alta calidad.\n",
    "\n",
    "Inicialicemos el PPOTrainer con la configuración y componentes especificados:\n",
    "\n",
    "* `config`: Ajustes de configuración para el entrenamiento PPO, como tasa de aprendizaje y nombre del modelo.\n",
    "* `modelo`: Modelo principal que se fine-tuneará con PPO.\n",
    "* `tokenizer`: Tokenizador correspondiente al modelo, usado para procesar texto de entrada.\n",
    "* `dataset`: Conjunto de datos para entrenamiento, que proporciona los datos de entrada al modelo.\n",
    "* `data_collator`: Collator para manejar la creación de lotes y el formateo de los datos de entrada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_trainer = PPOTrainer(config, modelo, ref_model, tokenizer, dataset=dataset, data_collator=collator)\n",
    "print(\"El objeto ppo_trainer  \",ppo_trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignora las advertencias anteriores, la versión de `trl` que instalaste soporta este módulo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determina el dispositivo adecuado (CPU o GPU) para el entrenamiento con el PPOTrainer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ppo_trainer.accelerator.device\n",
    "if ppo_trainer.accelerator.num_processes == 1:\n",
    "    device = 0 if torch.cuda.is_available() else \"cpu\"  \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Función de recompensa**\n",
    "\n",
    "En el aprendizaje por refuerzo con PPO, se usa una función de recompensa para proporcionar retroalimentación sobre la calidad de las acciones tomadas por la política. Para un modelo generativo como un chatbot, la función de recompensa puede evaluar la calidad de las respuestas generadas. A continuación se explica cómo usar el pipeline de análisis de sentimiento como función de recompensa:\n",
    "\n",
    "El pipeline de análisis de sentimiento sirve para evaluar las respuestas del chatbot y asignarles una recompensa basada en la puntuación de sentimiento. El PPO Trainer optimiza la política del chatbot para generar respuestas con mejor recepción y más atractivas. Aunque no es un modelo de recompensa típico, permite entrenar al chatbot de forma sencilla y efectiva.\n",
    "\n",
    "Primero, inicializa un pipeline de análisis de sentimiento usando un modelo preentrenado fine-tuneado con reseñas de IMDB. El modelo predice el sentimiento de entradas de texto, proporcionando puntuaciones para sentimientos positivos y negativos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipe = pipeline(\"sentiment-analysis\", model=\"lvwerra/distilbert-imdb\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al evaluar un texto negativo, obtendrás algo así:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"this movie was really bad!!\"\n",
    "sentiment_pipe(text, **sent_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clave `score` representa la confianza del modelo en su predicción. Valores más altos indican mayor seguridad en la clasificación de sentimiento, ya sea \"POSITIVE\" o \"NEGATIVE\". Así, el valor para la clase `POSITIVE` puede emplearse para determinar los valores de recompensa. Por ejemplo, una puntuación alta en \"POSITIVE\" incrementa la recompensa. Por el contrario, si el modelo no está seguro de que una reseña sea positiva, produce una recompensa baja, reduciendo la recompensa total. Esto significa que las reseñas con sentimiento negativo disminuyen la recompensa global, mientras que las positivas la aumentan.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"this movie was really good!!\"\n",
    "sentiment_pipe(text, **sent_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Generación de respuestas usando PPO**\n",
    "\n",
    "#### **Tokenización y preparación del lote de entrada**\n",
    "\n",
    "Esta sección de código muestra cómo generar respuestas usando el PPO (Proximal Policy Optimization) Trainer. El proceso implica tokenizar la entrada, preparar el lote para el entrenamiento, generar respuestas y decodificar los tokens generados a texto legible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código primero recupera un lote de datos del dataloader del PPO Trainer y selecciona las dos primeras entradas para su procesamiento:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(ppo_trainer.dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El lote contiene las claves `label`, `input_ids` y `query`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos un nuevo lote que contiene solo las dos primeras muestras del lote original:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {key: batch[key][0:2] for key in batch}\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializamos una lista de `response_tensors` para almacenar las respuestas que luego se evaluarán:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_tensors = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código extrae los `input_ids` del `batch` y los asigna a `query_tensors`. Estos tensores representan las secuencias de entrada tokenizadas que el modelo usará para generar respuestas. Se llaman \"query tensors\" porque representan las consultas iniciales que el modelo procesará:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tensors =  batch[\"input_ids\"]\n",
    "query_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, definimos una función lambda `get_text` que toma una lista de respuestas (`response`) y decodifica cada tensor en la lista usando el tokenizador, convirtiéndolo de nuevo en texto legible. El método `squeeze()` elimina dimensiones de tamaño 1 del tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_text = lambda response:''.join([tokenizer.decode(r.squeeze()) for r in response])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver las consultas originales en su forma de texto:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_text(query_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El diccionario `generation_kwargs` establece los parámetros para generar una secuencia del LLM (Modelo de Lenguaje). Los parámetros son:\n",
    "\n",
    "* `\"min_length\": -1` — Sin longitud mínima para el texto generado.\n",
    "* `\"top_k\": 0.0` — Sin filtrado top-k de tokens más probables.\n",
    "* `\"top_p\": 1.0` — Sin muestreo de núcleo (nucleus sampling), usando toda la distribución.\n",
    "* `\"do_sample\": True` — Habilita muestreo, permitiendo respuestas variadas.\n",
    "* `\"pad_token_id\": 50256` — ID del token de relleno, para uniformar la longitud de las secuencias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": 50256,\n",
    "}\n",
    "generation_kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El `output_length_sampler` se inicializa con `LengthSampler(output_min_length, output_max_length)`. Este objeto se utiliza para muestrear las longitudes de salida de las secuencias generadas, garantizando que se encuentren dentro del rango de longitud mínima y máxima especificado. Al variar las longitudes, se pueden generar resultados más diversos y naturales a partir del modelo de lenguaje, lo que evita la generación de secuencias demasiado cortas o excesivamente largas y mejora la calidad general de las respuestas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_min_length = 4\n",
    "output_max_length = 16\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código llama a `output_length_sampler` para determinar la longitud de las secuencias generadas. La longitud muestreada se almacena en la variable `gen_len`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_len = output_length_sampler()\n",
    "gen_len "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, establecemos el parámetro `max_new_tokens` en `generation_kwargs` al valor de `gen_len`, asegurando que el número máximo de tokens nuevos esté dentro del rango deseado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "generation_kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora procesemos la primera muestra usando PPO. Empezamos extrayendo el primer tensor de consulta:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=query_tensors[0]\n",
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos una respuesta para la consulta usando el PPO Trainer con los parámetros de generación especificados. El tensor de respuesta se almacena en `response`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ppo_trainer.generate(query, **generation_kwargs)\n",
    "response "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Nota: Puedes ignorar la advertencia anterior.\n",
    "\n",
    "Podemos imprimir el texto decodificado de la consulta y la respuesta usando `get_text`, para mostrar cómo el modelo ha añadido texto a la consulta original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Consulta:\",get_text(query))\n",
    "print(\"respuesta:\", get_text(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finalmente, añadimos los tokens recién generados a la lista `response_tensors`. El método `squeeze()` elimina dimensiones de tamaño 1, y `[-gen_len:]` asegura que solo se incluyan los tokens generados en esta iteración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_tensors.append(response.squeeze()[-gen_len:])\n",
    "print(\"tokens recién generados de la respuesta:\", get_text(response_tensors[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repetimos el proceso para la segunda muestra. Esta sección genera una respuesta para una consulta dada, decodifica la parte relevante y la añade a la lista `response_tensors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = query_tensors[1]\n",
    "gen_len = output_length_sampler()\n",
    "generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "response = ppo_trainer.generate(query, **generation_kwargs)\n",
    "print(\"Consulta:\", get_text(query))\n",
    "print(\"response acumulada:\", get_text(response_tensors))\n",
    "response_tensors.append(response.squeeze()[-gen_len:])\n",
    "print(\"tokens recién generados de la segunda respuesta:\", get_text(response_tensors[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertimos cada tensor en `response_tensors` a texto legible y lo almacenamos en el diccionario `batch` bajo la clave `response`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "batch[\"response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "El lote ahora contiene tanto `query` como `response`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Función de puntuación**\n",
    "\n",
    "A continuación, preparamos los textos para el análisis de sentimiento, que puede formar parte de una función de recompensa en PPO. Extraemos las cadenas `query` y `response` y las combinamos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las puntuaciones de sentimiento (`pipe_outputs`) se pueden utilizar como retroalimentación para actualizar la política\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n",
    "pipe_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas puntuaciones permiten evaluar la calidad o relevancia de las respuestas generadas, lo que indica la confianza del modelo en la probabilidad de que sean positivas. Las puntuaciones de las respuestas generadas se extraen de la lista `pipe_outputs`. Cada elemento de `pipe_outputs` contiene una lista de puntuaciones correspondientes a la salida del modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta línea itera sobre la lista `pipe_outputs`, extrae la puntuación de cada salida, la convierte en un tensor y la almacena en la lista `rewards`. Las puntuaciones representan la confianza del modelo en la probabilidad de que las respuestas sean oraciones positivas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_scores = [\n",
    "    item[\"score\"]\n",
    "    for output in pipe_outputs\n",
    "    for item in output\n",
    "    if item[\"label\"] == \"POSITIVE\"\n",
    "]\n",
    "rewards = [torch.tensor(score) for score in positive_scores]\n",
    "rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Optimización de política proximal (PPO)**\n",
    "\n",
    "El bucle de entrenamiento es responsable de realizar un único paso de actualización del algoritmo PPO. Las entradas a este proceso son los tensores de consulta, respuesta y recompensa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Consulta:\", get_text(query_tensors))\n",
    "print(\"\\n\")\n",
    "print(\"respuesta:\", get_text(response_tensors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cumplir con el requisito de tamaño mínimo de lote (batch) de 128 del PPO Trainer, puedes rellenar (`pad`) los tensores de respuesta con muestras adicionales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "query_tensors = pad_list_to_batch_size(query_tensors, batch_size, pad_token_id)\n",
    "\n",
    "response_tensors = pad_list_to_batch_size(response_tensors, batch_size, pad_token_id)\n",
    "rewards=rewards+[torch.tensor(0) for _ in range(batch_size-len(rewards))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora llama al método `step` del PPO Trainer para actualizar el modelo usando el algoritmo PPO con `query_tensors`, `response_tensors` y `rewards`.\n",
    "\n",
    "* Calcula las pérdidas de la política y de la función de valor.\n",
    "* Computa los gradientes y actualiza los parámetros de la red de política para mejorarla.\n",
    "* Garantiza que la actualización de la política se mantenga dentro de un rango determinado, evitando grandes desplazamientos de política, que es un aspecto central de PPO.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Nota:** El siguiente código está comentado para evitar que el kernel falle por no tener GPU en el entorno actual. Para ejecutarlo, descarga el notebook y ejecútalo en un entorno con GPU. Descomenta el código antes de ejecutarlo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats = ppo_trainer.step(query_tensors, response_tensors, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable `stats` es un diccionario que contiene diversas estadísticas del paso de entrenamiento PPO. Puedes imprimir sus claves usando la función `print_ppo_stats`. Estas estadísticas se organizan en dos categorías principales:\n",
    "\n",
    "* **Minimización de la pérdida del modelo de lenguaje** (`related_to_objective=True`):\n",
    "  Incluye métricas relacionadas con la optimización de parámetros del modelo, como la pérdida de política y la pérdida de valor.\n",
    "* **Cálculo de la recompensa**:\n",
    "  Incluye métricas propias del aprendizaje por refuerzo, como estimaciones de ventaja y cálculos de recompensa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_ppo_stats(stats, related_to_objective = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_ppo_stats(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable `sentiment` debe establecerse en `\"NEGATIVE\"` para respuestas malas y en `\"POSITIVE\"` para respuestas buenas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = \"POSITIVE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este fragmento de código representa un bucle de entrenamiento para el algoritmo PPO utilizando análisis de sentimiento. El bucle itera sobre lotes de datos del dataloader de `ppo_trainer` y realiza los siguientes pasos:\n",
    "\n",
    "1. **Extrae tensores de consulta**:\n",
    "   Se obtienen los `input_ids` (tensores de consulta) del lote.\n",
    "\n",
    "2. **Genera respuestas**:\n",
    "   Para cada tensor de consulta, se genera una respuesta con `ppo_trainer.generate` usando los `generation_kwargs` especificados. Las respuestas se decodifican y se añaden al lote bajo la clave `response`.\n",
    "\n",
    "3. **Calcula puntuaciones de sentimiento**:\n",
    "\n",
    "   * Se preparan los textos concatenando consultas y respuestas.\n",
    "   * Se realiza el análisis de sentimiento sobre los textos combinados para obtener las puntuaciones.\n",
    "   * Se convierten las puntuaciones en tensores y se almacenan en la lista `rewards`.\n",
    "\n",
    "4. **Ejecuta el paso de PPO**:\n",
    "\n",
    "   * Se llama a `ppo_trainer.step` para actualizar el modelo usando PPO con los tensores de consulta, respuesta y las recompensas calculadas.\n",
    "   * Este paso calcula pérdidas de política y de valor, computa gradientes y actualiza los parámetros de la red de política.\n",
    "   * La actualización se limita para evitar cambios drásticos en la política.\n",
    "\n",
    "5. **Registra estadísticas**:\n",
    "\n",
    "   * Las estadísticas resultantes del paso de entrenamiento PPO se registran y almacenan en la lista `all_stats`.\n",
    "\n",
    "> **Nota:** Entrenar el modelo en CPU será muy lento. Para tu conveniencia, ya se ha preentrenado el modelo en GPU y lo hemos guardado. Puedes omitir la parte de entrenamiento y cargar el modelo guardado, o descomentar el bloque de código para entrenarlo tú mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucle de entrenamiento PPO\n",
    "# for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "#     query_tensors = batch[\"input_ids\"]\n",
    "#     print(f\"epoca {epoch}\")\n",
    "# ## Obtiene respuestas de GPT-2\n",
    "#     response_tensors = []\n",
    "#     for query in query_tensors:\n",
    "#         gen_len = output_length_sampler()\n",
    "#         generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "#         response = ppo_trainer.generate(query, **generation_kwargs)\n",
    "#         response_tensors.append(response.squeeze()[-gen_len:])\n",
    "#     batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "\n",
    "#     texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "#     pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n",
    "#     positive_scores = [\n",
    "#            item[\"score\"]\n",
    "#            for output in pipe_outputs\n",
    "#            for item in output\n",
    "#            if item[\"label\"] == sentiment\n",
    "#        ]\n",
    "#    rewards = [torch.tensor(score) for score in positive_scores]\n",
    "\n",
    "#     #### Corremos los pasos PPO\n",
    "#     stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "#     ppo_trainer.log_stats(stats, batch, rewards)\n",
    "    \n",
    "#     all_stats.append(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Guardamos el modelo\n",
    "# model_dir = \"ppo-good\"\n",
    "# os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# # Guardamos la configuracion del modelo y los pesos\n",
    "# model_1.save_pretrained(model_dir)\n",
    "# tokenizer.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/gSWo8GeztngSmzHpqX_RaQ/ppo-good.pkl\n",
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/we8t5N-45dVq3VhxGwYRAg/ppo-good-tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre del archivo\n",
    "file_name = \"ppo-good-tar.gz\"\n",
    "\n",
    "# Abre el archivo tar.gz\n",
    "with tarfile.open(file_name, \"r:gz\") as tar:\n",
    "    # Extrae todo el contenido en el directorio actual\n",
    "    tar.extractall()\n",
    "\n",
    "print(\"Extracción completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"ppov3new1\"\n",
    "model_1 = AutoModelForCausalLMWithValueHead.from_pretrained(model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "# Carga estadísticas de entrenamiento\n",
    "file_name = \"ppo-good.pkl\"\n",
    "with open(file_name, 'rb') as f:\n",
    "    all_stats = pickle.load(f)\n",
    "\n",
    "model_1.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> Nota: Puedes ignorar con seguridad la advertencia anterior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Gráfica de la pérdida de entrenamiento PPO y recompensa media**\n",
    "\n",
    "1. **Extracción de valores**:\n",
    "\n",
    "   * `loss_values`: Valores de la pérdida total obtenidos de `all_stats`.\n",
    "   * `reward_values`: Valores de la recompensa media obtenidos de `all_stats`.\n",
    "\n",
    "2. **Gráfica de la función de pérdida**:\n",
    "\n",
    "   * Gráfica de línea de la pérdida total a lo largo de las épocas.\n",
    "\n",
    "3. **Gráfica de las recompensas**:\n",
    "\n",
    "   * Gráfica de línea de la recompensa media a lo largo de las épocas.\n",
    "\n",
    "4. **Mostrar las gráficas**:\n",
    "\n",
    "   * Organizar y mostrar las gráficas usando `plt.tight_layout()` y `plt.show()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values = [stat['ppo/loss/total'] for stat in all_stats]\n",
    "reward_values = [stat['ppo/mean_scores'] for stat in all_stats]\n",
    "\n",
    "# Grafica la pérdida\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(loss_values, label='Pérdida Total', color='b')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.title('Pérdida de Entrenamiento PPO a lo largo del tiempo')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Grafica las recompensas\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(reward_values, label='Recompensa media', color='g')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Recompensa')\n",
    "plt.title('Recompensa media de PPO a lo largo del tiempo')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Muestra las gráficas\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Generación y análisis de texto con PPO y modelos de referencia**\n",
    "\n",
    "**Configuración del dispositivo**:\n",
    "\n",
    "* Determinar si CUDA está disponible y asignar el dispositivo correspondiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pipeline_device = 0 if device.type == \"cuda\" else -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Función de generación de texto**:\n",
    "\n",
    "* `generate_some_text(input_text, mi_model)`: Tokeniza el texto de entrada, genera una respuesta y la decodifica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"max_new_tokens\": 20,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id\n",
    "}\n",
    "\n",
    "def generate_some_text(input_text, mi_model):\n",
    "    # Tokeniza el texto de entrada\n",
    "    input_ids = tokenizer(input_text, return_tensors='pt').input_ids.to(device)\n",
    "    generated_ids = mi_model.generate(input_ids, **gen_kwargs)\n",
    "\n",
    "    # Decodifica el texto generado\n",
    "    generated_text_ = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated_text_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generar texto con el modelo PPO**:\n",
    "\n",
    "* Generamos texto usando el modelo entrenado con PPO.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Once upon a time in a land far\"\n",
    "\n",
    "generated_text=generate_some_text(input_text,model_1)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Análisis de sentimiento**:\n",
    "\n",
    "* Analiza el sentimiento del texto generado usando `sentiment_pipe`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_outputs = sentiment_pipe(generated_text, **sent_kwargs)\n",
    "pipe_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generar texto con el modelo de referencia**:\n",
    "\n",
    "* Genera texto usando el modelo de referencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = generate_some_text(input_text,ref_model)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comparación de los modelos PPO y de referencia**\n",
    "\n",
    "1. **Parámetros de generación**:\n",
    "\n",
    "   * Define `gen_kwargs` para la generación de texto.\n",
    "\n",
    "2. **Preparar el lote**:\n",
    "\n",
    "   * Obtiene una muestra de tamaño `bs` del conjunto de datos y extraer los tensores de consulta.\n",
    "\n",
    "3. **Generar respuestas**:\n",
    "\n",
    "   * Para cada tensor de consulta, generar respuestas usando tanto el modelo de referencia como el modelo PPO.\n",
    "\n",
    "4. **Decodificar respuestas**:\n",
    "\n",
    "   * Decodifica los tensores de respuesta en texto legible.\n",
    "\n",
    "5. **Calcular puntuaciones de sentimiento**:\n",
    "\n",
    "   * Prepara textos concatenando consultas y respuestas.\n",
    "   * Calcula las puntuaciones de sentimiento para las respuestas antes y después del entrenamiento usando `sentiment_pipe`.\n",
    "\n",
    "6. **Almacenar resultados**:\n",
    "\n",
    "   * Guarda consultas, respuestas y puntuaciones de sentimiento en `game_data`.\n",
    "   * Convertir `game_data` a un DataFrame y devolverlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models_on_dataset(modelo, ref_model, dataset, tokenizer, sentiment_pipe, sent_kwargs, device, output_length_sampler):\n",
    "    gen_kwargs = {\n",
    "        \"min_length\": -1, \n",
    "        \"top_k\": 0.0, \n",
    "        \"top_p\": 1.0, \n",
    "        \"do_sample\": True, \n",
    "        \"pad_token_id\": tokenizer.eos_token_id\n",
    "    }\n",
    "    \n",
    "    bs = 16\n",
    "    game_data = dict()\n",
    "    dataset.set_format(\"pandas\")\n",
    "    df_batch = dataset[:].sample(bs)\n",
    "    game_data[\"query\"] = df_batch[\"query\"].tolist()\n",
    "    query_tensors = df_batch[\"input_ids\"].tolist()\n",
    "\n",
    "    response_tensors_ref, response_tensors = [], []\n",
    "\n",
    "    #  Obtiene el máximo de embeddings de posición para ambos modelos\n",
    "    max_position_embeddings_ref = ref_model.config.max_position_embeddings\n",
    "    max_position_embeddings_model = modelo.config.max_position_embeddings\n",
    "\n",
    "    for i in range(bs):\n",
    "        gen_len = output_length_sampler()\n",
    "\n",
    "        # Convierte tensores de consulta a input IDs\n",
    "        input_ids = torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device)\n",
    "\n",
    "        # Proceso para ref_model \n",
    "        total_length_ref = input_ids.shape[-1] + gen_len\n",
    "        if total_length_ref > max_position_embeddings_ref:\n",
    "            # Trunca input_ids para ajustarse al máximo\n",
    "            max_input_length_ref = max_position_embeddings_ref - gen_len\n",
    "            input_ids_ref = input_ids[:, -max_input_length_ref:]\n",
    "            total_length_ref = input_ids_ref.shape[-1] + gen_len\n",
    "        else:\n",
    "            input_ids_ref = input_ids\n",
    "        \n",
    "        output = ref_model.generate(\n",
    "            torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device), \n",
    "            max_new_tokens=gen_len, \n",
    "            **gen_kwargs\n",
    "        ).squeeze()[-gen_len:]\n",
    "        response_tensors_ref.append(output)\n",
    "\n",
    "        # Proceso para modelo\n",
    "        total_length_model = input_ids.shape[-1] + gen_len\n",
    "        if total_length_model > max_position_embeddings_model:\n",
    "            max_input_length_model = max_position_embeddings_model - gen_len\n",
    "            input_ids_model = input_ids[:, -max_input_length_model:]\n",
    "            total_length_model = input_ids_model.shape[-1] + gen_len\n",
    "        else:\n",
    "            input_ids_model = input_ids\n",
    "        \n",
    "        output = modelo.generate(\n",
    "            torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device), \n",
    "            max_new_tokens=gen_len, \n",
    "            **gen_kwargs\n",
    "        ).squeeze()[-gen_len:]\n",
    "        response_tensors.append(output)\n",
    "\n",
    "    game_data[\"respuesta (antes)\"] = [tokenizer.decode(t) for t in response_tensors_ref]\n",
    "    game_data[\"respuesta (después)\"] = [tokenizer.decode(t) for t in response_tensors]\n",
    "\n",
    "    texts_before = [q + r for q, r in zip(game_data[\"query\"], game_data[\"respuesta (antes)\"])]\n",
    "    game_data[\"puntuaciones (antes)\"] = [output[1][\"score\"] for output in sentiment_pipe(texts_before, **sent_kwargs)]\n",
    "\n",
    "    texts_after = [q + r for q, r in zip(game_data[\"query\"], game_data[\"respuesta (después)\"])]\n",
    "    game_data[\"puntuaciones (después)\"] = [output[1][\"score\"] for output in sentiment_pipe(texts_after, **sent_kwargs)]\n",
    "\n",
    "    df_results = pd.DataFrame(game_data)\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = compare_models_on_dataset(model_1, ref_model, dataset, tokenizer, sentiment_pipe, sent_kwargs, device, output_length_sampler)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejecución del modelo PPO con sentimiento negativo**\n",
    "\n",
    "Este código ejecuta el bucle de entrenamiento PPO con el sentimiento configurado como **NEGATIVO**, lo que evalúa el desempeño del modelo cuando se priorizan las puntuaciones de sentimiento negativo. El bucle de entrenamiento genera respuestas, calcula las puntuaciones de sentimiento, actualiza el modelo y registra las estadísticas en cada época.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = \"NEGATIVE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucle de entrenamiento PPO\n",
    "# for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "#     query_tensors = batch[\"input_ids\"]\n",
    "#     print(f\"época {epoch}\")\n",
    "#\n",
    "#     #### Obtiene respuestas de GPT-2\n",
    "#     response_tensors = []\n",
    "#     for query in query_tensors:\n",
    "#         gen_len = output_length_sampler()\n",
    "#         generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "#         response = ppo_trainer.generate(query, **generation_kwargs)\n",
    "#         response_tensors.append(response.squeeze()[-gen_len:])\n",
    "#     batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "#\n",
    "#     #### Calcula la puntuación de sentimiento\n",
    "#     texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "#     pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n",
    "#     negative_scores = [\n",
    "#         item[\"score\"]\n",
    "#         for output in pipe_outputs\n",
    "#         for item in output\n",
    "#         if item[\"label\"] == sentiment\n",
    "#     ]\n",
    "#     rewards = [torch.tensor(score) for score in negative_scores]\n",
    "#\n",
    "#     #### Ejecuta el paso de PPO\n",
    "#     stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "#     ppo_trainer.log_stats(stats, batch, rewards)\n",
    "#    \n",
    "#     all_stats.append(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Guarda el modelo entrenado\n",
    "#\n",
    "# model_dir = \"ppo-bad\"\n",
    "# os.makedirs(model_dir, exist_ok=True)\n",
    "#\n",
    "# # Guarda configuración y pesos del modelo\n",
    "# model_0.save_pretrained(model_dir)\n",
    "# tokenizer.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:** Entrenar el modelo en CPU será muy lento. El modelo ya ha sido preentrenado usando GPU y guardado para tu conveniencia. Puedes omitir la parte de entrenamiento, continuar con el siguiente bloque de código y cargar el modelo guardado. Si deseas, puedes descomentar el bloque de entrenamiento anterior para entrenarlo tú mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Descarga los archivos con nombres claros\n",
    "!wget -O ppo-bad.tar.gz \\\n",
    "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/8zCp__SHRSgGVlf5yP50Ag/ppo-bad-tar.gz\n",
    "!wget -O ppo-bad.pkl \\\n",
    "  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/jMW99Z9mvxesgYR-H6y6Yw/ppo-bad.pkl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pickle\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from trl.models import AutoModelForCausalLMWithValueHead\n",
    "\n",
    "# 1) Extraer el tar.gz en un directorio 'ppov3new_bad1'\n",
    "tar_path = \"ppo-bad.tar.gz\"\n",
    "extract_dir = \"ppov3new_bad1\"\n",
    "if not os.path.isdir(extract_dir):\n",
    "    with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "        tar.extractall(extract_dir)\n",
    "    print(f\"Contenido extraído en: {extract_dir}\")\n",
    "\n",
    "# 2) Función para encontrar la subcarpeta que contenga config.json\n",
    "def find_model_dir(base):\n",
    "    for entry in os.listdir(base):\n",
    "        path = os.path.join(base, entry)\n",
    "        if os.path.isdir(path) and \"config.json\" in os.listdir(path):\n",
    "            return path\n",
    "    # si no hay subcarpeta, asume que base ya tiene config.json\n",
    "    return base\n",
    "\n",
    "model_dir = find_model_dir(extract_dir)\n",
    "print(\"Cargando modelo desde:\", model_dir)\n",
    "\n",
    "# 3) Cargar modelo y tokenizer con local_files_only\n",
    "modelo = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    model_dir,\n",
    "    local_files_only=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_dir,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "# 4) Mover al dispositivo adecuado\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "modelo.to(device)\n",
    "print(\"Modelo cargado en:\", device)\n",
    "\n",
    "# 5) Cargar estadísticas de entrenamiento\n",
    "with open(\"ppo-bad.pkl\", \"rb\") as f:\n",
    "    all_stats = pickle.load(f)\n",
    "print(\"Estadísticas cargadas, número de epocas registrado:\", len(all_stats))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "extract_dir = \"ppov3new_bad1\"\n",
    "for root, dirs, files in os.walk(extract_dir):\n",
    "    if \"config.json\" in files:\n",
    "        print(\"→ Encontrado config.json en:\", root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "model_dir = \"ppov3new_bad1/ppov3new_bad1\"\n",
    "model_0 = AutoModelForCausalLMWithValueHead.from_pretrained(model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "# Carga estadísticas de entrenamiento\n",
    "file_name = \"ppo-bad.pkl\"\n",
    "with open(file_name, 'rb') as f:\n",
    "    all_stats = pickle.load(f)\n",
    "\n",
    "model_0.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Nota:** Puedes ignorar de forma segura la advertencia anterior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comparación de modelos con sentimiento negativo**\n",
    "\n",
    "El siguiente código compara el desempeño del modelo PPO entrenado (`model_0`) y el modelo de referencia sobre el conjunto de datos dado. La función `compare_models_on_dataset` genera respuestas de ambos modelos, calcula sus puntuaciones de sentimiento y devuelve los resultados en un DataFrame (`df_results`). Esta comparación ayuda a evaluar qué tan bien el modelo PPO genera respuestas negativas cuando `sentiment` está en **NEGATIVO**.\n",
    "\n",
    "Como el conjunto de datos es bastante grande, usaremos solo una muestra para la prueba.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = compare_models_on_dataset(model_0, ref_model, dataset, tokenizer, sentiment_pipe, sent_kwargs, device, output_length_sampler)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejercicio: Comparando modelos PPO**\n",
    "\n",
    "En este ejercicio, se pide comparar el desempeño de dos modelos PPO entrenados (`model_0` y `model_1`) usando la función `compare_models_on_dataset` y notarás la diferencia en su rendimiento.\n",
    "\n",
    "**Pasos para comparar modelos**:\n",
    "\n",
    "1. Llama a `compare_models_on_dataset` pasando `model_0` y `model_1`.\n",
    "2. Visualiza el DataFrame resultante para analizar las diferencias.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Escribe tu codigo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "prev_pub_hash": "febcb0ff319ab930e46d30d4d1bc1329ad2f8aa613c9a5ec96659fa44d3daf95"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
