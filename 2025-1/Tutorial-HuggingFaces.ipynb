{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5d48f65",
   "metadata": {},
   "source": [
    "### **Tutorial de Hugging Face**\n",
    "\n",
    "Hugging Face proporciona acceso a modelos (tanto el código que los implementa como sus pesos preentrenados, incluyendo los últimos LLMs como Llama3, DBRX, etc.), tokenizadores específicos de los modelos, así como pipelines para tareas comunes de NLP, y datasets y métricas en un paquete separado llamado `datasets`. Tiene implementaciones en PyTorch, Tensorflow y Flax.\n",
    "\n",
    "Vamos a repasar algunos casos de uso:\n",
    "\n",
    "* Descripción general de Tokenizers y modelos\n",
    "* Ajuste fino. Usaremos un ejemplo de clasificación de sentimientos.\n",
    "\n",
    "Se pueden aplicar a otros proyectos interesantes tambien:\n",
    "\n",
    "1. Aplicar un modelo preentrenado existente a una nueva aplicación o tarea y explorar cómo abordarlo/solucionarlo.\n",
    "2. Implementar una nueva o compleja arquitectura neural y demostrar su rendimiento en algunos datos.\n",
    "3. Analizar el comportamiento de un modelo: cómo representa el conocimiento lingüístico o qué tipo de fenómenos puede manejar o errores que comete.\n",
    "\n",
    "De estos, `transformers` será de mayor ayuda para (1) y para (3). (2) implica una forma muy conveniente diseñar un modelo basado en los existentes proporcionados por Hugging Face. No lo cubriremos aquí y por favor revisa a [este ejemplo](https://huggingface.co/docs/transformers/en/custom_models).\n",
    "\n",
    "Aquí hay recursos adicionales que introducen la librería que se utilizaron para hacer este cuaderno:\n",
    "\n",
    "* [Docs de Hugging Face](https://huggingface.co/docs/transformers/index)\n",
    "  * Documentación clara\n",
    "  * Tutoriales, recorridos y cuadernos de ejemplo\n",
    "  * Lista de modelos disponibles\n",
    "* [Curso de Hugging Face](https://huggingface.co/course/)\n",
    "* [Ejemplos de Hugging Face](https://github.com/huggingface/transformers/tree/main/examples/pytorch) Puedes encontrar estructuras de código muy similares en tareas/modelos descendentes muy diferentes usando Hugging Face.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffeeb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de las bibliotecas necesarias\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405187a2",
   "metadata": {},
   "source": [
    "Se escribe una función print_encoding diseñada para imprimir de manera legible el contenido de un diccionario,para mostrar las entradas del modelo después de la tokenización. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0626b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import json\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def print_encoding(model_inputs, indent=4):\n",
    "    indent_str = \" \" * indent\n",
    "    print(\"{\")\n",
    "    for k, v in model_inputs.items():\n",
    "        print(indent_str + k + \":\")\n",
    "        print(indent_str + indent_str + str(v))\n",
    "    print(\"}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7d4c78",
   "metadata": {},
   "source": [
    "### **Patrón común para usar Transformers de Hugging Face**\n",
    "\n",
    "Vamos a empezar con un patrón de uso común para Transformadores de Hugging Face, usando el ejemplo de análisis de sentimientos.\n",
    "\n",
    "Primero, encuentra un modelo en el [hub](https://huggingface.co/models) de Hugging Face. Cualquiera puede subir su modelo para que otras personas lo usen. (Estoy usando un modelo de análisis de sentimientos de [este artículo](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3489963)).\n",
    "\n",
    "Luego, hay dos objetos que necesitan ser inicializados: un **tokenizador** y un **modelo**\n",
    "\n",
    "* El tokenizador convierte cadenas en listas de IDs de vocabulario que el modelo requiere.\n",
    "* El modelo toma los IDs de vocabulario y produce una predicción.\n",
    "\n",
    "![full_nlp_pipeline.png](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg)\n",
    "De [https://huggingface.co/course/chapter2/2?fw=pt](https://huggingface.co/course/chapter2/2?fw=pt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b7c163",
   "metadata": {},
   "source": [
    "#### **RoBERTa**\n",
    "\n",
    "RoBERTa (Robustly optimized BERT approach) es un modelo de lenguaje preentrenado desarrollado por Facebook AI. Es una variante del modelo BERT (Bidirectional Encoder Representations from Transformers) con algunas mejoras en el entrenamiento que lo hacen más robusto y eficaz en diversas tareas de procesamiento de lenguaje natural (NLP).\n",
    "\n",
    "\n",
    "- Preentrenamiento con más datos: RoBERTa se entrena con más datos que BERT, lo que mejora su capacidad para capturar patrones y relaciones en el lenguaje.\n",
    "- Más pasos de entrenamiento: Realiza más pasos de entrenamiento para mejorar el aprendizaje del modelo.\n",
    "- Batch sizes más grandes: Utiliza lotes de datos más grandes durante el entrenamiento, lo que ayuda a estabilizar y mejorar el aprendizaje.\n",
    "- Sin enmascaramiento de próxima oración: A diferencia de BERT, RoBERTa elimina la tarea de predicción de la próxima oración, lo que simplifica el entrenamiento y se enfoca más en la predicción de palabras enmascaradas.\n",
    "\n",
    "El código siguiente utiliza el modelo RoBERTa preentrenado para la clasificación de secuencias, específicamente para la clasificación de sentimientos en inglés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257cd0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Inicializar el tokenizador\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"siebert/sentiment-roberta-large-english\")\n",
    "# Inicializar el modelo\n",
    "modelo = AutoModelForSequenceClassification.from_pretrained(\"siebert/sentiment-roberta-large-english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e5d6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = \"I'm  happy to learn about Hugging Face Transformers!\"\n",
    "tokenized_inputs = tokenizer(inputs, return_tensors=\"pt\")\n",
    "outputs = modelo(**tokenized_inputs)\n",
    "\n",
    "labels = ['NEGATIVE', 'POSITIVE']\n",
    "prediction = torch.argmax(outputs.logits)\n",
    "\n",
    "\n",
    "print(\"Entradas:\")\n",
    "print(inputs)\n",
    "print()\n",
    "print(\"Entrada tokenizada:\")\n",
    "print_encoding(tokenized_inputs)\n",
    "print()\n",
    "print(\"Salida del modelo:\")\n",
    "print(outputs)\n",
    "print()\n",
    "print(f\"La prediccion es {labels[prediction]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a9fb68",
   "metadata": {},
   "source": [
    "#### **Tokenizers (Tokenizadores)**\n",
    "\n",
    "Los modelos preentrenados se implementan junto con **tokenizadores** que se usan para preprocesar sus entradas. Los tokenizadores toman cadenas de texto o listas de cadenas y producen lo que son efectivamente diccionarios que contienen las entradas del modelo.\n",
    "\n",
    "Puedes acceder a los tokenizadores ya sea con la clase Tokenizer específica del modelo que deseas usar (aquí DistilBERT), o con la clase AutoTokenizer.\n",
    "Los Fast Tokenizers están escritos en Rust, mientras que sus versiones lentas están escritas en Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1171f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertTokenizerFast, AutoTokenizer\n",
    "name = \"distilbert/distilbert-base-cased\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b77af5",
   "metadata": {},
   "source": [
    "#### **DistilBERT**\n",
    "\n",
    "DistilBERT es una versión comprimida y optimizada del modelo BERT (Bidirectional Encoder Representations from Transformers). Fue desarrollado por Hugging Face con el objetivo de hacer que los modelos de lenguaje grandes sean más ligeros, rápidos y eficientes sin una pérdida significativa de rendimiento. DistilBERT se entrena utilizando un proceso llamado distillation (destilación), en el que un modelo más pequeño (el estudiante) aprende a reproducir el comportamiento de un modelo más grande (el maestro).\n",
    "\n",
    "**Características principales de DistilBERT**\n",
    "\n",
    "- Tamaño reducido: DistilBERT tiene aproximadamente la mitad de los parámetros de BERT base, lo que lo hace más ligero y fácil de desplegar en producción.\n",
    "- Velocidad: Debido a su menor tamaño, DistilBERT es más rápido tanto en entrenamiento como en inferencia.\n",
    "- Rendimiento: A pesar de ser más pequeño y rápido, DistilBERT mantiene alrededor del 97% de la precisión de BERT en una variedad de tareas de procesamiento de lenguaje natural.\n",
    "\n",
    "DistilBERT se entrena utilizando un método llamado destilación de conocimiento, que implica entrenar un modelo más pequeño para imitar el comportamiento de un modelo más grande. El proceso incluye:\n",
    "\n",
    "- Entrenamiento con pérdidas combinadas: El modelo estudiante se entrena con una combinación de la pérdida estándar (por ejemplo, pérdida de entropía cruzada) y la pérdida de distilación, que mide qué tan bien las predicciones del modelo estudiante coinciden con las del modelo maestro.\n",
    "- Conservación del conocimiento: El modelo estudiante aprende a conservar y replicar el conocimiento adquirido por el modelo maestro, pero de una manera más compacta y eficiente.\n",
    "\n",
    "El siguiente código muestra cómo inicializar y utilizar DistilBERT para tareas de tokenización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fee86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(name)      # escrito en Python\n",
    "print(tokenizer)\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(name)  # escrito en Rust\n",
    "print(tokenizer)\n",
    "tokenizer = AutoTokenizer.from_pretrained(name) # por defecto es Fast\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b835152",
   "metadata": {},
   "source": [
    "Este resultado muestra la configuración y las propiedades de diferentes tokenizadores de DistilBERT que se han inicializado con el nombre de modelo distilbert/distilbert-base-cased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10474e96",
   "metadata": {},
   "source": [
    "```\n",
    "DistilBertTokenizer(\n",
    "    name_or_path='distilbert/distilbert-base-cased',\n",
    "    vocab_size=28996,\n",
    "    model_max_length=512,\n",
    "    is_fast=False,\n",
    "    padding_side='right',\n",
    "    truncation_side='right',\n",
    "    special_tokens={\n",
    "        'unk_token': '[UNK]',\n",
    "        'sep_token': '[SEP]',\n",
    "        'pad_token': '[PAD]',\n",
    "        'cls_token': '[CLS]',\n",
    "        'mask_token': '[MASK]'\n",
    "    },\n",
    "    clean_up_tokenization_spaces=True\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e9c9d9",
   "metadata": {},
   "source": [
    "DistilBertTokenizerFast (dos veces con la misma configuración)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e155bafb",
   "metadata": {},
   "source": [
    "```\n",
    "DistilBertTokenizerFast(\n",
    "    name_or_path='distilbert/distilbert-base-cased',\n",
    "    vocab_size=28996,\n",
    "    model_max_length=512,\n",
    "    is_fast=True,\n",
    "    padding_side='right',\n",
    "    truncation_side='right',\n",
    "    special_tokens={\n",
    "        'unk_token': '[UNK]',\n",
    "        'sep_token': '[SEP]',\n",
    "        'pad_token': '[PAD]',\n",
    "        'cls_token': '[CLS]',\n",
    "        'mask_token': '[MASK]'\n",
    "    },\n",
    "    clean_up_tokenization_spaces=True\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dafa73",
   "metadata": {},
   "source": [
    "El resultado muestra que has inicializado tres tokenizadores para el modelo distilbert-base-cased:\n",
    "\n",
    "- DistilBertTokenizer: Este es el tokenizador estándar, escrito en Python, que no es tan rápido como su contraparte rápida, pero aún es ampliamente utilizado para tareas de NLP.\n",
    "- DistilBertTokenizerFast (dos veces con la misma configuración): Estos son los tokenizadores rápidos, escritos en Rust, que son más eficientes en términos de tiempo de ejecución. Aunque se muestran dos veces, ambos representan la misma configuración y funcionalidad, indicando que has inicializado el tokenizador rápido más de una vez."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0c9732",
   "metadata": {},
   "source": [
    "El código siguiente proporciona una demostración de cómo tokenizar una cadena de texto utilizando un tokenizador preentrenado de Hugging Face.\n",
    "\n",
    "La salida incluye los identificadores de los tokens y la máscara de atención, que son esenciales para el procesamiento por parte del modelo. Además, se demuestra cómo acceder a los tokens de dos maneras diferentes, resaltando la flexibilidad de los objetos devueltos por la biblioteca transformers.\n",
    "\n",
    "**Máscara de atención**\n",
    "\n",
    "Una máscara de atención (attention mask) es una herramienta utilizada en modelos de procesamiento de lenguaje natural, especialmente en arquitecturas de transformers, para indicar qué tokens (palabras o subpalabras) deben ser atendidos por el modelo y cuáles deben ser ignorados durante el proceso de atención.\n",
    "\n",
    "En el contexto de los transformers y modelos como BERT o DistilBERT, las secuencias de entrada suelen tener diferentes longitudes. Sin embargo, para procesarlas de manera eficiente en lotes (batches), las secuencias deben ser de la misma longitud. Esto se logra mediante el relleno (padding), que añade tokens especiales ([PAD]) al final de las secuencias más cortas para que todas alcancen la misma longitud. La máscara de atención se utiliza para asegurarse de que estos tokens de relleno no influyan en las predicciones del modelo.\n",
    "\n",
    "Algunas función de la máscara de atención son:\n",
    "\n",
    "- Indicar tokens relevantes: La máscara de atención señala qué tokens en la secuencia son relevantes y deben ser considerados por el modelo.\n",
    "- Ignorar tokens de relleno: La máscara de atención asegura que los tokens de relleno ([PAD]) añadidos a las secuencias más cortas sean ignorados durante el cálculo de la atención.\n",
    "\n",
    "La máscara de atención es una lista o tensor de la misma longitud que la secuencia de entrada tokenizada. Contiene valores binarios:\n",
    "\n",
    "- 1: Indica que el token correspondiente es relevante y debe ser atendido.\n",
    "- 0: Indica que el token correspondiente es un token de relleno y debe ser ignorado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4877a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Así es como llamas al tokenizador\n",
    "input_str = \"Hugging Face Transformers is great!\"\n",
    "tokenized_inputs = tokenizer(input_str) # https://huggingface.co/learn/nlp-course/en/chapter6/6\n",
    "\n",
    "print(\"Tokenización básica\")\n",
    "print_encoding(tokenized_inputs)\n",
    "print()\n",
    "\n",
    "# Dos formas de acceder:\n",
    "print(tokenized_inputs.input_ids)\n",
    "print(tokenized_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4981ae38",
   "metadata": {},
   "source": [
    "El código siguiente realiza una serie de pasos para tokenizar una cadena de texto, agregar tokens especiales (como los tokens de clasificación `[CLS]` y separación `[SEP])`, y luego decodificar los tokens de vuelta a texto.\n",
    "\n",
    "Estos pasos no crean la máscara de atención ni añaden los caracteres especiales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a05b7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = [tokenizer.cls_token_id]\n",
    "sep = [tokenizer.sep_token_id]\n",
    "\n",
    "# La tokenización ocurre en unos pocos pasos:\n",
    "input_tokens = tokenizer.tokenize(input_str)\n",
    "input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n",
    "input_ids_special_tokens = cls + input_ids + sep\n",
    "\n",
    "decoded_str = tokenizer.decode(input_ids_special_tokens)\n",
    "\n",
    "print(\"inicio:                \", input_str)\n",
    "print(\"tokeniza:             \", input_tokens)\n",
    "print(\"convert_tokens_to_ids:\", input_ids)\n",
    "print(\"agrega tokens especiales:   \", input_ids_special_tokens)\n",
    "print(\"--------\")\n",
    "print(\"decodifica:               \", decoded_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a151de27",
   "metadata": {},
   "source": [
    "El siguiente fragmento de código utiliza el FastTokenizer de la biblioteca transformers para tokenizar una cadena de texto y luego analiza los tokens resultantes en detalle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae106cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para Fast Tokenizer, hay otra opción también:\n",
    "inputs = tokenizer._tokenizer.encode(input_str)\n",
    "\n",
    "print(input_str)\n",
    "print(\"-\"*5)\n",
    "print(f\"Número de tokens: {len(inputs)}\")\n",
    "print(f\"Ids: {inputs.ids}\")\n",
    "print(f\"Tokens: {inputs.tokens}\")\n",
    "print(f\"Máscara de tokens especiales: {inputs.special_tokens_mask}\")\n",
    "print()\n",
    "print(\"char_to_word da el wordpiece de un carácter en la entrada\")\n",
    "char_idx = 8\n",
    "print(f\"Por ejemplo, el {char_idx + 1}º carácter de la cadena es '{input_str[char_idx]}',\"+\\\n",
    "      f\" y es parte del wordpiece {inputs.char_to_token(char_idx)}, '{inputs.tokens[inputs.char_to_token(char_idx)]}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd1de05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Otros trucos interesantes:\n",
    "# El tokenizador puede devolver tensores de pytorch\n",
    "model_inputs = tokenizer(\"¡Los Transformadores de Hugging Face son geniales!\", return_tensors=\"pt\")\n",
    "print(\"Tensores PyTorch:\")\n",
    "print_encoding(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f755801",
   "metadata": {},
   "source": [
    "El código siguiente demuestra cómo tokenizar y rellenar múltiples secuencias de texto para que tengan la misma longitud, lo cual es necesario para el procesamiento por lotes en modelos de transformers. \n",
    "\n",
    "También se muestra cómo se utilizan los tokens de relleno (padding) y las máscaras de atención para indicar qué partes de las secuencias son relevantes para el modelo.\n",
    "\n",
    "Esta técnica asegura que los modelos de lenguaje puedan procesar secuencias de longitud variable de manera eficiente y precisa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90500169",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer([\"Hugging Face Transformers is great!\",\n",
    "                         \"The quick brown fox jumps over the lazy dog.\" +\\\n",
    "                         \"Then the dog got up and ran away because she didn't like foxes.\",\n",
    "                         ],\n",
    "                         return_tensors=\"pt\",\n",
    "                         padding=True,\n",
    "                         truncation=True)\n",
    "print(f\"Token de relleno: {tokenizer.pad_token} | ID del token de relleno: {tokenizer.pad_token_id}\")\n",
    "print(\"Relleno (padding):\")\n",
    "print_encoding(model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19be9632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# También puedes decodificar un lote completo a la vez:\n",
    "print(\"Decodificación por Lote:\")\n",
    "print(tokenizer.batch_decode(model_inputs.input_ids))\n",
    "print()\n",
    "print(\"Decodificación por Lote: (sin caracteres especiales)\")\n",
    "print(tokenizer.batch_decode(model_inputs.input_ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099fa981",
   "metadata": {},
   "source": [
    "Para obtener más información sobre los tokenizadores, puedes consultar:\n",
    "[Hugging Face Transformers Docs](https://huggingface.co/docs/transformers/main_classes/tokenizer) y la [Hugging Face Tokenizers Library](https://huggingface.co/docs/tokenizers/python/latest/quicktour.html).\n",
    "\n",
    ">La librería de Tokenizers incluso te permite entrenar tus propios tokenizadores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f21f5ee",
   "metadata": {},
   "source": [
    "#### **Modelos**\n",
    "\n",
    "Inicializar un modelo es muy similar a inicializar un tokenizador. Puedes elegir:\n",
    "\n",
    "* **Clase concreta**, si conoces la arquitectura exacta que necesitas (por ejemplo, `DistilBertModel`, `BertForMaskedLM`, `GPT2Model`, etc.).\n",
    "* **Clases auto-configurables** (`AutoModel*`), cuando quieras flexibilidad o comparar varios modelos especificando simplemente su nombre como cadena.\n",
    "\n",
    "> **Recomendación:** usa `AutoModelForSequenceClassification`, `AutoModelForMaskedLM`, etc., para aprovechar el mapeo automático de heads según la tarea.\n",
    "\n",
    "Aunque la mayoría de los Transformers comparten una arquitectura base, tienen \"cabeceras\" (\"heads\") adicionales que debes entrenar para tareas específicas, como clasificación de secuencias, extracción de entidades, preguntas y respuestas, etc. Hugging Face configura automáticamente esas heads cuando eliges la clase apropiada:\n",
    "\n",
    "* **Clasificación de secuencias**:\n",
    "\n",
    "  ```python\n",
    "  from transformers import DistilBertForSequenceClassification\n",
    "  modelo = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "  ```\n",
    "* **Modelado de lenguaje enmascarado**:\n",
    "\n",
    "  ```python\n",
    "  from transformers import DistilBertForMaskedLM\n",
    "  modelo = DistilBertForMaskedLM.from_pretrained(\"distilbert-base-uncased\")\n",
    "  ```\n",
    "* **Representaciones generales**:\n",
    "\n",
    "  ```python\n",
    "  from transformers import DistilBertModel\n",
    "  modelo = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "  ```\n",
    "\n",
    "Aquí tienes los prefijos disponibles (donde `*` puede ser `AutoModel` o el nombre de un modelo específico):\n",
    "\n",
    "* **`*Model`**\n",
    "* **`*ForMaskedLM`**\n",
    "* **`*ForSequenceClassification`**\n",
    "* **`*ForTokenClassification`**\n",
    "* **`*ForQuestionAnswering`**\n",
    "* **`*ForMultipleChoice`**\n",
    "\n",
    "Y los tres grandes tipos de modelo:\n",
    "\n",
    "* **Encoders** (p. ej., `BertModel`, `DistilBertModel`)\n",
    "* **Decoders** (p. ej., `GPT2Model`)\n",
    "* **Encoder–Decoder** (p. ej., `BartModel`, `T5Model`)\n",
    "\n",
    "Una lista completa de clases y compatibilidades está disponible en los [docs de Transformers](https://huggingface.co/docs/transformers/model_doc/auto). Ten en cuenta que no todos los modelos soportan todas las tareas (por ejemplo, DistilBERT no es compatible con tareas Seq2Seq, ya que solo implementa un encoder).\n",
    "\n",
    "Aquí tienes una imagen de un modelo recreada a partir de una encontrada aquí: [https://huggingface.co/course/chapter2/2?fw=pt](https://huggingface.co/course/chapter2/2?fw=pt).\n",
    "![model_illustration.png](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head.svg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2c2ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, DistilBertForSequenceClassification, DistilBertModel\n",
    "print('Cargando el modelo base')\n",
    "modelo_base = DistilBertModel.from_pretrained('distilbert-base-cased')\n",
    "print(\"Cargando el modelo de clasificación desde el checkpoint del modelo base\")\n",
    "modelo = DistilBertForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=2)\n",
    "modelo = AutoModelForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ee7b7f",
   "metadata": {},
   "source": [
    "También puedes inicializar con pesos aleatorios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b3f890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertConfig, DistilBertModel\n",
    "\n",
    "# Inicializando una configuración de DistilBERT\n",
    "configuracion = DistilBertConfig()\n",
    "configuracion.num_labels=2\n",
    "# Inicializando un modelo (con pesos aleatorios) desde la configuración\n",
    "modelo = DistilBertForSequenceClassification(configuracion)\n",
    "\n",
    "# Accediendo a la configuración del modelo\n",
    "configuracion = modelo.config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf73a585",
   "metadata": {},
   "source": [
    "Pasar entradas al modelo es súper fácil. Este código realiza la inferencia utilizando un modelo de clasificación de secuencias entrenado con un texto tokenizado. Aquí:\n",
    "\n",
    "- Se convierte la cadena de entrada en tokens y los representa como tensores de PyTorch.\n",
    "- Los tokens se acompañan de una máscara de atención que indica qué tokens son relevantes.\n",
    "- Se realiza la inferencia usando los input_ids y attention_mask.\n",
    "- El modelo produce logits que representan las salidas antes de aplicar la función softmax.\n",
    "- Se aplica la función softmax a los logits para obtener probabilidades de clase.\n",
    "- Se interpreta la clase más probable basada en estas probabilidades.\n",
    "\n",
    "Este flujo de trabajo muestra cómo se utiliza un modelo de clasificación de secuencias preentrenado para hacer predicciones sobre una cadena de texto tokenizada. La salida incluye tanto los tokens de entrada como las predicciones del modelo en forma de logits y distribuciones de probabilidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92515745",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
    "# Opción 1\n",
    "model_outputs = modelo(input_ids=model_inputs.input_ids, attention_mask=model_inputs.attention_mask)\n",
    "\n",
    "# Opción 2 - las claves del diccionario que devuelve el tokenizador son las mismas que los argumentos de palabra clave\n",
    "#            que espera el modelo\n",
    "\n",
    "# f({k1: v1, k2: v2}) = f(k1=v1, k2=v2)\n",
    "\n",
    "model_outputs = modelo(**model_inputs)\n",
    "\n",
    "print(model_inputs)\n",
    "print()\n",
    "print(model_outputs)\n",
    "print()\n",
    "print(f\"Distribución sobre etiquetas: {torch.softmax(model_outputs.logits, dim=1)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b294bcf1",
   "metadata": {},
   "source": [
    "Si te das cuenta, es un poco extraño que tengamos dos clases para una tarea de clasificación binaria - podrías fácilmente tener una sola clase y simplemente elegir un umbral. Es así por cómo los modelos de huggingface calculan la pérdida. Esto aumentará el número de parámetros que tenemos, pero no debería afectar el rendimiento.\n",
    "\n",
    "Estos modelos son solo módulos de Pytorch. Puedes calcular la pérdida con tu `loss_func` y llamar a `loss.backward`. Puedes usar cualquiera de los optimizadores o planificadores de tasas de aprendizaje que usas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2722b1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puedes calcular la función de pérdida\n",
    "label = torch.tensor([1])\n",
    "loss = torch.nn.functional.cross_entropy(model_outputs.logits, label)\n",
    "print(loss)\n",
    "loss.backward()\n",
    "# Puedes obtener los parámetros\n",
    "list(modelo.named_parameters())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fef2da",
   "metadata": {},
   "source": [
    "Hugging Face proporciona una forma adicional fácil de calcular la pérdida también:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881b4bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para calcular la pérdida, necesitamos pasar una etiqueta:\n",
    "model_inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
    "\n",
    "labels = ['NEGATIVE', 'POSITIVE']\n",
    "model_inputs['labels'] = torch.tensor([1])\n",
    "\n",
    "model_outputs = modelo(**model_inputs)\n",
    "\n",
    "print(model_outputs)\n",
    "print()\n",
    "print(f\"Predicciones del modelo: {labels[model_outputs.logits.argmax()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58290886",
   "metadata": {},
   "source": [
    "Puedes obtener los estados ocultos y los pesos de atención de los modelos muy fácilmente. Esto es particularmente útil si estás trabajando en un proyecto de análisis. (Por ejemplo, ver [What does BERT look at?](https://arxiv.org/abs/1906.04341)).\n",
    "\n",
    "\n",
    "El código siguiente carga un modelo preentrenado de distilbert-base-cased utilizando la biblioteca transformers de Hugging Face. Luego, el modelo es utilizado para generar salidas ocultas y atenciones para un texto de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664de3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "modelo = AutoModel.from_pretrained(\"distilbert-base-cased\", output_attentions=True, output_hidden_states=True)\n",
    "modelo.eval()\n",
    "\n",
    "model_inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    model_output = modelo(**model_inputs)\n",
    "\n",
    "print(\"Tamaño del estado oculto (por capa):  \", model_output.hidden_states[0].shape)\n",
    "print(\"Tamaño del head de atención (por capa):\", model_output.attentions[0].shape)     # (capa, lote, índice_palabra_consulta, índices_palabra_clave)\n",
    "                                                                                       # eje y es consulta, eje x es clave\n",
    "# print(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e046ff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(model_inputs.input_ids[0])\n",
    "print(tokens)\n",
    "\n",
    "n_layers = len(model_output.attentions)\n",
    "n_heads = len(model_output.attentions[0][0])\n",
    "fig, axes = plt.subplots(6, 12)\n",
    "fig.set_size_inches(18.5*2, 10.5*2)\n",
    "for layer in range(n_layers):\n",
    "    for i in range(n_heads):\n",
    "        axes[layer, i].imshow(model_output.attentions[layer][0, i])\n",
    "        axes[layer][i].set_xticks(list(range(9)))\n",
    "        axes[layer][i].set_xticklabels(labels=tokens, rotation=\"vertical\")\n",
    "        axes[layer][i].set_yticks(list(range(9)))\n",
    "        axes[layer][i].set_yticklabels(labels=tokens)\n",
    "\n",
    "        if layer == 5:\n",
    "            axes[layer, i].set(xlabel=f\"head={i}\")\n",
    "        if i == 0:\n",
    "            axes[layer, i].set(ylabel=f\"layer={layer}\")\n",
    "\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983b4a94",
   "metadata": {},
   "source": [
    "La salida del código es una figura  con múltiples subplots organizados en una cuadrícula de 6 filas y 12 columnas. Cada subplot representa la matriz de atención de una cabecera de atención específica en una capa específica del modelo. En cada matriz de atención:\n",
    "\n",
    "- Eje X: Representa los tokens de la secuencia de entrada que actúan como claves.\n",
    "- Eje Y: Representa los tokens de la secuencia de entrada que actúan como consultas.\n",
    "- Valores: Los valores en la matriz indican cuánta atención está poniendo un token de consulta en cada token de clave. Un valor más alto indica más atención.\n",
    "\n",
    "Este tipo de visualización es útil para entender cómo el modelo está distribuyendo su atención en diferentes partes de la secuencia de entrada a través de sus múltiples capas y cabeceras de atención."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c9741a-7380-4140-a89a-c2326f8438fe",
   "metadata": {},
   "source": [
    "### **Ejemplo: Mini-pipeline de fine-tuning con Transformers**\n",
    "\n",
    "A continuación se muestra un componente explicativo, titulado que ilustra los pasos básicos para preparar un modelo para entrenamiento usando `datasets`, `DataCollator` y `Trainer` de Hugging Face.\n",
    "\n",
    "**Mini-pipeline de fine-tuning**\n",
    "\n",
    "1. **Cargar el dataset**\n",
    "   Usamos el conjunto de reseñas de IMDb desde `datasets`.\n",
    "\n",
    "   ```python\n",
    "   from datasets import load_dataset\n",
    "\n",
    "   raw_datasets = load_dataset(\"imdb\", split={\"train\": \"train[:1%]\", \"test\": \"test[:1%]\"})\n",
    "   ```\n",
    "\n",
    "2. **Tokenizar**\n",
    "   Inicializamos un tokenizador y aplicamos la tokenización en lote:\n",
    "\n",
    "   ```python\n",
    "   from transformers import AutoTokenizer\n",
    "\n",
    "   tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "   def tokenize_batch(batch):\n",
    "       return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "   tokenized = raw_datasets.map(tokenize_batch, batched=True)\n",
    "   ```\n",
    "\n",
    "3. **Data Collator**\n",
    "   Configuramos un collator que ajuste dinámicamente el padding por batch:\n",
    "\n",
    "   ```python\n",
    "   from transformers import DataCollatorWithPadding\n",
    "\n",
    "   data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "   ```\n",
    "\n",
    "4. **Inicializar el modelo**\n",
    "   Cargamos un modelo para clasificación de secuencias:\n",
    "\n",
    "   ```python\n",
    "   from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "   modelo = AutoModelForSequenceClassification.from_pretrained(\n",
    "       \"distilbert-base-uncased\", num_labels=2\n",
    "   )\n",
    "   ```\n",
    "\n",
    "5. **Configurar el Trainer**\n",
    "   Preparamos los argumentos de entrenamiento y creamos el `Trainer`:\n",
    "\n",
    "   ```python\n",
    "   from transformers import Trainer, TrainingArguments\n",
    "\n",
    "   training_args = TrainingArguments(\n",
    "       output_dir=\"./results\",\n",
    "       per_device_train_batch_size=8,\n",
    "       per_device_eval_batch_size=8,\n",
    "       logging_steps=10,\n",
    "       evaluation_strategy=\"steps\",\n",
    "       eval_steps=50,\n",
    "   )\n",
    "\n",
    "   trainer = Trainer(\n",
    "       model=modelo,\n",
    "       args=training_args,\n",
    "       train_dataset=tokenized[\"train\"],\n",
    "       eval_dataset=tokenized[\"test\"],\n",
    "       data_collator=data_collator,\n",
    "       tokenizer=tokenizer,\n",
    "   )\n",
    "   ```\n",
    "\n",
    "> **Nota:** aquí hemos omitido el bucle de entrenamiento explícito. Para iniciar el fine-tuning bastaría con llamar `trainer.train()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17e1359-5cd8-4225-9f45-d9cb2962177e",
   "metadata": {},
   "source": [
    "#### **Ejemplo: Inferencia rápida con pipelines de Transformers**\n",
    "\n",
    "**Análisis de sentimiento**\n",
    "\n",
    "Usando el pipeline de análisis de sentimientos, en unas pocas líneas obtienes la polaridad de cualquier texto:\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# Inicializamos el pipeline\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Ejemplo de uso\n",
    "resultados = sentiment_analyzer(\n",
    "    [\n",
    "        \"¡Este tutorial es fantástico!\",\n",
    "        \"No me gustó la última película que vi.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "for res in resultados:\n",
    "    print(f\"Etiqueta: {res['label']}, Confianza: {res['score']:.2f}\")\n",
    "```\n",
    "\n",
    "**Respuesta a preguntas**\n",
    "\n",
    "Para responder preguntas a partir de un contexto dado, usamos el pipeline de preguntas y respuestas:\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# Inicializamos el pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "\n",
    "# Contexto y pregunta\n",
    "contexto = (\n",
    "    \"Hugging Face es una empresa que desarrolla tecnologías open-source \"\n",
    "    \"para procesamiento de lenguaje natural. Su librería Transformers \"\n",
    "    \"proporciona acceso a modelos como BERT, GPT y T5.\"\n",
    ")\n",
    "pregunta = \"¿Qué librería proporciona acceso a modelos como BERT y GPT?\"\n",
    "\n",
    "# Ejecutamos la inferencia\n",
    "respuesta = qa_pipeline(question=pregunta, context=contexto)\n",
    "\n",
    "print(f\"Respuesta: {respuesta['answer']}\")\n",
    "print(f\"Puntaje: {respuesta['score']:.2f}\")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09a904f-47f8-4f6e-8608-3c02e701e6ea",
   "metadata": {},
   "source": [
    "### **Ejercicios**\n",
    "\n",
    "**Ejercicio 1: Identificación de clases**\n",
    "\n",
    "Para cada uno de estos casos, indica qué clase de modelo usarías (p. ej. `AutoModelForTokenClassification`, `BertModel`, etc.):\n",
    "\n",
    "1. Obtener representaciones ocultas de frases para un downstream propio.\n",
    "2. Entrenar un modelo para etiquetado de entidades (NER).\n",
    "3. Fine-tuning de un modelo para responder preguntas tipo SQuAD.\n",
    "4. Continuar preentrenamiento de BERT usando MLM.\n",
    "5. Clasificar reseñas de producto en positivo/negativo.\n",
    "\n",
    "**Entrega:** una tabla con dos columnas: \"caso\" y \"clase recomendada\".\n",
    "\n",
    "\n",
    "**Ejercicio 2: Código de inicialización**\n",
    "\n",
    "Escribe snippets de Python que carguen desde el hub el modelo `bert-base-cased` para:\n",
    "\n",
    "a) Clasificación de secuencias\n",
    "b) Modelado de lenguaje enmascarado\n",
    "c) Extracción de representaciones (\"feature‐extraction\")\n",
    "\n",
    "Incluye la importación y la llamada a `from_pretrained`, y comenta brevemente cuándo usarías cada uno.\n",
    "\n",
    "\n",
    "**Ejercicio 3: Comparativa AutoModel vs clase específica**\n",
    "\n",
    "1. Carga `bert-base-uncased` como `AutoModelForSequenceClassification` y como `BertForSequenceClassification`.\n",
    "2. Escribe un pequeño script que imprima el número total de parámetros de cada uno y comprueba si difieren.\n",
    "3. Explica por qué querrías usar una u otra en un entorno de producción.\n",
    "\n",
    "\n",
    "**Ejercicio 4: Debug de heads**\n",
    "\n",
    "Supón que al intentar entrenar:\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForMaskedLM\n",
    "modelo = AutoModelForMaskedLM.from_pretrained(\"distilbert-base-uncased\")\n",
    "```\n",
    "\n",
    "obtienes un error de \"size mismatch\" en la cabecera de MLM.\n",
    "\n",
    "* ¿Cuál puede ser la causa?\n",
    "* ¿Cómo lo resolverías sin cambiar la arquitectura base?\n",
    "\n",
    "**Ejercicio 5: Construcción de un modelo encoder–decoder**\n",
    "\n",
    "1. Elige un modelo Seq2Seq (p. ej. T5 o BART) y carga sus componentes encoder y decoder por separado usando las clases específicas.\n",
    "2. Muestra con un snippet cómo extraer la representación del encoder y luego pasarla al decoder.\n",
    "3. Describe en qué escenarios reales usarías este flujo manual en lugar de `AutoModelForSeq2SeqLM`.\n",
    "\n",
    "**Ejercicio 6: Experimento de fine-tuning**\n",
    "\n",
    "En un dataset pequeño (puede ser IMDB o alguno de `datasets`), realiza un fine-tuning rápido de `distilbert-base-uncased` para clasificación binaria:\n",
    "\n",
    "1. Prepara el tokenizador y codifica el dataset.\n",
    "2. Inicializa `AutoModelForSequenceClassification`.\n",
    "3. Escribe el bucle de entrenamiento (al menos un epoch) con `AdamW`.\n",
    "4. Reporta accuracy en validación.\n",
    "\n",
    "**Bonus:** Compara tiempo y precisión entrenando con la clase específica (`DistilBertForSequenceClassification`) y con la auto‐configurable.\n",
    "\n",
    "**Ejercicio 7: Visualiza los pesos de atención de un modelo preentrenado**\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Carga el modelo bert-base-uncased con la opción output_attentions=True.\n",
    "- Usa una frase de tu elección y pasa por el modelo para obtener los pesos de atención.\n",
    "- Visualiza los pesos de atención usando matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000699cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5fb49d-5e7c-4863-bf35-d9c674e11e1d",
   "metadata": {},
   "source": [
    "### **Low-Rank Adaptation (LoRA)**\n",
    "\n",
    "\n",
    "Low-Rank Adaptation (LoRA) es una técnica de *fine-tuning* eficiente que introduce matrices de baja rango en las capas de atención y feed-forward de un modelo preentrenado, en lugar de actualizar todos los parámetros.\n",
    "\n",
    "* **Motivación**: reducir el número de parámetros entrenables y la memoria necesaria.\n",
    "* **Idea clave**: en lugar de actualizar la matriz $W \\in \\mathbb{R}^{d \\times k}$, se factoriza la actualización $\\Delta W = A B$, con $A \\in \\mathbb{R}^{d \\times r}$, $B \\in \\mathbb{R}^{r \\times k}$ y $r \\ll \\min(d, k)$.\n",
    "\n",
    "**Ventajas de LoRA**\n",
    "\n",
    "* **Eficiencia de memoria**: solo almacenas $A$ y $B$, no todo $\\Delta W$.\n",
    "* **Velocidad de entrenamiento**: menos gradientes y parámetros a actualizar.\n",
    "* **Compartición de pesos base**: el modelo original se mantiene intacto, facilitando combinaciones de varios LoRAs.\n",
    "\n",
    "**Cómo aplicar LoRA con Hugging Face**\n",
    "\n",
    "\n",
    "```bash\n",
    "pip install peft transformers accelerate\n",
    "```\n",
    "\n",
    "**Flujo básico de trabajo**\n",
    "\n",
    "1. **Carga de modelo y tokenizador**\n",
    "\n",
    "   ```python\n",
    "   from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "   from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "   model_name = \"bert-base-uncased\"\n",
    "   tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "   base_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "   ```\n",
    "\n",
    "2. **Configuración de LoRA**\n",
    "\n",
    "   ```python\n",
    "   peft_config = LoraConfig(\n",
    "       task_type=TaskType.SEQ_CLS,  # tipo de tarea\n",
    "       r=8,                         # rango bajo\n",
    "       alpha=16,                    # escala de LoRA\n",
    "       target_modules=[\"query\", \"value\"],  # capas a adaptar\n",
    "       lora_dropout=0.05\n",
    "   )\n",
    "   ```\n",
    "\n",
    "3. **Construcción del modelo LoRA**\n",
    "\n",
    "   ```python\n",
    "   lora_model = get_peft_model(base_model, peft_config)\n",
    "   ```\n",
    "\n",
    "4. **Fine-tuning**\n",
    "\n",
    "   ```python\n",
    "   from transformers import Trainer, TrainingArguments\n",
    "\n",
    "   training_args = TrainingArguments(\n",
    "       output_dir=\"./lora_results\",\n",
    "       per_device_train_batch_size=16,\n",
    "       num_train_epochs=3,\n",
    "       logging_steps=10,\n",
    "       save_total_limit=2,\n",
    "   )\n",
    "   trainer = Trainer(\n",
    "       model=lora_model,\n",
    "       args=training_args,\n",
    "       train_dataset=tokenized_train,\n",
    "       eval_dataset=tokenized_eval,\n",
    "       tokenizer=tokenizer,\n",
    "   )\n",
    "   trainer.train()\n",
    "   ```\n",
    "\n",
    "5. **Guardado y carga**\n",
    "\n",
    "   ```python\n",
    "   lora_model.save_pretrained(\"./lora_adapter\")\n",
    "   # Para reusar:\n",
    "   from peft import PeftModel\n",
    "   loaded = PeftModel.from_pretrained(base_model, \"./lora_adapter\")\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51394ba3-723f-41b0-a1b4-1998d0122622",
   "metadata": {},
   "source": [
    "#### **Cuantización (Quantization) con bitsandbytes y Optimum**\n",
    "\n",
    "\n",
    "La cuantización convierte pesos de punto flotante (FP32) a representaciones de menor precisión (p. ej., INT8, INT4) para:\n",
    "\n",
    "* **Reducir memoria**: los modelos ocupan 2-4× menos espacio.\n",
    "* **Acelerar inferencia**: menos datos a mover en GPU/CPU.\n",
    "* **Mantener precisión**: las bibliotecas modernas minimizan la pérdida de calidad.\n",
    "\n",
    "**bitsandbytes (bnb)**\n",
    "\n",
    "1. **Instalación**\n",
    "\n",
    "   ```bash\n",
    "   pip install bitsandbytes\n",
    "   ```\n",
    "\n",
    "2. **Carga de un modelo cuantizado**\n",
    "\n",
    "   ```python\n",
    "   from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "   tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "   model_8bit = AutoModelForSequenceClassification.from_pretrained(\n",
    "       \"bert-base-uncased\",\n",
    "       load_in_8bit=True,            # activa cuantización a 8-bit\n",
    "       device_map=\"auto\"             # mapea capas a GPUs/CPU automáticamente\n",
    "   )\n",
    "   ```\n",
    "\n",
    "3. **Uso en inferencia**\n",
    "\n",
    "   ```python\n",
    "   from transformers import pipeline\n",
    "\n",
    "   sentiment = pipeline(\"sentiment-analysis\", model=model_8bit, tokenizer=tokenizer)\n",
    "   print(sentiment(\"La cuantización me permite ahorrar memoria.\"))\n",
    "   ```\n",
    "\n",
    "**Hugging Face Optimum**\n",
    "\n",
    "Optimum es una extensión oficial de Hugging Face para optimización en hardware diverso.\n",
    "\n",
    "1. **Instalación**\n",
    "\n",
    "   ```bash\n",
    "   pip install optimum optimum-bnb\n",
    "   ```\n",
    "\n",
    "2. **QuantizationConfig y cuantización con Optimum**\n",
    "\n",
    "   ```python\n",
    "   from optimum.bnb import BitsAndBytesConfig\n",
    "   from optimum.bnb import BNBFineTuner\n",
    "\n",
    "   bnb_config = BitsAndBytesConfig(\n",
    "       load_in_4bit=True,      # o load_in_8bit\n",
    "       bnb_4bit_quant_type=\"nf4\",\n",
    "       bnb_4bit_use_double_quant=True\n",
    "   )\n",
    "\n",
    "   tuner = BNBFineTuner(\n",
    "       model_name_or_path=\"bert-base-uncased\",\n",
    "       quantization_config=bnb_config,\n",
    "       output_dir=\"./bnb_quant\"\n",
    "   )\n",
    "   tuner.quantize()\n",
    "   model_4bit = tuner.get_quantized_model()\n",
    "   ```\n",
    "\n",
    "3. **Beneficios de Optimum + bnb**\n",
    "\n",
    "   * Compatibilidad con ONNX y aceleradores especializados.\n",
    "   * Pipelines automatizados para cuantización y benchmarking.\n",
    "   * Integración con **Accelerate** para despliegue escalable.\n",
    "\n",
    "Con estas técnicas, podrás entrenar y desplegar LLMs de gran tamaño de forma más eficiente en recursos y coste, manteniendo alta calidad en tus aplicaciones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594fbf59-1ff9-4ab0-b410-d99c019c48bb",
   "metadata": {},
   "source": [
    "### **Uso de LLMs: Generación de texto con Llama 3 y DBRX**\n",
    "\n",
    "A continuación tienes un ejemplo ligero de cómo cargar un modelo de generación grande (Llama 3 o DBRX) y realizar un prompt básico, optimizado para consumir pocos recursos gracias a la cuantización en 8 bits y al mapeo automático de dispositivos.\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# 1. Cargar pipeline de generación en 8-bit\n",
    "generator_llama3 = pipeline(\n",
    "    \"text-generation\",\n",
    "    modelo=\"meta-llama/Llama-3b\",        # Variante de 3 mil millones de parámetros\n",
    "    tokenizer=\"meta-llama/Llama-3b\",\n",
    "    device_map=\"auto\",                   # Asigna capas a GPU/CPU según disponibilidad\n",
    "    load_in_8bit=True,                   # Cuantiza pesos a 8-bits para reducir memoria\n",
    ")\n",
    "\n",
    "# 2. Prompt de ejemplo\n",
    "prompt = (\n",
    "    \"Eres un asistente experto en biología molecular. \"\n",
    "    \"Explica brevemente qué es la transcripción del ADN.\"\n",
    ")\n",
    "\n",
    "# 3. Generación\n",
    "outputs = generator_llama3(\n",
    "    prompt,\n",
    "    max_new_tokens=60,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "print(outputs[0][\"generated_text\"])\n",
    "```\n",
    "\n",
    "> **Tip de eficiencia**:\n",
    ">\n",
    "> * `load_in_8bit=True` ahorra \\~4× en memoria.\n",
    "> * `device_map=\"auto\"` reparte el modelo entre GPU(s) y CPU para evitar OOM.\n",
    "> * Ajusta `max_new_tokens` y `temperature` según necesidad de creatividad y longitud.\n",
    "\n",
    "**DBRX**\n",
    "\n",
    "```python\n",
    "# Ejemplo similar con un modelo DBRX (reemplaza el identificador por el disponible en HF Hub)\n",
    "generator_dbrx = pipeline(\n",
    "    \"text-generation\",\n",
    "    modelo=\"dbrx/DbrX-medium\",\n",
    "    tokenizer=\"dbrx/DbrX-medium\",\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "prompt2 = \"Resume en tres líneas las ventajas de la energía solar.\"\n",
    "out2 = generator_dbrx(prompt2, max_new_tokens=40, do_sample=False)\n",
    "print(out2[0][\"generated_text\"])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72efd6d6-4f92-4c1a-a43b-3b4fd455bca3",
   "metadata": {},
   "source": [
    "#### **Ejercicios**\n",
    "\n",
    "#### **Ejercicios LoRA**\n",
    "\n",
    "1. **Calcular ahorro de parámetros**\n",
    "\n",
    "   * Dado un modelo con una capa de tamaño $d\\times k$, elige tres valores de rango $r$ (p. ej. 4, 16, 32).\n",
    "   * Calcula cuántos parámetros adicionales introduce LoRA en cada caso y qué porcentaje representa respecto al fine-tuning completo.\n",
    "\n",
    "2. **Selección de módulos**\n",
    "\n",
    "   * Enumera al menos cuatro submódulos de un transformer (query, key, value, dense, etc.).\n",
    "   * Para cada uno, argumenta brevemente (2-3 líneas) si lo adaptarías con LoRA para una tarea de clasificación de texto corto.\n",
    "\n",
    "3. **Diseño de experimento sencillo**\n",
    "\n",
    "   * Diseña un experimento de validación cruzada (por ejemplo, 3-fold) comparando fine-tuning completo vs. LoRA con $r=8$.\n",
    "   * Especifica las métricas a recoger (accuracy, tiempo de entrenamiento, uso de memoria).\n",
    "\n",
    "\n",
    "#### **Ejercicios de cuantización**\n",
    "\n",
    "1. **Conversión teórica a 8-bits**\n",
    "\n",
    "   * Explica en 3–4 líneas qué sucede con la representación de los pesos al pasar de FP32 a INT8.\n",
    "   * Describe un posible efecto adverso en la calidad de inferencia.\n",
    "\n",
    "2. **Planificación de benchmark**\n",
    "\n",
    "   * Propón un mini-benchmark para medir la velocidad de inferencia en un modelo cuantizado vs. original (sin escribir código, solo pasos y métricas).\n",
    "\n",
    "3. **Combinación con LoRA**\n",
    "\n",
    "   * Esboza un flujo donde primero aplicas LoRA y luego cuantizas el modelo resultante.\n",
    "   * ¿En qué orden y por qué?\n",
    "\n",
    "\n",
    "#### **Ejercicios con Llama 3 y DBRX**\n",
    "\n",
    "1. **Selección de modelo en el Hub**\n",
    "\n",
    "   * Navega el Hugging Face Hub y elige un identificador de modelo Llama 3 y otro de DBRX.\n",
    "   * Anota la configuración recomendada (parámetros, licencia, tamaño).\n",
    "\n",
    "2. **Diseño de prompt comparativo**\n",
    "\n",
    "   * Define dos prompts breves (uno educativo y otro creativo).\n",
    "   * Explica cómo variarías `temperature` y `max_new_tokens` para cada prompt y por qué.\n",
    "\n",
    "3. **Análisis de salida**\n",
    "\n",
    "   * Genera dos respuestas (una con Llama 3 y otra con DBRX) para el mismo prompt.\n",
    "   * Anota en una tabla comparativa diferencias en coherencia, longitud y estilo (sin código, solo describiendo).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8dee52-dcb9-4d59-abe0-a2834d90e6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
