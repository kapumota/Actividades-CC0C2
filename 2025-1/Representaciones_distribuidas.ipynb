{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Representaciones distribuidas**\n",
    "\n",
    "Las representaciones distribuidas han transformado el campo del procesamiento de lenguaje natural (NLP) y el aprendizaje automático. A diferencia de enfoques basados en codificaciones locales, como one-hot encoding, que generan vectores dispersos de alta dimensión con valores mayoritariamente en cero, las representaciones distribuidas aprenden vectores densos de dimensión fija en un espacio continuo. \n",
    "\n",
    "Gracias a la hipótesis distributiva, según la cual palabras con contextos similares comparten significados próximos, estos métodos capturan relaciones semánticas complejas, optimizan el uso de memoria y facilitan el aprendizaje de significados de palabras nuevas o raras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Características principales**\n",
    "\n",
    "Veamos algunos características de estos métodos:\n",
    "\n",
    "- A diferencia de las representaciones locales, las distribuidas pueden capturar relaciones complejas entre palabras, como sinónimos, antónimos o términos que suelen aparecer en contextos similares.\n",
    "\n",
    "- Al representar palabras como vectores de tamaño fijo en un espacio continuo, se reduce la dimensionalidad del problema comparado con métodos de representación más simples pero de alta dimensionalidad, como el one-hot encoding.\n",
    "\n",
    "- Estos modelos pueden generalizar para entender palabras nuevas o raras a partir de sus componentes (por ejemplo, entender palabras compuestas a partir de los significados de sus partes).\n",
    "\n",
    "**Ejemplos y modelos**\n",
    "\n",
    "- Word2Vec: Probablemente el ejemplo más conocido de representaciones distribuidas. Word2Vec utiliza redes neuronales para aprender representaciones vectoriales de palabras a partir de grandes conjuntos de datos de texto. Ofrece dos arquitecturas principales: CBOW (Continuous Bag of Words) y Skip-gram, cada una diseñada para aprender representaciones que predigan palabras en función de sus contextos o viceversa.\n",
    "\n",
    "- GloVe (Global Vectors for Word Representation): Un modelo que aprende representaciones de palabras a partir de las estadísticas co-ocurrenciales de palabras en un corpus. La idea es que las relaciones semánticas entre palabras pueden ser capturadas observando qué tan frecuentemente aparecen juntas en un gran corpus.\n",
    "\n",
    "- Embeddings contextuales: Modelos más recientes como ELMo, BERT y GPT ofrecen una evolución de las representaciones distribuidas, generando vectores de palabras que varían según el contexto en el que aparecen, lo que permite capturar usos y significados múltiples de una misma palabra dependiendo de la oración en la que se encuentre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Embeddings de palabras**\n",
    "\n",
    "Los embeddings de palabras son representaciones vectoriales densas y de baja dimensión de palabras, diseñadas para capturar el significado semántico, sintáctico y relaciones entre ellas. A diferencia de las representaciones de texto más antiguas, como el one-hot encoding, que son dispersas (la mayoría de los valores son cero) y de alta dimensión, los embeddings de palabras se representan en un espacio vectorial continuo donde palabras con significados similares están ubicadas cercanamente en el espacio vectorial.\n",
    "\n",
    "**Características de los embeddings de palabras**\n",
    "\n",
    "- Cada palabra se representa como un vector denso, lo que significa que cada dimensión tiene un valor real, a diferencia de los vectores dispersos de otras técnicas de representación.\n",
    "\n",
    "- Los embeddings generalmente tienen un tamaño de dimensión fijo y relativamente pequeño (por ejemplo, 100, 200, 300 dimensiones) independientemente del tamaño del vocabulario.\n",
    "\n",
    "- Estos vectores intentan capturar el contexto y el significado de una palabra, no solo su presencia o ausencia. Palabras que se usan en contextos similares tendrán embeddings similares.\n",
    "\n",
    "- Pueden ayudar a los modelos de aprendizaje automático a generalizar mejor a palabras no vistas durante el entrenamiento, dado que las palabras con significados similares se mapean a puntos cercanos en el espacio vectorial.\n",
    "\n",
    "\n",
    "En 2013, un trabajo fundamental de Mikolov [Efficient Estimationof Word Representations in Vector Space](https://arxiv.org/abs/1301.3781) demostraron que su modelo de representación de palabras basado en una red neuronal conocido como `Word2vec`, basado en la `similitud distributiva`, puede capturar relaciones de analogía de palabras como: \n",
    "\n",
    "$$King - Man + Woman \\approx Queen$$\n",
    "\n",
    "Conceptualmente, Word2vec toma un gran corpus de texto como entrada y \"aprende\" a representar las palabras en un espacio vectorial común en función de los contextos en los que aparecen en el corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# 1. Corpus de ejemplo\n",
    "sentences = [\n",
    "    ['king', 'queen', 'man', 'woman'],\n",
    "    ['man', 'king', 'prince'],\n",
    "    ['woman', 'queen', 'princess'],\n",
    "    ['prince', 'son', 'king'],\n",
    "    ['princess', 'daughter', 'queen']\n",
    "]\n",
    "\n",
    "# 2. Construir vocabulario\n",
    "words = [w for sent in sentences for w in sent]\n",
    "vocab = {w: i for i, (w, _) in enumerate(Counter(words).items())}\n",
    "inv_vocab = {i: w for w, i in vocab.items()}\n",
    "V = len(vocab)\n",
    "\n",
    "# 3. Generar pares (target, context)\n",
    "window_size = 2\n",
    "pairs = []\n",
    "for sent in sentences:\n",
    "    indices = [vocab[w] for w in sent]\n",
    "    for center_pos, center in enumerate(indices):\n",
    "        for w in range(-window_size, window_size + 1):\n",
    "            context_pos = center_pos + w\n",
    "            if context_pos < 0 or context_pos >= len(indices) or context_pos == center_pos:\n",
    "                continue\n",
    "            pairs.append((center, indices[context_pos]))\n",
    "\n",
    "# 4. Modelo Skip‑gram simple\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        super().__init__()\n",
    "        self.u_embeddings = nn.Embedding(vocab_size, emb_dim, sparse=True)\n",
    "        self.v_embeddings = nn.Embedding(vocab_size, emb_dim, sparse=True)\n",
    "\n",
    "    def forward(self, u, v):\n",
    "        u_emb = self.u_embeddings(u)       # (batch, emb_dim)\n",
    "        v_emb = self.v_embeddings(v)       # (batch, emb_dim)\n",
    "        score = torch.mul(u_emb, v_emb).sum(dim=1)  # dot product\n",
    "        return score\n",
    "\n",
    "# 5. Entrenamiento\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "modelo = SkipGram(vocab_size=V, emb_dim=50).to(device)\n",
    "optimizer = optim.SGD(modelo.parameters(), lr=0.01)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# función para muestrear negativos\n",
    "def negative_sampling(batch_size, K):\n",
    "    negs = torch.randint(0, V, (batch_size * K,), device=device)\n",
    "    return negs\n",
    "\n",
    "epochs = 200\n",
    "K = 5  # número de negativos por par positivo\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    random.shuffle(pairs)\n",
    "    for center, context in pairs:\n",
    "        u = torch.tensor([center], dtype=torch.long, device=device)\n",
    "        v_pos = torch.tensor([context], dtype=torch.long, device=device)\n",
    "        # positivos → etiqueta 1\n",
    "        pos_score = modelo(u, v_pos)\n",
    "        pos_loss  = criterion(pos_score, torch.ones_like(pos_score))\n",
    "        # negativos → etiqueta 0\n",
    "        neg_v = negative_sampling(batch_size=1, K=K)\n",
    "        neg_score = modelo(u.repeat(K), neg_v)\n",
    "        neg_loss  = criterion(neg_score, torch.zeros_like(neg_score))\n",
    "        # retroprop y optimización\n",
    "        loss = pos_loss + neg_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(f\"Epoca {epoch+1}/{epochs}, Loss: {total_loss/len(pairs):.4f}\")\n",
    "\n",
    "# 6. Obtener embeddings y analogías\n",
    "embeddings = modelo.u_embeddings.weight.data.cpu()\n",
    "\n",
    "def most_similar(word, topn=3):\n",
    "    idx = vocab[word]\n",
    "    query = embeddings[idx]\n",
    "    # similitud coseno\n",
    "    cos = torch.nn.functional.cosine_similarity(query.unsqueeze(0), embeddings)\n",
    "    best = torch.topk(cos, topn+1).indices.tolist()[1:]  # excluye la misma palabra\n",
    "    return [(inv_vocab[i], cos[i].item()) for i in best]\n",
    "\n",
    "print(\"\\nSimilares a 'king':\", most_similar('king'))\n",
    "print(\"Analogía (king - man + woman):\")\n",
    "# v = king - man + woman\n",
    "vec = embeddings[vocab['king']] - embeddings[vocab['man']] + embeddings[vocab['woman']]\n",
    "cos = torch.nn.functional.cosine_similarity(vec.unsqueeze(0), embeddings)\n",
    "best = torch.topk(cos, 3).indices.tolist()\n",
    "for i in best:\n",
    "    print(f\"  {inv_vocab[i]}: {cos[i].item():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Embeddings de palabras pre-entrenadas**\n",
    "\n",
    "Podemos embeddings de Word2vec previamente entrenadas y buscar las palabras más similares (clasificadas por similitud de coseno) a una palabra determinada. \n",
    "\n",
    "Tomemos un ejemplo de un modelo word2vec previamente entrenado y cómo podemos usarlo para buscar la mayoría de las palabras similares. Usaremos los embeddings de vectores de Google News. https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
    "\n",
    "Se pueden encontrar algunos otros modelos de embeddings de palabras previamente entrenados y detalles sobre los medios para acceder a ellos a través de gensim en: https://github.com/RaRe-Technologies/gensim-data\n",
    "\n",
    "El código que sigue cubre los pasos clave. Aquí encontramos las palabras que semánticamente son más similares a la palabra \"beautiful\"; la última línea devuelve el vector de embeddings de la palabra \" beautiful \":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from heapq import nlargest\n",
    "import numpy as np\n",
    "\n",
    "# 1. Carga el modelo con vectores de tamaño 300\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# 2. Obtén el vector de \"beautiful\"\n",
    "v_beautiful = nlp.vocab[\"beautiful\"].vector\n",
    "\n",
    "# 3. Calcula similitud coseno con todo el vocabulario\n",
    "sims = []\n",
    "norm_beautiful = np.linalg.norm(v_beautiful)\n",
    "for lex in nlp.vocab:\n",
    "    if lex.has_vector and not lex.is_stop and lex.is_alpha:\n",
    "        v = lex.vector\n",
    "        score = np.dot(v_beautiful, v) / (norm_beautiful * np.linalg.norm(v))\n",
    "        sims.append((lex.text, float(score)))\n",
    "\n",
    "# 4. Top‑10 más similares\n",
    "top10 = nlargest(10, sims, key=lambda x: x[1])\n",
    "print(\"Top 10 palabras similares a 'beautiful':\")\n",
    "for w, score in top10:\n",
    "    print(f\"  {w:15s} → {score:.4f}\")\n",
    "\n",
    "# 5. Inspecciona el vector de 'beautiful'\n",
    "print(\"\\nDimensión del vector:\", v_beautiful.shape)\n",
    "print(\"Primeros 10 valores:\", v_beautiful[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dos cosas a tener en cuenta al utilizar modelos previamente entrenados:\n",
    "\n",
    "* Los tokens/palabras siempre están en minúsculas. Si una palabra no está en el vocabulario, el modelo genera una excepción.\n",
    "* Por lo tanto, siempre es una buena idea encapsular esas declaraciones en bloques `try/except`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Representaciones distribuidas avanzadas**\n",
    "\n",
    "En la evolución de las representaciones distribuidas, surgieron cuatro líneas de investigación fundamentales que atendieron necesidades concretas: mejorar la generalización, reducir la carga computacional, incorporar conocimiento morfológico y compartir representaciones entre tareas. A continuación describimos cada enfoque, su motivación y la fórmula matemática que explica su funcionamiento.\n",
    "\n",
    "#### **Neural network language model (NNLM)**\n",
    "\n",
    "**Contexto**  \n",
    "Antes de 2003, las representaciones basadas en conteo (coocurrencias, SVD, PMI) carecían de capacidad para modelar interacciones no lineales. Bengio et al. propusieron entrenar de forma conjunta los embeddings y una red feed‑forward que, para una ventana de contexto fija $(w_{t-n+1},\\dots,w_{t-1})$, estimara la probabilidad de la siguiente palabra $w_t$.\n",
    "\n",
    "**Ecuación clave**  \n",
    "\n",
    "$$\n",
    "P(w_t \\mid w_{t-n+1},\\dots,w_{t-1})\n",
    "=\n",
    "\\frac{\\exp\\bigl(u_{w_t}^{\\!\\top}\\,f\\bigl(W\\,[\\,e_{w_{t-n+1}},\\dots,e_{w_{t-1}}]\\bigr)\\bigr)}\n",
    "     {\\sum_{w\\in V}\\exp\\bigl(u_{w}^{\\!\\top}\\,f\\bigl(W\\,[\\,e_{w_{t-n+1}},\\dots,e_{w_{t-1}}]\\bigr)\\bigr)},\n",
    "$$  \n",
    "\n",
    "donde:  \n",
    "- $e_{w}\\in\\mathbb{R}^d$ es el embedding de cada palabra de contexto.  \n",
    "- $W$ y $u_w$ son parámetros entrenables de la red.  \n",
    "- $f$ es una función oculta no lineal (por ejemplo, tanh).  \n",
    "- El denominador softmax normaliza sobre todo el vocabulario $V$.  \n",
    "\n",
    "Este planteamiento mejoró la predicción de secuencias y sentó las bases de los embeddings neuronales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. Parámetros del modelo\n",
    "vocab = [\"I\", \"like\", \"dogs\", \"cats\", \"and\"]\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "V = len(vocab)         # tamaño del vocabulario\n",
    "d = 10                 # dimensión de embeddings\n",
    "n = 2                  # tamaño de la ventana de contexto\n",
    "hidden_dim = 20        # dimensión de la capa oculta\n",
    "\n",
    "# 2. Definición del modelo NNLM\n",
    "class NNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, context_size, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.linear1 = nn.Linear(context_size * emb_dim, hidden_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.linear2 = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, context_idxs):\n",
    "        \"\"\"\n",
    "        context_idxs: tensor Long de forma (batch_size, context_size)\n",
    "        Salida: distribuciones de probabilidad Softmax de forma (batch_size, vocab_size)\n",
    "        \"\"\"\n",
    "        emb = self.emb(context_idxs)                        # (batch, context_size, emb_dim)\n",
    "        emb = emb.view(emb.size(0), -1)                     # (batch, context_size*emb_dim)\n",
    "        h = self.tanh(self.linear1(emb))                    # (batch, hidden_dim)\n",
    "        logits = self.linear2(h)                            # (batch, vocab_size)\n",
    "        probs = torch.softmax(logits, dim=1)                # (batch, vocab_size)\n",
    "        return probs\n",
    "\n",
    "# 3. Crear instancia del modelo y optimizador\n",
    "modelo = NNLM(V, d, n, hidden_dim)\n",
    "optimizer = optim.SGD(modelo.parameters(), lr=0.1)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# 4. Ejemplo de un batch de entrenamiento\n",
    "# Contexto: [\"I\",\"like\"] predice target \"dogs\"\n",
    "context_batch = torch.tensor([[word2idx[\"I\"], word2idx[\"like\"]]], dtype=torch.long)\n",
    "target_batch = torch.tensor([word2idx[\"dogs\"]], dtype=torch.long)\n",
    "\n",
    "# Forward pass\n",
    "probs = modelo(context_batch)                            # (1, V)\n",
    "log_probs = torch.log(probs)\n",
    "\n",
    "# Cálculo de la pérdida y backward\n",
    "loss = criterion(log_probs, target_batch)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# 5. Salidas\n",
    "print(\"Distribución P(w_t|context=['I','like']):\")\n",
    "for w, idx in word2idx.items():\n",
    "    print(f\"  P({w}) = {probs[0, idx].item():.4f}\")\n",
    "print(f\"\\nPérdida (NLLLoss): {loss.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **C&W Model (Collobert & Weston)**\n",
    "\n",
    "**Contexto**  \n",
    "La necesidad de compartir embeddings entre múltiples tareas (clasificación, análisis sintáctico, etc.) y de evitar el alto coste de la normalización softmax condujo a Collobert y Weston (2008) a diseñar un esquema de aprendizaje por ranking con redes convolucionales.\n",
    "\n",
    "**Ecuación clave**  \n",
    "\n",
    "Definiendo una ventana real $x^+$ y variantes corruptas $x^-$, el objetivo hinge es:  \n",
    "$$\n",
    "\\mathcal{L}_{\\mathrm{C\\&W}}\n",
    "=\n",
    "\\sum_{x^+}\\sum_{x^-}\n",
    "\\max\\bigl(\\,0,\\,1 - s(x^+) + s(x^-)\\bigr),\n",
    "$$  \n",
    "donde $s(x)$ es una puntuación escalar obtenida aplicando convoluciones y capas densas sobre la secuencia de embeddings concatenados. Esta pérdida garantiza que los ejemplos reales obtengan siempre al menos un margen de 1 frente a los negativos, acelerando el entrenamiento y facilitando el multitasking.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Parámetros básicos\n",
    "vocab = [\"I\", \"love\", \"cats\", \"dogs\", \"and\"]\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "V = len(vocab)\n",
    "d = 50             # dimensión de embeddings\n",
    "window_size = 3    # tamaño de la ventana\n",
    "np.random.seed(42)\n",
    "\n",
    "# 2. Inicializar embeddings y vector de scoring\n",
    "E = np.random.randn(V, d)                     # tabla de embeddings aleatorios\n",
    "w_score = np.random.randn(window_size * d)    # vector para scoring\n",
    "\n",
    "# 3. Función de scoring s(x)\n",
    "def score(window):\n",
    "    # Concatena embeddings de la ventana y aplica el scoring lineal\n",
    "    concat_emb = np.hstack([E[word2idx[w]] for w in window])\n",
    "    return float(concat_emb.dot(w_score))\n",
    "\n",
    "# 4. Ventana real (x+) y corrupta (x-)\n",
    "x_pos = [\"I\", \"love\", \"cats\"]\n",
    "x_neg = [\"I\", \"dogs\", \"cats\"]  # corrompemos la palabra central\n",
    "\n",
    "# 5. Cálculo de scores y pérdida hinge\n",
    "s_pos = score(x_pos)\n",
    "s_neg = score(x_neg)\n",
    "margin = 1.0\n",
    "loss = max(0.0, 1 - s_pos + s_neg)\n",
    "\n",
    "# 6. Resultados\n",
    "print(\"Ventana real:\", x_pos, \"→ score:\", f\"{s_pos:.4f}\")\n",
    "print(\"Ventana corrupta:\", x_neg, \"→ score:\", f\"{s_neg:.4f}\")\n",
    "print(\"C&W Hinge Loss:\", f\"{loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **CBOW y Skip‑gram**\n",
    "\n",
    "**Contexto**  \n",
    "Word2Vec introdujo dos tareas inversas que, combinadas con técnicas de muestreo, hicieron posible entrenar embeddings de alta calidad en vocabularios enormes.\n",
    "\n",
    "- **CBOW** promedia el contexto para predecir la palabra central.\n",
    "- **Skip‑gram** invierte la tarea, usando la palabra central para predecir su contexto.\n",
    "\n",
    "**Ecuación clave (Negative Sampling)**  \n",
    "En lugar de softmax completo, usan:\n",
    "$$\n",
    "\\mathcal{L}_{\\text{NS}}\n",
    "=\n",
    "-\\log\\sigma\\bigl(u_{w_t}^{\\!\\top}v_C\\bigr)\n",
    "\\;-\\;\n",
    "\\sum_{i=1}^k\n",
    "\\mathbb{E}_{w_i\\sim P_n}\n",
    "\\bigl[\\log\\sigma\\bigl(-u_{w_i}^{\\!\\top}v_C\\bigr)\\bigr],\n",
    "$$\n",
    "donde  \n",
    "- $v_C$ es el embedding del contexto (o de la palabra central).  \n",
    "- $\\sigma(x)=1/(1+e^{-x})$.  \n",
    "- $P_n$ es una distribución de ruido habitual (frecuencias a la 3/4).  \n",
    "- $k$ es el número de muestras negativas.  \n",
    "\n",
    "Gracias a esta reformulación, la complejidad por ejemplo pasó de $O(|V|)$ a $O(k)$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Vocabulario y mapeo\n",
    "vocab = [\"I\", \"like\", \"NLP\", \"and\", \"AI\"]\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "V = len(vocab)\n",
    "d = 5  # dimensión de embeddings\n",
    "\n",
    "# Inicializar embeddings de entrada (W_in) y salida (W_out)\n",
    "np.random.seed(0)\n",
    "W_in = np.random.randn(V, d)\n",
    "W_out = np.random.randn(V, d)\n",
    "\n",
    "# Distribución de ruido unigram^3/4\n",
    "freqs = np.array([1, 2, 1, 1, 1], dtype=np.float32)\n",
    "p_noise = freqs**0.75\n",
    "p_noise /= p_noise.sum()\n",
    "\n",
    "# Función sigma\n",
    "sigma = lambda x: 1 / (1 + np.exp(-x))\n",
    "\n",
    "# --- CBOW con Negative Sampling ---\n",
    "context = [\"I\", \"NLP\", \"and\", \"AI\"]\n",
    "target = \"like\"\n",
    "# Embedding del contexto\n",
    "v_C = np.mean([W_in[word2idx[w]] for w in context], axis=0)  # (d,)\n",
    "# Score positivo\n",
    "u_t = W_out[word2idx[target]]\n",
    "pos_score = sigma(u_t.dot(v_C))\n",
    "# Muestras negativas\n",
    "k = 2\n",
    "neg_idxs = np.random.choice(V, size=k, p=p_noise)\n",
    "neg_scores = sigma(-W_out[neg_idxs].dot(v_C))\n",
    "# Pérdida CBOW\n",
    "cbow_loss = -np.log(pos_score) - np.sum(np.log(neg_scores))\n",
    "\n",
    "# Mostrar resultados CBOW\n",
    "print(\"CBOW con Negative Sampling\")\n",
    "print(f\"Contexto: {context} → Target: '{target}'\")\n",
    "print(f\"Positive idx: {word2idx[target]}, Negative idxs: {neg_idxs.tolist()}\")\n",
    "print(f\"pos_score: {pos_score:.4f}\")\n",
    "print(f\"neg_scores: {[round(s,4) for s in neg_scores]}\")\n",
    "print(f\"CBOW Loss: {cbow_loss:.4f}\\n\")\n",
    "\n",
    "# --- Skip-gram con Negative Sampling ---\n",
    "center = \"like\"\n",
    "contexts = [\"I\", \"NLP\"]\n",
    "v_center = W_in[word2idx[center]]\n",
    "pos_scores_sg = sigma(np.array([W_out[word2idx[w]].dot(v_center) for w in contexts]))\n",
    "# Muestras negativas para cada contexto\n",
    "neg_idxs_sg = np.random.choice(V, size=(len(contexts), k), p=p_noise)\n",
    "neg_scores_sg = sigma(-np.array([W_out[neg_idxs_sg[i]].dot(v_center) for i in range(len(contexts))]))\n",
    "# Pérdida Skip-gram\n",
    "skip_loss = -np.sum(np.log(pos_scores_sg)) - np.sum(np.log(neg_scores_sg))\n",
    "\n",
    "# Mostrar resultados Skip-gram\n",
    "print(\"Skip-gram con Negative Sampling\")\n",
    "print(f\"Center: '{center}' → Contextos: {contexts}\")\n",
    "print(f\"Positive idxs: {[word2idx[w] for w in contexts]}, Negative idxs:\\n  {neg_idxs_sg.tolist()}\")\n",
    "print(f\"pos_scores: {[round(s,4) for s in pos_scores_sg]}\")\n",
    "print(f\"neg_scores:\\n  {[[round(s,4) for s in row] for row in neg_scores_sg]}\")\n",
    "print(f\"Skip-gram Loss: {skip_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Método híbrido caracter‑palabra**\n",
    "\n",
    "**Contexto**  \n",
    "Las lenguas ricas en morfología y el problema de las palabras OOV inspiraron la combinación de embeddings de caracteres y de palabra. Mediante CNNs o LSTMs se aprenden vectores de caracteres o n‑gramas, que luego se fusionan con el embedding de la palabra completa.\n",
    "\n",
    "**Ecuación clave**  \n",
    "Para un término $w$ compuesto por $L$ n‑gramas de caracteres $c_i$, la representación final es:  \n",
    "$$\n",
    "v_w\n",
    "=\n",
    "W_1\\,e_w\n",
    "\\;+\\,\n",
    "W_2\\,\n",
    "\\Bigl(\\tfrac{1}{L}\\sum_{i=1}^L c_i\\Bigr),\n",
    "$$  \n",
    "con $W_1$ y $W_2$ como matrices de combinación entrenables. FastText ejemplifica esta idea descomponiendo cada palabra en todos sus n‑gramas y sumando sus embeddings, logrando robustez frente a variaciones ortográficas y morfológicas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Definición de palabras de ejemplo\n",
    "words = [\"unbelievable\", \"running\"]\n",
    "\n",
    "# 2. Función para extraer n‑gramas de caracteres\n",
    "def extract_ngrams(word, min_n=3, max_n=6):\n",
    "    word_aug = f\"<{word}>\"\n",
    "    ngrams = []\n",
    "    for n in range(min_n, max_n + 1):\n",
    "        for i in range(len(word_aug) - n + 1):\n",
    "            ngrams.append(word_aug[i:i+n])\n",
    "    return ngrams\n",
    "\n",
    "# 3. Construir vocabulario de caracteres (n‑gramas)\n",
    "all_ngrams = set(ng for w in words for ng in extract_ngrams(w))\n",
    "char_vocab = {ng: i for i, ng in enumerate(sorted(all_ngrams))}\n",
    "\n",
    "# 4. Inicializar embeddings aleatorios y matrices de combinación\n",
    "emb_dim = 50\n",
    "np.random.seed(0)\n",
    "# Embedding de la \"palabra completa\"\n",
    "word_emb_table = {w: np.random.randn(emb_dim) for w in words}\n",
    "# Embedding de cada n‑grama\n",
    "char_emb_table = np.random.randn(len(char_vocab), emb_dim)\n",
    "# Matrices W1 y W2 entrenables\n",
    "W1 = np.random.randn(emb_dim, emb_dim)\n",
    "W2 = np.random.randn(emb_dim, emb_dim)\n",
    "\n",
    "# 5. Cálculo de embeddings híbridos según la ecuación v_w = W1 e_w + W2 (1/L sum c_i)\n",
    "hybrid_embeddings = {}\n",
    "for w in words:\n",
    "    e_w = word_emb_table[w]  # embedding de la palabra\n",
    "    ngrams = extract_ngrams(w)\n",
    "    c_idxs = [char_vocab[ng] for ng in ngrams]\n",
    "    e_c = char_emb_table[c_idxs].mean(axis=0)  # promedio de embeddings de n‑gramas\n",
    "    v_w = W1 @ e_w + W2 @ e_c\n",
    "    hybrid_embeddings[w] = v_w\n",
    "\n",
    "# 6. Mostrar resultados\n",
    "for w, vec in hybrid_embeddings.items():\n",
    "    print(f\"'{w}' → embedding híbrido (primeros 5 valores): {vec[:5]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Representaciones distribuidas de frases**\n",
    "\n",
    "Las representaciones distribuidas de frases constituyen un paso intermedio entre los embeddings de palabras y los de documentos completos. Si bien las palabras aportan unidades atómicas de significado, las frases introducen composiciones semánticas y relaciones sintácticas complejas que los modelos deben capturar para tareas como análisis de sentimientos, clasificación de oraciones y recuperación de información. \n",
    "\n",
    "En este contexto, las dos familias clásicas de métodos son los basados en Bag‑of‑Words (BoW), que extienden de forma lineal la lógica de los embeddings de palabras, y los basados en autoencoders, que emplean arquitecturas neuronales para codificar y decodificar la secuencia completa.\n",
    "\n",
    "#### **Basadas en bolsas de palabras**\n",
    "\n",
    "Los métodos Bag‑of‑Words para frases aprovechan la disponibilidad de embeddings de palabras preentrenados y definen la representación de una frase como agregación de los vectores de sus tokens. La simplicidad y eficiencia de estos esquemas han garantizado su vigencia en entornos con grandes volúmenes de datos.\n",
    "\n",
    "**Promedio simple de embeddings**\n",
    "\n",
    "Sea una frase $S = (w_1, w_2, \\,\\dots\\,, w_n)$ con embeddings de palabras $v_i = e_{w_i} \\in \\mathbb{R}^d$. El promedio simple define:\n",
    "\n",
    "$$\n",
    "  v_{S} = \\frac{1}{n} \\sum_{i=1}^n v_i.\n",
    "$$\n",
    "\n",
    "Este método asume que cada token contribuye de igual manera a la semántica global. A pesar de ignorar el orden y las interacciones no lineales, su bajo coste computacional $O(n\\times d)$ lo convierte en una opción popular, especialmente para tareas de similitud semántica donde la magnitud de la frase importa más que su estructura interna.\n",
    "\n",
    "**Promedios ponderados y smooth inverse frequency (SIF)**\n",
    "\n",
    "Para reflejar la relevancia diferencial de cada palabra, se introducen pesos basados en estadísticas de corpus. El esquema SIF (Arora et al., 2017) propone:\n",
    "\n",
    "$$\n",
    "  \\alpha_i = \\frac{a}{a + p(w_i)},\n",
    "  \\quad\n",
    "  v_{S} = \\frac{1}{\\sum_{i=1}^n \\alpha_i} \\sum_{i=1}^n \\alpha_i v_i,\n",
    "$$\n",
    "\n",
    "donde $p(w_i)$ es la frecuencia de $w_i$ en un gran corpus y $a$ un parámetro de suavizado (por ejemplo, $10^{-3}$). Después se suprime la primera componente principal $u$ de todo el conjunto de vectores de frases:\n",
    "\n",
    "$$\n",
    "  v_{S}' = v_{S} - u\\bigl(u^\\top v_{S}\\bigr).\n",
    "$$\n",
    "\n",
    "La eliminación de la componente dominante reduce el efecto de palabras muy frecuentes y mejora la discriminación entre frases semánticamente similares.\n",
    "\n",
    "**Representación TF‑IDF ponderada**\n",
    "\n",
    "Como alternativa, se utiliza el peso TF‑IDF de cada término:\n",
    "\n",
    "$$\n",
    "  \\alpha_i = \\mathrm{tf}(w_i, S) \\times \\log\\frac{N}{\\mathrm{df}(w_i)},\n",
    "  \\quad\n",
    "  v_{S} = \\frac{1}{\\sum_{i=1}^n \\alpha_i} \\sum_{i=1}^n \\alpha_i v_i,\n",
    "$$\n",
    "\n",
    "con $N$ el número total de documentos y $\\mathrm{df}(w_i)$ la cantidad de documentos que contienen $w_i$. Este esquema potencia términos distintivos y atenúa los muy comunes.\n",
    "\n",
    "**Max‑pooling, min‑pooling y concatenaciones**\n",
    "\n",
    "Otros esquemas no lineales incluyen:\n",
    "\n",
    "- **Max‑pooling:** $v_S(j) = \\max_{1\\le i\\le n} v_i(j)$ para cada dimensión $j$.\n",
    "- **Min‑pooling:** $v_S(j) = \\min_{1\\le i\\le n} v_i(j)$.\n",
    "- **Concatenación:** combinación de $\\langle\\mathrm{mean},\\mathrm{max},\\mathrm{min}\\rangle$ generando un vector de dimensión $3d$.\n",
    "\n",
    "Aunque capturan características de forma más rica, incrementan la dimensionalidad y el coste de almacenamiento.\n",
    "\n",
    "**Interpretación empírica y aplicaciones**\n",
    "\n",
    "Los métodos BoW han mostrado:\n",
    "\n",
    "- **Buen desempeño en similitud semántica:** la correlación coseno entre $v_S$ y otro vector de referencia se alinea con la percepción humana de similitud.\n",
    "- **Eficiencia en recuperación de información:** indexación de vectores en aproximaciones de \"nearest neighbor\" para búsqueda rápida.\n",
    "\n",
    "No obstante, son sensibles a la presencia de stopwords y carecen de modelado de negaciones y dependencia sintáctica.\n",
    "\n",
    "**Limitaciones sintácticas y semánticas**\n",
    "\n",
    "| Limitación                  | Descripción                                                                         |\n",
    "|-----------------------------|-------------------------------------------------------------------------------------|\n",
    "| Invarianza al orden         | No distingue \"rojo coche rápido\" de \"rápido coche rojo\"                           |\n",
    "| Escasa interacción léxica   | No captura modismos ni expresiones idiomáticas                                      |\n",
    "| Vulnerable a palabras vacías| Stopwords frecuentes pueden dominar la media sin corrección de ponderación adecuada |\n",
    "| Sin dependencia gramatical  | No modela relaciones sujeto-verbo u objetos directos e indirectos                    |\n",
    "\n",
    "En entornos donde la sintaxis sea clave (por ejemplo, detección de sarcasmo o análisis profundo de relaciones), se prefieren métodos más complejos.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 1. Definir frases de ejemplo\n",
    "sentences = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"A fast brown fox leaps over lazy dogs in the park\",\n",
    "    \"Deep learning models require large amounts of data\",\n",
    "    \"Machine learning and deep learning are subfields of AI\",\n",
    "    \"Natural language processing enables computers to understand text\"\n",
    "]\n",
    "\n",
    "# 2. Tokenización básica (split + limpiar no alfabéticos)\n",
    "tokenized = []\n",
    "for sent in sentences:\n",
    "    tokens = [w.lower() for w in sent.split() if w.isalpha()]\n",
    "    tokenized.append(tokens)\n",
    "\n",
    "# 3. Construir vocabulario de tokens únicos\n",
    "all_tokens = [tok for sent in tokenized for tok in sent]\n",
    "vocab = sorted(set(all_tokens))\n",
    "word2idx = {w:i for i,w in enumerate(vocab)}\n",
    "V = len(vocab)\n",
    "d = 50  # dimensión de embeddings\n",
    "\n",
    "# 4. Inicializar embeddings aleatorios\n",
    "np.random.seed(0)\n",
    "E = np.random.randn(V, d)\n",
    "\n",
    "# 5. Frecuencias de palabras p(w) para SIF\n",
    "word_counts = {w: all_tokens.count(w) for w in vocab}\n",
    "total = sum(word_counts.values())\n",
    "\n",
    "# 6. Función auxiliar: promedio simple\n",
    "def avg_embedding(tokens):\n",
    "    vecs = np.array([E[word2idx[t]] for t in tokens])\n",
    "    return vecs.mean(axis=0)\n",
    "\n",
    "# 7. Promedio simple para cada frase\n",
    "simple_avg = [avg_embedding(sent) for sent in tokenized]\n",
    "\n",
    "# 8. SIF embeddings\n",
    "a = 1e-3\n",
    "sif_vecs = []\n",
    "for sent in tokenized:\n",
    "    weights = [a / (a + word_counts[t]/total) for t in sent]\n",
    "    vecs = np.array([E[word2idx[t]] for t in sent])\n",
    "    weighted = (vecs * np.array(weights)[:,None]).sum(axis=0) / sum(weights)\n",
    "    sif_vecs.append(weighted)\n",
    "\n",
    "# 9. Eliminar componente principal de SIF\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(np.vstack(sif_vecs))\n",
    "u = pca.components_[0]\n",
    "sif_processed = [v - u*(u.dot(v)) for v in sif_vecs]\n",
    "\n",
    "# 10. TF-IDF ponderado\n",
    "tfidf = TfidfVectorizer(lowercase=True, token_pattern=r\"(?u)\\b[a-zA-Z]+\\b\")\n",
    "X = tfidf.fit_transform(sentences)\n",
    "feat = tfidf.get_feature_names_out()\n",
    "tfidf_embs = []\n",
    "for i, sent in enumerate(tokenized):\n",
    "    weights, vecs = [], []\n",
    "    for t in sent:\n",
    "        if t in feat:\n",
    "            idx = np.where(feat==t)[0][0]\n",
    "            w = X[i, idx]\n",
    "            weights.append(w)\n",
    "            vecs.append(E[word2idx[t]])\n",
    "    weights = np.array(weights)\n",
    "    vecs = np.array(vecs)\n",
    "    tfidf_embs.append((vecs*weights[:,None]).sum(axis=0) / weights.sum())\n",
    "\n",
    "# 11. Pooling: mean, max, min y concatenación\n",
    "pool_embs = []\n",
    "for sent in tokenized:\n",
    "    vecs = np.array([E[word2idx[t]] for t in sent])\n",
    "    meanp = vecs.mean(axis=0)\n",
    "    maxp  = vecs.max(axis=0)\n",
    "    minp  = vecs.min(axis=0)\n",
    "    pool_embs.append(np.concatenate([meanp, maxp, minp]))\n",
    "\n",
    "# 12. Mostrar resultados y dimensiones\n",
    "print(\"Dimensiones de vectores:\")\n",
    "print(\" Simple avg:\", simple_avg[0].shape)\n",
    "print(\" SIF proc :\", sif_processed[0].shape)\n",
    "print(\" TF-IDF   :\", tfidf_embs[0].shape)\n",
    "print(\" Pool cat :\", pool_embs[0].shape)\n",
    "\n",
    "# 13. Ejemplo de similitud usando promedio simple\n",
    "from numpy.linalg import norm\n",
    "def cosine(u, v): return u.dot(v)/(norm(u)*norm(v))\n",
    "sim = cosine(simple_avg[0], simple_avg[1])\n",
    "print(f\"\\nSimilidad coseno entre frase 0 y 1: {sim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Basadas en autoencoder**\n",
    "\n",
    "Los autoencoders resuelven las debilidades de BoW al aprender codificaciones latentes que capturan orden, sintaxis e interacciones no lineales.\n",
    "\n",
    "**Arquitectura de encoder-decoder**\n",
    "\n",
    "Un autoencoder de secuencia implementa:\n",
    "\n",
    "- **Encoder:** procesa $(x_1,\\dots,x_n)$ (embeddings de palabra) mediante una red recurrente:\n",
    "  $$\n",
    "    h_t = f(h_{t-1}, x_t),\n",
    "    \\quad\n",
    "    z = h_n,\n",
    "  $$\n",
    "  donde $f$ puede ser LSTM o GRU.\n",
    "\n",
    "- **Decoder:** genera la secuencia reconstruida:\n",
    "  $$\n",
    "    s_t = g(s_{t-1}, z),\n",
    "    \\quad\n",
    "    \\hat x_t = \\mathrm{softmax}(W_o s_t),\n",
    "  $$\n",
    "\n",
    "La pérdida de reconstrucción se define como entropía cruzada acumulada:\n",
    "\n",
    "$$\n",
    "  \\mathcal{L}_{AE} = -\\sum_{t=1}^n \\log P( x_t \\mid s_{t-1}, z ).\n",
    "$$\n",
    "\n",
    "Esta configuración favorece latentes $z$ que capturan la información esencial de la frase.\n",
    "\n",
    "**Autoencoder denoising**\n",
    "\n",
    "Al introducir un operador de ruido $T(\\cdot)$ que borra o altera tokens, el encoder recibe $\\tilde x = T(x)$ y aprende a reconstruir $x$. La pérdida es:\n",
    "\n",
    "$$\n",
    "  \\mathcal{L}_{DAE} = -\\sum_{t=1}^n \\log P( x_t \\mid f(\\tilde x) ).\n",
    "$$\n",
    "\n",
    "Este mecanismo impulsa robustez frente a errores menores y parsinómias de tokenización.\n",
    "\n",
    "**Autoenconder variacional (VAE)**\n",
    "\n",
    "El VAE aplica un enfoque probabilístico:\n",
    "\n",
    "- **Encoder estocástico:** $q_\\phi(z\\mid x) = \\mathcal{N}(\\mu(x), \\mathrm{diag}(\\sigma^2(x)))$.\n",
    "- **Decoder condicional:** $p_\\theta(x\\mid z)$.\n",
    "\n",
    "La función objetivo maximiza la evidencia libre inferior:\n",
    "\n",
    "$$\n",
    "  \\mathcal{L}_{VAE}\n",
    "  = \\mathbb{E}_{q_\\phi} [ \\log p_\\theta(x\\mid z) ]\n",
    "    - \\mathrm{D}_{KL}\\bigl(q_\\phi(z\\mid x) \\| p(z)\\bigr).\n",
    "$$\n",
    "\n",
    "Este esquema favorece latentes continuos y permite generar frases nuevas mediante muestreo en el espacio $z$.\n",
    "\n",
    "**Vectores Skip‑Thought**\n",
    "\n",
    "Skip‑Thought extiende la reconstrucción a las oraciones adyacentes:\n",
    "\n",
    "1. Encoder produce $h_i = f(s_i)$.\n",
    "2. Dos decoders generan $s_{i-1}$ y $s_{i+1}$.\n",
    "3. La pérdida combina ambas reconstrucciones:\n",
    "   $$\n",
    "     \\mathcal{L}_{SkipThought}\n",
    "     = -\\sum_t [ \\log P(s_{i-1}[t]\\mid h_i) + \\log P(s_{i+1}[t]\\mid h_i) ].\n",
    "   $$\n",
    "\n",
    "Los vectores $h_i$ codifican información semántica inter-oracional.\n",
    "\n",
    "**Modelos basados en atención (Transformers)**\n",
    "\n",
    "Aunque no estrictamente autoencoders, los Transformers ofrecen representaciones de frases mediante atención múltiple:\n",
    "\n",
    "$$\n",
    "  \\mathrm{Attention}(Q,K,V) = \\mathrm{softmax}\\bigl(\\tfrac{QK^\\top}{\\sqrt{d_k}}\\bigr)V.\n",
    "$$\n",
    "\n",
    "En BERT, el token especial `[CLS]` produce un vector $h_{CLS}$ que sirve de representación de frase. La autoatención captura dependencias de largo alcance y orden sintáctico.\n",
    "\n",
    "**Comparativa y aplicaciones prácticas**\n",
    "\n",
    "\n",
    "| Método            | Captura orden | Robustez OOV | Eficiencia  | Uso típico                            |\n",
    "|-------------------|---------------|--------------|-------------|---------------------------------------|\n",
    "| BoW promedio      | No            | Baja         | Alta        | Búsqueda, similitud rápida            |\n",
    "| BoW ponderado     | No            | Media        | Alta        | Recuperación con stopword handling    |\n",
    "| Autoencoder Seq2Seq | Sí         | Media        | Media       | Generación de texto, compresión de secuencias |\n",
    "| VAE               | Sí            | Alta         | Baja        | Generación creativa de frases         |\n",
    "| Skip‑Thought      | Sí            | Alta         | Baja        | Arquitectura preentrenada general     |\n",
    "| Transformers      | Sí            | Alta         | Media/Baja  | Tareas de clasificación de frases     |\n",
    "\n",
    "Las representaciones de frases alimentan distintas aplicaciones:\n",
    "\n",
    "- **Clasificación de sentimientos:** embeddings de frase como características de entrada a clasificadores lineales o no lineales.\n",
    "- **Respuesta a preguntas (QA):** mapeo de pregunta y contextos a vectores comparables.\n",
    "- **Detección de plagio y similitud:** medición de coseno entre vectores de frases largas.\n",
    "- **Agrupamiento de opiniones:** clustering de reseñas por similitud semántica.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo detallado de diversos autoencoders para representaciones de frases\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "# 1. Preparar vocabulario y datos sintéticos\n",
    "sentences = [\n",
    "    [\"i\", \"love\", \"deep\", \"learning\"],\n",
    "    [\"transformers\", \"are\", \"powerful\", \"models\"],\n",
    "    [\"autoencoders\", \"learn\", \"latent\", \"spaces\"],\n",
    "    [\"variational\", \"autoencoder\", \"is\", \"probabilistic\"],\n",
    "    [\"skip\", \"thought\", \"models\", \"encode\", \"context\"],\n",
    "]\n",
    "# Construir vocabulario\n",
    "vocab = sorted({tok for sent in sentences for tok in sent})\n",
    "word2idx = {w: i+1 for i, w in enumerate(vocab)}  # 0 para PAD\n",
    "word2idx[\"<pad>\"] = 0\n",
    "idx2word = {i:w for w,i in word2idx.items()}\n",
    "V = len(word2idx)\n",
    "# Hiperparámetros\n",
    "emb_dim = 32\n",
    "hid_dim = 64\n",
    "batch_size = 2\n",
    "max_len = max(len(s) for s in sentences)\n",
    "\n",
    "# Dataset simple que hace padding\n",
    "class SentenceDataset(Dataset):\n",
    "    def __init__(self, sents, w2i, max_len):\n",
    "        self.data = sents\n",
    "        self.w2i = w2i\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.data[idx]\n",
    "        seq = [self.w2i[w] for w in s]\n",
    "        # padding\n",
    "        seq = seq + [0]*(self.max_len-len(seq))\n",
    "        return torch.tensor(seq, dtype=torch.long), torch.tensor(seq, dtype=torch.long)\n",
    "\n",
    "dataset = SentenceDataset(sentences, word2idx, max_len)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 2. Seq2Seq Autoencoder básico\n",
    "class Seq2SeqAE(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.encoder = nn.LSTM(emb_dim, hid_dim, batch_first=True)\n",
    "        self.decoder = nn.LSTM(emb_dim, hid_dim, batch_first=True)\n",
    "        self.out = nn.Linear(hid_dim, vocab_size)\n",
    "    def forward(self, x):\n",
    "        emb = self.embed(x)\n",
    "        _, (h,n) = self.encoder(emb)\n",
    "        # Decoder input: usar como primer input same seq (teacher forcing)\n",
    "        dec_input = emb\n",
    "        dec_out, _ = self.decoder(dec_input, (h,n))\n",
    "        logits = self.out(dec_out)\n",
    "        return logits\n",
    "\n",
    "# 3. Denoising Autoencoder extiende Seq2SeqAE\n",
    "class DenoisingAE(Seq2SeqAE):\n",
    "    def __init__(self, *args, noise_prob=0.3, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.noise_prob = noise_prob\n",
    "    def forward(self, x):\n",
    "        # Corromper tokens con <pad>\n",
    "        x_noisy = x.clone()\n",
    "        mask = torch.rand_like(x_noisy.float()) < self.noise_prob\n",
    "        x_noisy[mask] = 0\n",
    "        return super().forward(x_noisy)\n",
    "\n",
    "# 4. VAE Autoencoder\n",
    "class VAEAE(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, z_dim=16):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.encoder = nn.LSTM(emb_dim, hid_dim, batch_first=True)\n",
    "        self.fc_mu = nn.Linear(hid_dim, z_dim)\n",
    "        self.fc_logvar = nn.Linear(hid_dim, z_dim)\n",
    "        self.decoder = nn.LSTM(emb_dim+z_dim, hid_dim, batch_first=True)\n",
    "        self.out = nn.Linear(hid_dim, vocab_size)\n",
    "    def encode(self, x):\n",
    "        emb = self.embed(x)\n",
    "        _, (h,_) = self.encoder(emb)\n",
    "        h = h.squeeze(0)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "    def reparam(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparam(mu, logvar)\n",
    "        # expand z a secuencia\n",
    "        seq_z = z.unsqueeze(1).repeat(1, x.size(1), 1)\n",
    "        emb = self.embed(x)\n",
    "        dec_in = torch.cat([emb, seq_z], dim=2)\n",
    "        dec_out, _ = self.decoder(dec_in)\n",
    "        logits = self.out(dec_out)\n",
    "        return logits, mu, logvar\n",
    "\n",
    "# 5. Skip-Thought simplificado: encoder + dos decoders\n",
    "class SkipThought(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.encoder = nn.GRU(emb_dim, hid_dim, batch_first=True)\n",
    "        self.decoder_prev = nn.GRU(emb_dim, hid_dim, batch_first=True)\n",
    "        self.decoder_next = nn.GRU(emb_dim, hid_dim, batch_first=True)\n",
    "        self.out = nn.Linear(hid_dim, vocab_size)\n",
    "    def forward(self, x, prev, next_):\n",
    "        emb = self.embed(x)\n",
    "        _, h = self.encoder(emb)\n",
    "        # reconstruir previo\n",
    "        out_prev, _ = self.decoder_prev(self.embed(prev), h)\n",
    "        logits_prev = self.out(out_prev)\n",
    "        # reconstruir siguiente\n",
    "        out_next, _ = self.decoder_next(self.embed(next_), h)\n",
    "        logits_next = self.out(out_next)\n",
    "        return logits_prev, logits_next\n",
    "\n",
    "# 6. Multi-Head Attention básico\n",
    "class MultiHeadAttn(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads=4):\n",
    "        super().__init__()\n",
    "        assert hid_dim % n_heads == 0\n",
    "        self.d_k = hid_dim // n_heads\n",
    "        self.n_heads = n_heads\n",
    "        # Proyecciones\n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "    def forward(self, q, k, v):\n",
    "        batch_size = q.size(0)\n",
    "        # linea a heads\n",
    "        def proj(x, fc):\n",
    "            x = fc(x).view(batch_size, -1, self.n_heads, self.d_k)\n",
    "            return x.permute(0,2,1,3)\n",
    "        Q = proj(q, self.fc_q)\n",
    "        K = proj(k, self.fc_k)\n",
    "        V = proj(v, self.fc_v)\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / (self.d_k**0.5)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        x = torch.matmul(attn, V)\n",
    "        x = x.permute(0,2,1,3).contiguous().view(batch_size, -1, self.n_heads*self.d_k)\n",
    "        return self.fc_o(x)\n",
    "\n",
    "# 7. Ejemplo de entrenamiento paso a paso\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "modelos = {\n",
    "    \"AE\": Seq2SeqAE(V, emb_dim, hid_dim).to(device),\n",
    "    \"DAE\": DenoisingAE(V, emb_dim, hid_dim).to(device),\n",
    "    \"VAE\": VAEAE(V, emb_dim, hid_dim).to(device),\n",
    "}\n",
    "optims = {name: torch.optim.Adam(m.parameters(), lr=1e-3) for name,m in modelos.items()}\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "# Un batch para SkipThought (usar sent i=2 y sus vecinos i=1,3)\n",
    "batch = next(iter(loader))\n",
    "x, _ = batch\n",
    "# para SkipThought, crear prev y next padding si es necesario\n",
    "prev = x.clone()\n",
    "next_ = x.clone()\n",
    "\n",
    "for name, modelo in modelos.items():\n",
    "    modelo.train()\n",
    "    optims[name].zero_grad()\n",
    "    outputs = modelo(x.to(device))\n",
    "    if name==\"VAE\":\n",
    "        logits, mu, logvar = outputs\n",
    "        # perdida reconstruction + KL\n",
    "        rec = criterion(logits.view(-1,V), x.view(-1).to(device))\n",
    "        kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)\n",
    "        loss = rec + kl\n",
    "    else:\n",
    "        logits = outputs\n",
    "        loss = criterion(logits.view(-1,V), x.view(-1).to(device))\n",
    "    loss.backward()\n",
    "    optims[name].step()\n",
    "    print(f\"{name} loss: {loss.item():.4f}\")\n",
    "\n",
    "# Ejemplo de atención\n",
    "q = k = v = torch.randn(batch_size, max_len, hid_dim, device=device)\n",
    "attn = MultiHeadAttn(hid_dim)\n",
    "out = attn(q,k,v)\n",
    "print(\"Forma de salida de  multiHeadAttention :\", out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Representaciones distribuidas de oraciones**\n",
    "\n",
    "Las oraciones, a diferencia de palabras o frases cortas, contienen estructuras sintácticas más complejas y relaciones semánticas de largo alcance, lo que impone nuevos retos a los modelos de representación. Mientras que los embeddings de palabras capturan información léxica y los vectores de frases medianas se centran en composiciones sencillas, las representaciones de oraciones deben integrar:\n",
    "\n",
    "1. **Información de orden y estructura:** la secuencia de tokens y la dependencia gramatical.\n",
    "2. **Semántica de alto nivel:** intenciones, relaciones argumentales y tono.\n",
    "3. **Contexto extraoracional:** en algunos casos, información del párrafo o documento de origen.\n",
    "\n",
    "Explora dos grandes enfoques: las representaciones generales de oraciones, obtenidas mediante preentrenamiento en tareas de inferencia textual, y las representaciones dirigidas a tareas específicas, donde la arquitectura se adapta y optimiza para el objetivo final.\n",
    "\n",
    "##### Representación general de oraciones\n",
    "\n",
    "Los modelos de representación general buscan generar vectores que funcionen bien en una amplia variedad de tareas sin modificación adicional. Dos ejemplos paradigmáticos son **InferSent** y el **Universal Sentence Encoder (USE)**.\n",
    "\n",
    "**InferSent: Bi‑LSTM con max‑pooling**\n",
    "\n",
    "**Arquitectura:** InferSent emplea una red bidireccional LSTM (Bi‑LSTM) para procesar la oración:\n",
    "\n",
    "- Para cada token $w_t$, el encoder genera una representación hacia adelante\n",
    "  $$\n",
    "    \\overrightarrow{h}_t = \\mathrm{LSTM}_f(\\overrightarrow{h}_{t-1}, e_{w_t}),\n",
    "  $$\n",
    "  y otra hacia atrás\n",
    "  $$\n",
    "    \\overleftarrow{h}_t = \\mathrm{LSTM}_b(\\overleftarrow{h}_{t+1}, e_{w_t}).\n",
    "  $$\n",
    "\n",
    "- El vector de la oración se construye mediante *max‑pooling* dimensional:\n",
    "  $$\n",
    "    v_{\\mathrm{sent}}(j) = \\max_{1 \\le t \\le n} \\bigl[\\overrightarrow{h}_t(j),\\overleftarrow{h}_t(j)\\bigr]\n",
    "  $$\n",
    "  donde $j$ recorre las $d$ dimensiones de los estados ocultos.\n",
    "\n",
    "**Entrenamiento:** Se entrena en la tarea de **inferencia textual natural (NLI)**, usando datasets como SNLI y MultiNLI. Dado un par de oraciones\n",
    "- oración base $u$,\n",
    "- hipótesis $v$,\n",
    "InferSent crea un vector combinado:\n",
    "  $$\n",
    "    x = [\\,u,\\; v,\\; u - v,\\; u \\odot v\\,],\n",
    "  $$\n",
    "que se pasa a una red feed‑forward con función de activación ReLU y capa softmax para predecir las clases {entailment, contradiction, neutral}. La función de pérdida es la entropía cruzada:\n",
    "  $$\n",
    "    \\mathcal{L}_{\\text{NLI}} = -\\sum_{c \\in \\{e,n,c\\}} y_c \\log p_c(x)\n",
    "  $$\n",
    "con $y_c$ la etiqueta real y $p_c(x)$ la probabilidad predicha.\n",
    "\n",
    "**Ventajas:**\n",
    "- Captura dependencias de largo alcance mediante LSTM.  \n",
    "- Max‑pooling extrae rasgos sintácticos y semánticos clave.  \n",
    "\n",
    "**Limitaciones:**\n",
    "- Entrenamiento costoso en datos NLI.  \n",
    "- No adapta el embedding para tareas fuera del dominio NLI sin fine‑tuning.\n",
    "\n",
    "**Universal Sentence Encoder (USE)**\n",
    "\n",
    "Google desarrolló el USE con dos variantes que equilibran eficiencia y precisión:\n",
    "\n",
    "1. **Deep Averaging Network (DAN):**  \n",
    "   - Se calculan embeddings de palabras y bi‑gramas:\n",
    "     $$ v_i = e_{w_i},\\quad g_i = e_{(w_i,w_{i+1})}. $$\n",
    "   - Se promedian:\n",
    "     $$ m = \\frac{1}{n-1} \\sum_{i=1}^{n-1} (v_i + g_i). $$\n",
    "   - Se proyecta mediante capas densas con ReLU:\n",
    "     $$ h = \\mathrm{ReLU}(W_1 m + b_1),\\quad v_{\\mathrm{sent}} = \\mathrm{ReLU}(W_2 h + b_2). $$\n",
    "   - El modelo es rápido y de baja latencia, adecuado para aplicaciones en línea.\n",
    "\n",
    "2. **Transformer Encoder:**  \n",
    "   - Cada token y bi‑grama recibe un vector de embeddings.  \n",
    "   - Se aplica *multi-head self-attention* en varias capas:\n",
    "     $$\n",
    "       \\mathrm{Attention}(Q,K,V) = \\mathrm{softmax}\\bigl(\\tfrac{QK^\\top}{\\sqrt{d_k}}\\bigr)V,\n",
    "     $$\n",
    "     donde $Q,K,V$ provienen de proyecciones lineales de las entradas.  \n",
    "   - Tras $L$ capas, se extrae el vector del token especial [CLS] o se promedian todos los vectores de salida para obtener $v_{\\mathrm{sent}}$.\n",
    "\n",
    "**Preentrenamiento:** USE se entrena con tareas de clasificación de parágrafos y predicción de texto, combinando señales de similitud semántica, minería de parágrafos duplicados y clasificación NLI. La función de pérdida suele ser entropía cruzada sobre múltiples objetivos simultáneos (multi-task), por ejemplo:\n",
    "  $$\n",
    "    \\mathcal{L}_{\\mathrm{USE}} = \\lambda_1 \\mathcal{L}_{\\mathrm{sim}} + \\lambda_2 \\mathcal{L}_{\\mathrm{NLI}} + \\lambda_3 \\mathcal{L}_{\\mathrm{dup}},\n",
    "  $$\n",
    "con pesos $\\lambda_i$.\n",
    "\n",
    "**Comparativa:**\n",
    "\n",
    "| Variante       | Precisión en transferencia | Latencia | Parámetros   |\n",
    "|----------------|-----------------------------|----------|--------------|\n",
    "| DAN            | Media                       | Muy baja | ~20M         |\n",
    "| Transformer    | Alta                        | Media    | ~110M        |\n",
    "\n",
    "**Aplicaciones generales**\n",
    "\n",
    "Las representaciones entrenadas en NLI o en tareas multi‑tarea demuestran desempeño sólido en:\n",
    "- **Clasificación de sentimientos**         \n",
    "- **Recuperación de información**           \n",
    "- **Detección de similitud semántica**      \n",
    "- **Respuesta a preguntas (QA) extractiva**\n",
    "\n",
    "Además, al ser preentrenamientos, pueden reutilizarse sin modificaciones en tareas con pocos datos propios.\n",
    "\n",
    "#### **Representación dirigida por tarea**\n",
    "\n",
    "Cuando el objetivo final difiere significativamente del preentrenamiento general, adaptar la representación de oración al dominio y la tarea específica mejora el rendimiento.\n",
    "\n",
    "**Atención supervisada sobre tokens relevantes**\n",
    "\n",
    "Se añade una capa de atención que pondera cada token $h_t$ según su importancia para la tarea:\n",
    "\n",
    "- Cálculo de puntuaciones:\n",
    "  $$\n",
    "    e_t = v_a^\\top \\tanh(W_h h_t + W_s s),\n",
    "  $$\n",
    "  donde $s$ puede ser un vector de estado inicial o contexto externo, y $v_a,W_h,W_s$ son parámetros aprendidos.\n",
    "\n",
    "- Normalización con softmax:\n",
    "  $$\n",
    "    \\alpha_t = \\frac{\\exp(e_t)}{\\sum_{k=1}^n \\exp(e_k)}.\n",
    "  $$\n",
    "\n",
    "- Vector de oración ponderado:\n",
    "  $$\n",
    "    v_{\\mathrm{task}} = \\sum_{t=1}^n \\alpha_t h_t.\n",
    "  $$\n",
    "\n",
    "Este mecanismo permite al modelo enfocarse en fragmentos críticos (por ejemplo, negaciones o entidades) y reduce el ruido de tokens irrelevantes.\n",
    "\n",
    "**Multi-task Learning y pérdida conjunta**\n",
    "\n",
    "Al entrenar simultáneamente en la tarea principal y tareas auxiliares (por ejemplo, etiquetado de entidades, detección de sentimientos), se define una pérdida combinada:\n",
    "  $$\n",
    "    \\mathcal{L} = \\lambda_p \\mathcal{L}_{\\mathrm{principal}} + \\sum_{i=1}^m \\lambda_i \\mathcal{L}_{\\mathrm{aux}_i},\n",
    "  $$\n",
    "con $\\lambda_p,\\lambda_i$ pesos de cada tarea y pérdidas específicas $\\mathcal{L}_{\\mathrm{aux}}$ (entropía cruzada, CRF, etc.). Esta configuración refuerza las características compartidas y mejora la generalización.\n",
    "\n",
    "**Fine‑tuning de modelos preentrenados (Transfer Learning)**\n",
    "\n",
    "Métodos modernos como BERT incorporan un preentrenamiento masivo y capas de Transformer. Para tareas de clasificación de oraciones se procede a:\n",
    "\n",
    "1. **Añadir cabecera de clasificación:** un vector de salida `[CLS]` se conecta a una capa densa:\n",
    "   $$\n",
    "     \\hat y = \\mathrm{softmax}(W_{cls} h_{[CLS]} + b_{cls}).\n",
    "   $$\n",
    "2. **Fine‑tuning end‑to‑end:** todos los parámetros de BERT y la capa adicional se actualizan mínimo un par de epochs con tasa de aprendizaje baja.\n",
    "3. **Pérdida de entropía cruzada:**\n",
    "   $$\n",
    "     \\mathcal{L}_{\\mathrm{BERT-tune}} = -\\sum_{c} y_c \\log \\hat y_c.\n",
    "   $$\n",
    "\n",
    "Este enfoque adapta las representaciones de oraciones a características propias de la tarea (voz activa/pasiva, tono, polaridad).\n",
    "\n",
    "**Atención multi-head y pooling adaptativo**\n",
    "\n",
    "Al utilizar Transformers, se pueden extraer representaciones dirigidas mediante *attention pooling*:\n",
    "\n",
    "1. **Multi-head Attention Output:** de la última capa se obtienen vectores $H = [h_1,\\dots,h_n]$.\n",
    "2. **Cálculo de logits de atención para cada cabecera:**\n",
    "   $$\n",
    "     A = \\mathrm{softmax}\\bigl(W_A H^\\top\\bigr),\n",
    "   $$\n",
    "   produciendo pesos $\\alpha_{t}^{(i)}$ por head.\n",
    "3. **Pooling adaptativo:**\n",
    "   $$\n",
    "     v_{\\mathrm{mh}} = \\bigl[\\sum_t \\alpha_t^{(1)} h_t;\\dots;\\sum_t \\alpha_t^{(k)} h_t\\bigr],\n",
    "   $$\n",
    "   concatenando los outputs de $k$ cabeceras para capturar perspectivas múltiples.\n",
    "\n",
    "**Comparativa de enfoques dirigidos**\n",
    "\n",
    "| Método                    | Adaptación | Dependencia de datos | Costo computacional |\n",
    "|---------------------------|------------|----------------------|---------------------|\n",
    "| Capa atención simple      | Media      | Moderada             | Baja–Media          |\n",
    "| Multi-task Learning       | Alta       | Alta                 | Media–Alta          |\n",
    "| Fine‑tuning BERT          | Muy alta   | Moderada–Alta        | Alta                |\n",
    "| Attention pooling adapt.  | Media      | Moderada             | Media               |\n",
    "\n",
    "Cada técnica presenta un balance entre la cantidad de datos etiquetados necesarios, la flexibilidad para tareas nuevas y el coste de entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# Datos de ejemplo: batch de oraciones\n",
    "sentences = [\n",
    "    \"Deep learning models capture complex semantics.\",\n",
    "    \"Transformers excel at long-range dependencies.\",\n",
    "    \"Sentence representations power many NLP tasks.\"\n",
    "]\n",
    "batch_size = len(sentences)\n",
    "max_len = 10  # Número máximo de tokens por oración (relleno)\n",
    "\n",
    "# 1. Tokenización simple (solo para ilustrar, se recomienda usar un tokenizer real) \n",
    "# Se crea un vocabulario con índice para cada palabra\n",
    "vocab = {w: i+1 for i, w in enumerate(set(\" \".join(sentences).split()))}\n",
    "vocab[\"<pad>\"] = 0  # Índice para padding\n",
    "\n",
    "# Función para tokenizar y aplicar padding a las oraciones\n",
    "def tokenize_and_pad(sents, vocab, max_len):\n",
    "    tokenized = []\n",
    "    for s in sents:\n",
    "        tokens = [vocab.get(w, 0) for w in s.split()][:max_len]\n",
    "        tokens = tokens + [0] * (max_len - len(tokens))  # Padding\n",
    "        tokenized.append(tokens)\n",
    "    return torch.tensor(tokenized, dtype=torch.long)\n",
    "\n",
    "inputs = tokenize_and_pad(sentences, vocab, max_len)\n",
    "\n",
    "# 2. Modelo InferSent: Bi-LSTM + max pooling\n",
    "class InferSent(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)  # Capa de embedding\n",
    "        self.encoder = nn.LSTM(emb_dim, hid_dim, bidirectional=True, batch_first=True)  # Bi-LSTM\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.emb(x)  # (Batch, Longitud, Dimensión)\n",
    "        h, _ = self.encoder(emb)  # (Batch, Longitud, 2 * Hidden)\n",
    "        pooled, _ = torch.max(h, dim=1)  # Max pooling a lo largo de la secuencia\n",
    "        return pooled  # Representación final (Batch, 2H)\n",
    "\n",
    "#3. Modelo USE basado en Deep Averaging Network (DAN) \n",
    "class USE_DAN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.fc1 = nn.Linear(emb_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.emb(x).mean(dim=1)  # Promedio de embeddings por oración\n",
    "        h = F.relu(self.fc1(emb))     # Primera capa oculta con ReLU\n",
    "        out = F.relu(self.fc2(h))     # Capa de salida con ReLU\n",
    "        return out  # Representación final\n",
    "\n",
    "# 4. Modelo USE con codificador Transformer (una sola capa)\n",
    "class USE_Trans(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, n_heads, hidden_dim, n_layers):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=emb_dim, nhead=n_heads, dim_feedforward=hidden_dim\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.emb(x).permute(1,0,2)  # Cambia a formato (Longitud, Batch, Dim)\n",
    "        out = self.encoder(emb)          # Salida del Transformer\n",
    "        cls = out[0]                     # Usar primer token como representación tipo [CLS]\n",
    "        return cls\n",
    "\n",
    "#  5. Mecanismo de atención para hacer pooling sobre salidas de Bi-LSTM \n",
    "class AttentionPool(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hid_dim, 1)  # Capa de atención\n",
    "\n",
    "    def forward(self, h):\n",
    "        # h tiene forma (Batch, Longitud, Dim)\n",
    "        scores = self.attn(h).squeeze(-1)        # Obtiene puntuaciones (Batch, Longitud)\n",
    "        weights = F.softmax(scores, dim=1)       # Normaliza puntuaciones\n",
    "        pooled = torch.bmm(weights.unsqueeze(1), h).squeeze(1)  # Aplica atención\n",
    "        return pooled  # Representación con atención\n",
    "\n",
    "# 6. Clasificador con fine-tuning de BERT\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, out_dim):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")  # Cargar BERT preentrenado\n",
    "        self.cls_fc = nn.Linear(self.bert.config.hidden_size, out_dim)  # Capa final\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        cls_token = outputs.last_hidden_state[:, 0, :]  # Representación del token [CLS]\n",
    "        logits = self.cls_fc(cls_token)  # Logits de salida\n",
    "        return logits\n",
    "\n",
    "# 7. Modelo multi-tarea con InferSent compartido + dos salidas\n",
    "class MultiTaskSent(nn.Module):\n",
    "    def __init__(self, encoder, out1, out2):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.head1 = nn.Linear(encoder.encoder.hidden_size*2, out1)  # Tarea 1\n",
    "        self.head2 = nn.Linear(encoder.encoder.hidden_size*2, out2)  # Tarea 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        rep = self.encoder(x)  # Representación de oración (Batch, 2H)\n",
    "        y1 = self.head1(rep)   # Salida para tarea 1\n",
    "        y2 = self.head2(rep)   # Salida para tarea 2\n",
    "        return y1, y2\n",
    "\n",
    "# Instanciar modelos con parámetros de ejemplo \n",
    "vocab_size = len(vocab)\n",
    "emb_dim = 64\n",
    "hid_dim = 128\n",
    "\n",
    "use_dan = USE_DAN(vocab_size, emb_dim, 64, 128)\n",
    "infer_sent = InferSent(vocab_size, emb_dim, hid_dim)\n",
    "use_trans = USE_Trans(vocab_size, emb_dim, n_heads=4, hidden_dim=256, n_layers=1)\n",
    "attn_pool = AttentionPool(hid_dim*2)\n",
    "multi_task = MultiTaskSent(infer_sent, out1=3, out2=2)\n",
    "\n",
    "# Pruebas de paso hacia adelante (forward)\n",
    "x = inputs  # Tensor de entrada (Batch, Longitud)\n",
    "\n",
    "print(\" Forma de  InferSent:\", infer_sent(x).shape)\n",
    "print(\"Forma del USE DAN:\", use_dan(x).shape)\n",
    "print(\"Forma de USE Trans:\", use_trans(x).shape)\n",
    "\n",
    "# Obtener salidas del Bi-LSTM para atención\n",
    "h_bilstm, _ = infer_sent.encoder(use_dan.emb(x))\n",
    "print(\"Forma de salida del AttentionPool:\", attn_pool(h_bilstm).shape)\n",
    "\n",
    "# Salidas del modelo multitarea\n",
    "mt1, mt2 = multi_task(x)\n",
    "print(\"Forma de las cabeceras de  multiTask:\", mt1.shape, mt2.shape)\n",
    "\n",
    "# Clasificación con BERT\n",
    "# Tokenización con tokenizer real\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "encoded = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Instanciar clasificador BERT y hacer forward\n",
    "bert_clf = BERTClassifier(out_dim=2)\n",
    "logits = bert_clf(encoded[\"input_ids\"], attention_mask=encoded[\"attention_mask\"])\n",
    "print(\"Forma de los logits del BERTClassifier:\", logits.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Representaciones distribuidas de documentos** \n",
    "\n",
    "Doc2vec nos permite aprender directamente las representaciones de textos de longitud arbitraria (frases, oraciones, párrafos y documentos), teniendo en cuenta el contexto de las palabras del texto.\n",
    "\n",
    "Esto es similar a Word2vec en términos de su arquitectura general, excepto que, además de los vectores de palabras, también aprende un \"vector de párrafo\" que aprende una representación del texto completo (es decir, con palabras en contexto). Cuando se aprende con un corpus grande de muchos textos, los vectores de párrafo son únicos para un texto determinado (donde \"texto\" puede significar cualquier fragmento de texto de longitud arbitraria), mientras que los vectores de palabras se compartirán en todos los textos.  \n",
    "\n",
    "Hay dos arquitecturas del modelo Doc2Vec, que es una extensión de Word2Vec diseñada para generar representaciones vectoriales no solo para palabras sino también para piezas de texto más grandes como oraciones, párrafos y documentos. Estas representaciones vectoriales son útiles para muchas tareas de procesamiento del lenguaje natural, como la clasificación de textos y la búsqueda semántica. Aquí están las dos arquitecturas: \n",
    "\n",
    "**Memoria distribuida (DM)**: \n",
    "\n",
    "En el modelo DM de Doc2Vec, cada palabra y el párrafo (o documento) entero tienen su propio vector de aprendizaje único en una \"Paragraph Matrix\" y en una \"Word Matrix\", respectivamente. \n",
    "\n",
    "Durante el entrenamiento, el modelo intenta predecir la siguiente palabra en un contexto dada una ventana de palabras y el vector único del párrafo/documento. \n",
    "\n",
    "Los vectores de las palabras y del párrafo se pueden promediar o concatenar antes de enviarlos a una capa de clasificador, que intenta predecir la palabra siguiente. \n",
    "\n",
    "El objetivo es que al final del entrenamiento, el vector del párrafo capture la esencia del texto, lo que hace posible usar este vector para tareas de clasificación o comparación de similitud. \n",
    "\n",
    "**Bolsa de palabras distribuidas (DBOW)**: \n",
    "\n",
    "El modelo DBOW funciona de manera inversa al DM. Ignora el contexto de las palabras y, en su lugar, fuerza al modelo a predecir las palabras en un párrafo/documento dada solo la identificación del párrafo (es decir, su vector único). \n",
    "\n",
    "No hay una capa de promedio o concatenación; el modelo directamente predice las palabras a partir del vector del párrafo. \n",
    "\n",
    "Al igual que en el modelo DM, el vector del párrafo se entrena para representar el contenido completo del párrafo/documento. \n",
    "\n",
    "DBOW es eficaz para grandes conjuntos de datos donde la semántica puede ser capturada incluso sin el orden exacto de las palabras. \n",
    "\n",
    "Ambos métodos son útiles para aprender representaciones vectoriales que reflejan el significado de los párrafos o documentos, aunque capturan diferentes aspectos de los datos: DM toma en cuenta el orden de las palabras, mientras que DBOW se centra en la ocurrencia de las palabras. Estos vectores resultantes pueden ser utilizados en diversas tareas, tales como agrupación de documentos, clasificación y búsqueda por similitud semántica. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "# 1. Datos sintéticos: párrafos y frases\n",
    "paragraphs = {\n",
    "    0: [\"the cat sat on the mat\".split(), \"the cat is happy\".split()],\n",
    "    1: [\"dogs are loyal animals\".split(), \"my dog loves to play\".split()],\n",
    "    2: [\"deep learning models require lots of data\".split()]\n",
    "}\n",
    "\n",
    "# 2. Construir vocabulario\n",
    "all_words = set(w for para in paragraphs.values() for sent in para for w in sent)\n",
    "word2idx = {w: i for i, w in enumerate(sorted(all_words), start=1)}\n",
    "word2idx[\"<pad>\"] = 0\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "V = len(word2idx)\n",
    "P = len(paragraphs)\n",
    "\n",
    "# 3. Parámetros de modelo\n",
    "emb_dim = 50\n",
    "context_size = 2  # ventana izquierda y derecha\n",
    "batch_size = 4\n",
    "\n",
    "# 4. Dataset DM: (para cada palabra central, contexto + para_id) -> target word\n",
    "class Doc2VecDMDataset(Dataset):\n",
    "    def __init__(self, paragraphs, word2idx, context_size):\n",
    "        self.samples = []\n",
    "        for pid, sents in paragraphs.items():\n",
    "            for sent in sents:\n",
    "                indices = [word2idx[w] for w in sent]\n",
    "                for i, target in enumerate(indices):\n",
    "                    context = []\n",
    "                    for offset in range(-context_size, context_size+1):\n",
    "                        j = i + offset\n",
    "                        if offset != 0 and 0 <= j < len(indices):\n",
    "                            context.append(indices[j])\n",
    "                    if context:\n",
    "                        self.samples.append((pid, context, target))\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        pid, context, target = self.samples[idx]\n",
    "        # pad context to fixed size\n",
    "        pad = [0] * (2*context_size - len(context))\n",
    "        context = context + pad\n",
    "        return torch.tensor(pid), torch.tensor(context), torch.tensor(target)\n",
    "\n",
    "# 5. Modelo DM\n",
    "class Doc2VecDM(nn.Module):\n",
    "    def __init__(self, vocab_size, para_count, emb_dim, context_size):\n",
    "        super().__init__()\n",
    "        self.word_emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.para_emb = nn.Embedding(para_count, emb_dim)\n",
    "        self.linear = nn.Linear((2*context_size)*emb_dim + emb_dim, vocab_size)\n",
    "    def forward(self, pid, context_idxs):\n",
    "        # context_idxs: (B, 2*context_size)\n",
    "        w_ctx = self.word_emb(context_idxs)  # (B, 2C, D)\n",
    "        w_ctx = w_ctx.view(w_ctx.size(0), -1)  # (B, 2C*D)\n",
    "        p_vec = self.para_emb(pid)             # (B, D)\n",
    "        x = torch.cat([w_ctx, p_vec], dim=1)   # (B, 2C*D + D)\n",
    "        logits = self.linear(x)                # (B, V)\n",
    "        return logits\n",
    "\n",
    "# 6. Modelo DBOW\n",
    "class Doc2VecDBOW(nn.Module):\n",
    "    def __init__(self, para_count, vocab_size, emb_dim):\n",
    "        super().__init__()\n",
    "        self.para_emb = nn.Embedding(para_count, emb_dim)\n",
    "        self.linear = nn.Linear(emb_dim, vocab_size)\n",
    "    def forward(self, pid):\n",
    "        p_vec = self.para_emb(pid)  # (B, D)\n",
    "        logits = self.linear(p_vec) # (B, V)\n",
    "        return logits\n",
    "\n",
    "# 7. Preparar dataloaders\n",
    "dm_dataset = Doc2VecDMDataset(paragraphs, word2idx, context_size)\n",
    "dm_loader = DataLoader(dm_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Para DBOW, generamos muestras (para_id -> random word from paragraph)\n",
    "dbow_samples = []\n",
    "for pid, sents in paragraphs.items():\n",
    "    words = [word2idx[w] for sent in sents for w in sent]\n",
    "    for w in words:\n",
    "        dbow_samples.append((pid, w))\n",
    "class DBOWDataset(Dataset):\n",
    "    def __init__(self, samples):\n",
    "        self.samples = samples\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, i): return torch.tensor(self.samples[i][0]), torch.tensor(self.samples[i][1])\n",
    "dbow_loader = DataLoader(DBOWDataset(dbow_samples), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 8. Entrenamiento de ejemplo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dm_model = Doc2VecDM(V, P, emb_dim, context_size).to(device)\n",
    "dbow_model = Doc2VecDBOW(P, V, emb_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "dm_opt = optim.Adam(dm_model.parameters(), lr=0.01)\n",
    "dbow_opt = optim.Adam(dbow_model.parameters(), lr=0.01)\n",
    "\n",
    "# 9. Un paso de entrenamiento DM\n",
    "pid, ctx, tgt = next(iter(dm_loader))\n",
    "pid, ctx, tgt = pid.to(device), ctx.to(device), tgt.to(device)\n",
    "dm_model.train()\n",
    "dm_opt.zero_grad()\n",
    "logits = dm_model(pid, ctx)\n",
    "loss_dm = criterion(logits, tgt)\n",
    "loss_dm.backward()\n",
    "dm_opt.step()\n",
    "print(f\"Perdida DM: {loss_dm.item():.4f}\")\n",
    "\n",
    "# 10. Un paso de entrenamiento DBOW\n",
    "pid2, tgt2 = next(iter(dbow_loader))\n",
    "pid2, tgt2 = pid2.to(device), tgt2.to(device)\n",
    "dbow_model.train()\n",
    "dbow_opt.zero_grad()\n",
    "logits2 = dbow_model(pid2)\n",
    "loss_dbow = criterion(logits2, tgt2)\n",
    "loss_dbow.backward()\n",
    "dbow_opt.step()\n",
    "print(f\"Perdida DBOW : {loss_dbow.item():.4f}\")\n",
    "\n",
    "# 11. Obtener embeddings de párrafo y palabras\n",
    "with torch.no_grad():\n",
    "    para_vecs = dm_model.para_emb.weight  # (P, D)\n",
    "    word_vecs = dm_model.word_emb.weight  # (V, D)\n",
    "print(\"Para[0] muestra de embedding :\", para_vecs[0][:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Representación orientada a tareas**\n",
    "\n",
    "En entornos de procesamiento de lenguaje natural (NLP) y recuperación de información (IR), los documentos adoptan variadas formas y tamaños, desde párrafos breves hasta informes completos. Si bien los embeddings generales de documentos (como Doc2Vec en sus variantes DM y DBOW) ofrecen vectores eficaces para tareas amplias, en escenarios específicos es fundamental **afinar** estas representaciones para reflejar las necesidades particulares de la aplicación final.\n",
    "\n",
    "La **representación orientada a tareas** surge para reducir la brecha entre la generalidad del embedding base y la especialización requerida por tareas como resumen automático, clasificación de textos o sistemas de pregunta‑respuesta (QA).\n",
    "\n",
    "Exploramos dos estrategias principales: el **fine‑tuning** de embeddings preentrenados mediante retropropagación y el **aprendizaje multi‑tarea** (\"multi-task learning\"), donde múltiples objetivos se entrenan de forma conjunta. Ambas metodologías se apoyan en formulaciones matemáticas rigurosas para adaptar vectores de documento a dominios concretos.\n",
    "\n",
    "#### **Fine‑tuning de embeddings de documento**\n",
    "\n",
    "El proceso de fine‑tuning consiste en tomar un modelo preentrenado, por ejemplo, un encoder de documentos en arquitectura Transformer o un Doc2Vec y continuar su entrenamiento en un conjunto etiquetado para la tarea meta. Imagina que disponemos de un conjunto de documentos $\\{D_i\\}_{i=1}^N$, cada uno con su embedding inicial $v_i^{(0)}\\in \\mathbb{R}^d$, generado por la red base. Para una tarea de clasificación, asociamos a cada documento una etiqueta $y_i\\in \\{1,\\dots,C\\}$.\n",
    "\n",
    "**Función objetivo y retropropagación**\n",
    "\n",
    "Se define una capa de clasificación sobre el embedding:\n",
    "\n",
    "$$\n",
    "  \\hat p_i = \\mathrm{softmax}(W_{c} v_i + b_{c}),\n",
    "$$\n",
    "\n",
    "donde $W_{c}\\in \\mathbb{R}^{C\\times d}$ y $b_{c}\\in\\mathbb{R}^C$ son parámetros de la capa de salida, y\n",
    "\n",
    "$$\n",
    "  \\hat p_{i,j} = \\frac{\\exp\\bigl((W_{c} v_i + b_{c})_j\\bigr)}{\\sum_{k=1}^C \\exp\\bigl((W_{c} v_i + b_{c})_k\\bigr)}.\n",
    "$$\n",
    "\n",
    "La pérdida de entropía cruzada para un solo ejemplo es:\n",
    "\n",
    "$$\n",
    "  \\mathcal{L}_i = -\\sum_{j=1}^C \\mathbb{I}_{[y_i=j]} \\log\\bigl(\\hat p_{i,j}\\bigr).\n",
    "$$\n",
    "\n",
    "El fine‑tuning también actualiza los parámetros internos $\\theta$ del encoder de documento (sea Transformer, RNN o Doc2Vec). El objetivo total sobre el dataset es:\n",
    "\n",
    "$$\n",
    "  \\mathcal{L}_{\\mathrm{fine}} = \\frac{1}{N} \\sum_{i=1}^N \\mathcal{L}_i + \\lambda\\,\\lVert \\theta \\rVert^2,\n",
    "$$\n",
    "\n",
    "donde $\\lambda$ aplica regularización L2 para evitar sobreajuste. El descenso por gradiente (o variantes como Adam) actualiza simultáneamente $W_c, b_c$ y $\\theta$:\n",
    "\n",
    "$$\n",
    "  \\theta \\leftarrow \\theta - \\eta \\frac{\\partial \\mathcal{L}_{\\mathrm{fine}}}{\\partial \\theta},\n",
    "  \\quad\n",
    "  W_{c} \\leftarrow W_{c} - \\eta \\frac{\\partial \\mathcal{L}_{\\mathrm{fine}}}{\\partial W_{c}},\n",
    "$$\n",
    "\n",
    "y $\\eta$ es la tasa de aprendizaje. Este ajuste continuo permite que el embedding capture patrones cruciales para la clasificación de documentos, tales como la presencia de términos discriminativos, la estructura argumental y la coherencia temática.\n",
    "\n",
    "**Ejemplo: fine‑tuning para resumen extractivo**\n",
    "\n",
    "En resumen automático extractivo, se entrena un modelo que aprueba o rechaza cada frase $s_{i,j}$ de un documento $D_i$. Se obtiene el embedding de frase $v_{i,j}$ y se aplica:\n",
    "\n",
    "$$\n",
    "  \\hat p_{i,j} = \\sigma\\bigl(w_{s}^{\\top} v_{i,j} + b_{s}\\bigr),\n",
    "$$\n",
    "\n",
    "con $\\sigma(x) = 1/(1+e^{-x})$. La pérdida binaria log‑loss:\n",
    "\n",
    "$$\n",
    "  \\mathcal{L}_{i,j} = -\\bigl[y_{i,j}\\log\\hat p_{i,j} + (1-y_{i,j})\\log(1-\\hat p_{i,j})\\bigr].\n",
    "$$\n",
    "\n",
    "Fine‑tuning ajusta tanto $w_s, b_s$ como $\\theta$ del encoder de frase, permitiendo que la representación enfatice contenidos relevantes para el resumen.\n",
    "\n",
    "**Ventajas y desafíos del fine‑tuning**\n",
    "\n",
    "- **Ventajas:**\n",
    "  - Alta precisión en el dominio específico.  \n",
    "  - Aprovechamiento de conocimiento general preentrenado.  \n",
    "\n",
    "- **Desafíos:**\n",
    "  - Riesgo de sobreajuste con pocos datos etiquetados.  \n",
    "  - Coste computacional elevado si el encoder es muy grande.  \n",
    "  - Necesidad de ajustes cuidadosos de la tasa de aprendizaje $\\eta$ y regularización $\\lambda$.  \n",
    "\n",
    "#### **Aprendizaje multi‑tarea (Multi‑Task Learning)**\n",
    "\n",
    "El **multi-task learning** optimiza simultáneamente varias tareas relacionadas, compartiendo representaciones subyacentes y promoviendo la transferencia de conocimiento entre tareas. Para documentos, esto puede incluir tareas como clasificación temática, análisis de sentimiento y extracción de entidades.\n",
    "\n",
    "**Formulación matemática**\n",
    "\n",
    "Supongamos $T$ tareas, cada una con función de pérdida $\\mathcal{L}^{(t)}$. Un ejemplo de tareas podría ser:\n",
    "\n",
    "1. **Resumen extractivo**: $\\mathcal{L}^{(1)}$.  \n",
    "2. **Clasificación de tema**: $\\mathcal{L}^{(2)}$.  \n",
    "3. **Detección de sentimiento**: $\\mathcal{L}^{(3)}$.\n",
    "\n",
    "Se comparte el encoder de documento con parámetros $\\theta_s$ (“shared”), y cada tarea posee parámetros específicos $\\theta_t$ (\"task‑specific\"). La pérdida total se define como:\n",
    "\n",
    "$$\n",
    "  \\mathcal{L}_{\\mathrm{MTL}} = \\sum_{t=1}^T \\lambda_t \\frac{1}{N_t} \\sum_{i=1}^{N_t} \\mathcal{L}_i^{(t)}(\\theta_s,\\theta_t),\n",
    "$$\n",
    "\n",
    "con pesos $\\lambda_t>0$ que equilibran la importancia de cada tarea y $N_t$ el número de ejemplos de la tarea $t$. Las actualizaciones por gradiente son:\n",
    "\n",
    "$$\n",
    "  \\theta_s \\leftarrow \\theta_s - \\eta \\sum_{t=1}^T \\lambda_t \\frac{\\partial \\mathcal{L}^{(t)}}{\\partial \\theta_s},\n",
    "$$\n",
    "$$\n",
    "  \\theta_t \\leftarrow \\theta_t - \\eta \\lambda_t \\frac{\\partial \\mathcal{L}^{(t)}}{\\partial \\theta_t},\\quad t=1,\\dots,T.\n",
    "$$\n",
    "\n",
    "Este esquema aprovecha la señal adicional de tareas relacionadas para mejorar la calidad del embedding compartido $\\theta_s$, reduciendo la probabilidad de sobreajuste y mejorando la generalización.\n",
    "\n",
    "**Ejemplo: tareas combinadas de QA y clasificación**\n",
    "\n",
    "En un sistema de pregunta‑respuesta, podemos combinar:\n",
    "- **Respuesta extractiva (QA):** $\\mathcal{L}^{(1)}$ entropía cruzada sobre spans de texto.  \n",
    "- **Clasificación de tema:** $\\mathcal{L}^{(2)}$ entropía cruzada sobre categorías.\n",
    "\n",
    "La capa compartida $\\theta_s$ aprende características sintácticas útiles para identificar tanto las secciones relevantes de texto como el dominio temático, mientras que $\\theta_1$ y $\\theta_2$ se especializan en cada tarea.\n",
    "\n",
    "**Regularización implícita y explícita**\n",
    "\n",
    "- **Regularización implícita:** el multi-task actúa como un \"armado inductivo\", ya que la necesidad de resolver varias tareas impide sobreajustar una sola.  \n",
    "- **Regularización explícita:** se pueden añadir términos penalizadores similares a L2 o dropout en $\\theta_s$ para mejorar la robustez.\n",
    "\n",
    "**Weighting dinámico de tareas**\n",
    "\n",
    "Determinar pesos $\\lambda_t$ adecuados es crítico. Estrategias dinámicas incluyen:\n",
    "1. **Uncertainty weighting** (Kendall et al., 2018): el peso se ajusta según la varianza de la pérdida de cada tarea, definiendo $\\lambda_t = 1/(2\\sigma_t^2)$ y aprendiendo $\\sigma_t$.  \n",
    "2. **GradNorm** (Chen et al., 2018): equilibra magnitudes de gradiente para que ninguna tarea domine la actualización de $\\theta_s$.  \n",
    "Ambos métodos conducen a mejoras en la convergencia y a representaciones más equilibradas.\n",
    "\n",
    "**Comparativa y consideraciones de diseño**\n",
    "\n",
    "| Aspecto                | Fine‑tuning simple  | Multi‑Task Learning      | Ventaja clave                               |\n",
    "|------------------------|---------------------|--------------------------|---------------------------------------------|\n",
    "| Dependencia de datos   | Alta                | Moderada                 | MTL necesita múltiples anotaciones          |\n",
    "| Costo computacional    | Alto (por tarea)    | Compartido                | MTL entrena un solo encoder para varias     |\n",
    "| Riesgo de sobreajuste  | Alto (pocos datos)  | Bajo                     | MTL regulariza implícitamente               |\n",
    "| Adaptabilidad          | Alta                | Media                    | Fine‑tuning específico para cada tarea      |\n",
    "\n",
    "\n",
    "**Extensiones y tendencias actuales**\n",
    "\n",
    "1. **Meta‑Learning para NLP:** se entrena un modelo que aprende a adaptarse rápidamente a tareas nuevas con pocos ejemplos (learning-to-learn).  \n",
    "2. **Adapters en Transformers:** módulos ligeros insertados entre capas preentrenadas, entrenables por tarea, reducen drásticamente el número de parámetros ajustados.  \n",
    "3. **Prompt‑Tuning:** en modelos de lenguaje grande, se optimizan vectores de \"prompt\" para dirigir el encoder a la tarea deseada sin modificar parámetros básicos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1. Datos sintéticos: documentos y etiquetas para dos tareas\n",
    "docs = [\n",
    "    \"deep learning improves nlp tasks\",\n",
    "    \"distributed representations capture semantics\",\n",
    "    \"fine tuning adapts embeddings to domain\",\n",
    "    \"multi task learning shares knowledge\",\n",
    "    \"transformers use attention mechanisms\"\n",
    "]\n",
    "labels_task1 = [0, 1, 0, 1, 1]      # e.g., topic classification (2 clases)\n",
    "labels_task2 = [1, 0, 1, 0, 1]      # e.g., sentiment (positivo=1/negativo=0)\n",
    "\n",
    "# 2. Tokenización simple y vocabulario\n",
    "tokenized = [doc.split() for doc in docs]\n",
    "vocab = set(word for sent in tokenized for word in sent)\n",
    "word2idx = {w: i+1 for i, w in enumerate(sorted(vocab))}\n",
    "word2idx[\"<pad>\"] = 0\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "# 3. Dataset y DataLoader\n",
    "max_len = max(len(sent) for sent in tokenized)\n",
    "class DocDataset(Dataset):\n",
    "    def __init__(self, tokenized, labels1, labels2, w2i, max_len):\n",
    "        self.data = tokenized\n",
    "        self.labels1 = labels1\n",
    "        self.labels2 = labels2\n",
    "        self.w2i = w2i\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.data[idx]\n",
    "        ids = [self.w2i[w] for w in tokens]\n",
    "        # padding\n",
    "        ids = ids + [0] * (self.max_len - len(ids))\n",
    "        return torch.tensor(ids), torch.tensor(self.labels1[idx]), torch.tensor(self.labels2[idx])\n",
    "\n",
    "dataset = DocDataset(tokenized, labels_task1, labels_task2, word2idx, max_len)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# 4. Encoder de documento: promedio de embeddings seguido de MLP\n",
    "class DocEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.fc = nn.Linear(emb_dim, hidden_dim)\n",
    "    def forward(self, x):\n",
    "        # x: (B, L)\n",
    "        emb = self.emb(x)                # (B, L, D)\n",
    "        mean = emb.mean(dim=1)           # (B, D)\n",
    "        h = torch.relu(self.fc(mean))    # (B, H)\n",
    "        return h\n",
    "\n",
    "# 5. Fine-tuning para tarea de clasificación (task1)\n",
    "class FineTuneClassifier(nn.Module):\n",
    "    def __init__(self, encoder, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "    def forward(self, x):\n",
    "        v = self.encoder(x)              # embedding víctorial v_i\n",
    "        logits = self.classifier(v)      # W_c v_i + b_c\n",
    "        return logits, v\n",
    "\n",
    "# 6. Entrenamiento de fine-tuning\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "enc = DocEncoder(vocab_size, emb_dim=16, hidden_dim=32).to(device)\n",
    "model_ft = FineTuneClassifier(enc, hidden_dim=32, num_classes=2).to(device)\n",
    "opt_ft = optim.Adam(model_ft.parameters(), lr=1e-3, weight_decay=1e-4)  # lambda L2\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Mostrar fórmula de pérdida:\n",
    "# L_fine = 1/N sum_i [-sum_j I[y_i=j] log p_{i,j}] + lambda ||theta||^2\n",
    "\n",
    "for epoch in range(3):\n",
    "    model_ft.train()\n",
    "    total_loss = 0\n",
    "    for x, y1, _ in loader:\n",
    "        x, y1 = x.to(device), y1.to(device)\n",
    "        logits, v = model_ft(x)\n",
    "        loss = criterion(logits, y1)\n",
    "        opt_ft.zero_grad()\n",
    "        loss.backward()\n",
    "        opt_ft.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"[Fine-tuning] Epoch {epoch+1}, Loss: {total_loss/len(loader):.4f}\")\n",
    "\n",
    "# 7. Multi-task Learning: compartir encoder, dos cabeceras\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, encoder, hidden_dim, num_classes1, num_classes2):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.head1 = nn.Linear(hidden_dim, num_classes1)  # theta_1\n",
    "        self.head2 = nn.Linear(hidden_dim, num_classes2)  # theta_2\n",
    "    def forward(self, x):\n",
    "        shared = self.encoder(x)        # theta_s\n",
    "        out1 = self.head1(shared)       # logits tarea1\n",
    "        out2 = self.head2(shared)       # logits tarea2\n",
    "        return out1, out2\n",
    "\n",
    "# 8. Entrenamiento multi-task\n",
    "mtl_model = MultiTaskModel(enc, hidden_dim=32, num_classes1=2, num_classes2=2).to(device)\n",
    "opt_mtl = optim.Adam(mtl_model.parameters(), lr=1e-3)\n",
    "# Pesos lambda_t\n",
    "lambda1, lambda2 = 0.7, 0.3\n",
    "\n",
    "# Pérdida total: L_MTL = lambda1 * L1 + lambda2 * L2\n",
    "for epoch in range(3):\n",
    "    mtl_model.train()\n",
    "    total_mtl = 0\n",
    "    for x, y1, y2 in loader:\n",
    "        x, y1, y2 = x.to(device), y1.to(device), y2.to(device)\n",
    "        logits1, logits2 = mtl_model(x)\n",
    "        loss1 = criterion(logits1, y1)\n",
    "        loss2 = criterion(logits2, y2)\n",
    "        loss = lambda1 * loss1 + lambda2 * loss2\n",
    "        opt_mtl.zero_grad()\n",
    "        loss.backward()\n",
    "        opt_mtl.step()\n",
    "        total_mtl += loss.item()\n",
    "    print(f\"[Multi-Task] Epoca {epoch+1}, Loss: {total_mtl/len(loader):.4f}\")\n",
    "\n",
    "# 9. Evaluación rápida\n",
    "mtl_model.eval()\n",
    "with torch.no_grad():\n",
    "    x, y1, y2 = next(iter(loader))\n",
    "    x = x.to(device)\n",
    "    p1, p2 = mtl_model(x)\n",
    "    pred1 = p1.argmax(dim=1)\n",
    "    pred2 = p2.argmax(dim=1)\n",
    "    print(\"Pred task1:\", pred1.cpu().tolist(), \" True:\", y1.tolist())\n",
    "    print(\"Pred task2:\", pred2.cpu().tolist(), \" True:\", y2.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Ejercicios\n",
    "\n",
    "#### 1. Embeddings de palabras avanzados\n",
    "\n",
    "**Objetivo**: Comparar calidad y sesgos de distintos embeddings pre‑entrenados (Word2Vec, GloVe, FastText).\n",
    "\n",
    "- **Tarea 1.1**: Elige tres modelos pre‑entrenados (por ejemplo, Google News Word2Vec, GloVe Common Crawl y FastText Wikipedia).  \n",
    "  - Diseña un benchmark de analogías (p. ej. \"king – man + woman ≈ queen\") y cálculo de similitud de pares (p. ej. datasets of RG65 o WordSim-353).  \n",
    "  - Mide exactitud en analogías y correlación Spearman en similitud.  \n",
    "  - Analiza la varianza de desempeño con distintas dimensiones (50, 100, 300).\n",
    "\n",
    "- **Tarea 1.2**: Evalúa sesgos de género y raciales en cada embedding.  \n",
    "  - Usa el método de \"Word embedding association test\" (WEAT) para cuantificar sesgo.  \n",
    "  - Propón una estrategia de mitigación (p. ej. neutralización y equalización).  \n",
    "  - Repite WEAT tras mitigación y comenta resultados.\n",
    "\n",
    "\n",
    "#### 2. Representaciones de frases con BoW\n",
    "\n",
    "**Objetivo**: Implementar y contrastar los esquemas BoW: promedio simple, SIF y TF‑IDF ponderado.\n",
    "\n",
    "- **Tarea 2.1**: Dado un corpus de frases pareadas (p. ej. STS Benchmark en inglés),  \n",
    "  - Calcula para cada frase sus representaciones con (i) promedio simple, (ii) SIF ($a=10^{-3}$) y (iii) TF‑IDF.  \n",
    "  - Mide correlación coseno vs. anotaciones humanas.  \n",
    "  - Grafica con TSNE las frases de mayor y menor similitud según cada método.\n",
    "\n",
    "- **Tarea 2.2**: Incorpora pooling adicional: max‑pooling, min‑pooling y concatenación mean | max | min.  \n",
    "  - Evalúa cómo cambia la correlación STS y el coste de cómputo.  \n",
    "  - Concluye qué esquema es más adecuado en un entorno de baja latencia.\n",
    "\n",
    "\n",
    "#### 3. Autoencoders de frases\n",
    "\n",
    "**Objetivo**: Profundizar en codificaciones latentes que capturen orden y robustez.\n",
    "\n",
    "- **Tarea 3.1**: Entrena un **Seq2Seq AE** para reconstruir frases de un corpus (p. ej. IMDB).  \n",
    "  - Mide pérdida de reconstrucción y porcentaje de tokens correctamente reconstruidos.  \n",
    "  - Verifica que el vector latente $z$ captura información de orden (intercambia dos palabras y mide la pérdida).\n",
    "\n",
    "- **Tarea 3.2**: Extiende a **Denoising AE**:  \n",
    "  - Aplica ruido (borra o sustituye tokens aleatorios) y entrena a reconstruir la frase original.  \n",
    "  - Evalúa robustez ante ruido: compara calidad de reconstrucción vs. AE estándar.\n",
    "\n",
    "- **Tarea 3.3**: Implementa un **Variational AE (VAE)** de frases.  \n",
    "  - Visualiza el espacio latente usando interpolaciones lineales entre dos frases y genera oraciones intermedias.  \n",
    "  - Comenta ventajas e inconvenientes de la entropía KL en el entrenamiento (p. ej. \"posterior collapse\").\n",
    "\n",
    "\n",
    "#### 4. Skip‑Thought y Transformers\n",
    "\n",
    "**Objetivo**: Explorar representaciones inter‑oracionales y basadas en atención.\n",
    "\n",
    "- **Tarea 4.1**: Entrena un Skip‑Thought (encoder Bi‑GRU + dos decoders) sobre un corpus de párrafos (p. ej. BookCorpus).  \n",
    "  - Evalúa las representaciones en tareas de **NLI** (SNLI) con un clasificador ad hoc.  \n",
    "  - Contrástalas con un modelo InferSent entrenado desde cero.\n",
    "\n",
    "- **Tarea 4.2**: Implementa un encoder Transformer (una capa) y extrae el token `[CLS]` como vector de oración.  \n",
    "  - Fine‑tunea este encoder en SNLI y mide exactitud vs. InferSent y Skip‑Thought.  \n",
    "  - Analiza la importancia del número de heads y la profundidad (1 vs. 2 capas).\n",
    "\n",
    "\n",
    "#### 5. Doc2Vec: DM vs. DBOW\n",
    "\n",
    "**Objetivo**: Aprender y comparar representaciones de documentos.\n",
    "\n",
    "- **Tarea 5.1**: Entrena Doc2Vec DM y DBOW sobre un corpus de noticias (p. ej. AG News).  \n",
    "  - Usa muestreo negativo y ventana de contexto=5.  \n",
    "  - Extrae los vectores de párrafo y úsalos en una tarea de clasificación temática.  \n",
    "  - Compara macro‑f1 y micro‑f1 de ambos modelos.\n",
    "\n",
    "- **Tarea 5.2**: Usa los mismos vectores para un clustering de documentos (k‑means).  \n",
    "  - Evalúa pureza y NMI (Normalized Mutual Information).  \n",
    "  - Discute cómo DM (que considera orden) y DBOW (solo bag‑of‑words) afectan al agrupamiento.\n",
    "\n",
    "#### 6. Fine‑tuning de representaciones para tareas específicas\n",
    "\n",
    "**Objetivo**: Adaptar embeddings generales a aplicaciones concretas.\n",
    "\n",
    "- **Tarea 6.1**: Tomando un encoder de documentos (p. ej. un Transformer pre‑entrenado o Doc2Vec),  \n",
    "  - Añade una capa de clasificación (softmax) y haz fine‑tuning en un dataset de **resumen extractivo**:  \n",
    "    - Etiquetas binarias por frase (incluir vs. no incluir).  \n",
    "    - Evalúa ROUGE‑1/2 y F1 binaria.  \n",
    "  - Experimenta con distintos valores de tasa de aprendizaje $\\eta$ y regularización $\\lambda$.  \n",
    "\n",
    "- **Tarea 6.2**: Realiza fine‑tuning para clasificación de sentimiento (p. ej. SST‑2).  \n",
    "  - Compara desempeño con embeddings estáticos vs. actualizando pesos del encoder completo.  \n",
    "  - Grafica curvas de validación vs. número de epochs.\n",
    "\n",
    "#### 7. Aprendizaje multitask\n",
    "\n",
    "**Objetivo**: Compartir señales de varias tareas para mejorar la generalización.\n",
    "\n",
    "- **Tarea 7.1**: Diseña un modelo multi‑tarea para **clasificación temática** y **sentimiento** simultáneos:  \n",
    "  - Encoder compartido + dos cabeceras de clasificación.  \n",
    "  - Define pérdidas $L_1, L_2$ y pesos $\\lambda_1, \\lambda_2$.  \n",
    "  - Entrena en un corpus mixto (por ejemplo, noticias con polaridad).  \n",
    "  - Ajusta $\\lambda$ para equilibrar ambas tareas y documenta el impacto.\n",
    "\n",
    "- **Tarea 7.2**: Incorpora una tarea auxiliar de **NER** (reconocimiento de entidades) usando un decoder CRF sobre el mismo encoder.  \n",
    "  - Mide si la señal de NER mejora la clasificación de sentimiento y tema.  \n",
    "  - Analiza la evolución de las pérdidas individuales y compartida.\n",
    "\n",
    "\n",
    "#### 8. Métodos de atención supervisada y adapters\n",
    "\n",
    "**Objetivo**: Explorar mecanismos ligeros de adaptación.\n",
    "\n",
    "- **Tarea 8.1**: Añade sobre BERT un **módulo de atención supervisada** que aprenda pesos $\\alpha_t$ para cada token (ecuaciones de atención vista).  \n",
    "  - Entrena en un dataset de QA extractiva (SQuAD) y mide Exact Match / F1.  \n",
    "  - Visualiza los pesos de atención en ejemplos de prueba.\n",
    "\n",
    "- **Tarea 8.2**: Implementa **Adapters** en un Transformer pre‑entrenado para clasificación de oraciones:  \n",
    "  - Inserta capas de bajo rango entre capas de BERT.  \n",
    "  - Entrena únicamente los adapters y la cabecera final.  \n",
    "  - Compara número de parámetros ajustados vs. fine‑tuning completo y desempeño en SST‑2.\n",
    "\n",
    "\n",
    "Para cada ejercicio:\n",
    "\n",
    "1. Presenta un **informe técnico** con:  \n",
    "   - Descripción del pipeline y arquitectura.  \n",
    "   - Hiperparámetros utilizados.  \n",
    "   - Métricas y gráficas de desempeño.  \n",
    "   - Análisis de ventajas, limitaciones y lecciones aprendidas.\n",
    "\n",
    "2. **Discusión comparativa** que contraste al menos dos métodos vistos.\n",
    "\n",
    "3. **Recomendaciones** sobre elección de modelo según requisitos de latencia, datos y recursos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
