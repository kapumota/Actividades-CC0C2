{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LoRA con PyTorch**\n",
    "\n",
    "LoRA (Low-Rank Adaptation) es una técnica de **afinado de parámetros eficientes** para grandes modelos de lenguaje (LLM) que permite adaptar un modelo preentrenado a una nueva tarea sin necesidad de actualizar o almacenar todos sus billones de parámetros. A continuación se detalla algunas características importantes:\n",
    "\n",
    "\n",
    "- `Eficiencia en las actualizaciones de parámetros:` LoRA introduce solo una pequeña fracción de parámetros adicionales comparado con el número total de parámetros de un modelo grande. Esto hace que el proceso de entrenamiento sea más rápido y menos intensivo en recursos, ya que se necesitan actualizar menos parámetros durante la retropropagación.\n",
    "\n",
    "- `Preservación del conocimiento preentrenado:` Al mantener la mayoría de los pesos del modelo fijos y ajustarlos únicamente mediante matrices de baja dimensión, LoRA ayuda a conservar las ricas representaciones que el modelo aprendió durante el preentrenamiento. Esto es especialmente beneficioso para tareas que no requieren desviaciones drásticas del comportamiento aprendido en el preentrenamiento.\n",
    "\n",
    "- `Personalización para tareas específicas:` A pesar de las actualizaciones mínimas, los cambios introducidos por LoRA son lo suficientemente significativos como para adaptar el modelo a tareas concretas. Esto permite afinar modelos grandes en tareas especializadas sin necesidad de un reentrenamiento extenso.\n",
    "\n",
    "- `Reducción del sobreajuste:` Debido a que solo se adaptan un número limitado de parámetros, el riesgo de sobreajuste es menor en comparación con el ajuste fino completo del modelo, especialmente al adaptar el modelo a conjuntos de datos más pequeños.\n",
    "\n",
    "- `Escalabilidad:` LoRA escala bien con el tamaño del modelo. A medida que los modelos se vuelven más grandes, el incremento relativo en el número de parámetros introducidos por LoRA se hace aún más pequeño, lo que lo convierte en una opción especialmente atractiva para adaptar modelos muy grandes.\n",
    "\n",
    "- `Compatibilidad y simplicidad:` El método puede aplicarse fácilmente a diferentes tipos de redes neuronales, especialmente a aquellas basadas en la arquitectura Transformer. No requiere cambios importantes en la arquitectura existente, lo que simplifica su integración en pipelines ya establecidos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Adaptación de baja dimensión (LoRA)**\n",
    "\n",
    "PyTorch y la librería Hugging Face ofrecen herramientas sólidas para la manipulación de modelos con LoRA, pero no son muy intuitivas. \n",
    "\n",
    "En esta sección, profundizaremos en la construcción de una implementación de LoRA (Low-Rank Adaptation) desde cero usando PyTorch. LoRA es un método general, pero se aplica comúnmente a la capa de atención. Por simplicidad, en este cuaderno lo aplicaremos a una red neuronal. Esta decisión se toma porque acceder a los parámetros de *atención* en el módulo Encoder de PyTorch puede resultar complejo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **LoRA**\n",
    "\n",
    "1. Para cualquier capa arbitraria de una red, tienes el modelo con parámetros preentrenados $W_0$, que son los parámetros del modelo. Si solo consideras los parámetros de atención de cada capa, al menos $4 \\times m \\times n$ por capa. En muchos modelos, esto puede alcanzar billones de parámetros aprendibles. Cada vez que afinamos con un nuevo conjunto de datos, tendrías que almacenar billones de parámetros.\n",
    "\n",
    "2. $\\Delta W$ representa dos matrices $B$ y $A$, donde $B$ y $A$ están constriñidas de manera que $B \\in \\mathbb{R}^{m \\times r}$, $A \\in \\mathbb{R}^{r \\times n}$, y $r \\le \\min(m,n)$. El número total de parámetros en $A$ y $B$ es mucho menor que en $W_1$ y mucho más fácil de almacenar.\n",
    "\n",
    "$$\n",
    "W_1 \\approx W_0 + \\Delta W = W_0 + BA\n",
    "$$\n",
    "\n",
    "3. Para entrenar y predecir, en el pase hacia adelante $W_0$ se mantiene constante:\n",
    "\n",
    "$$\n",
    "h = W_0 x + BAx\n",
    "$$\n",
    "\n",
    "Para escalar $\\Delta W$ por $\\tfrac{\\alpha'}{r}$, donde $\\alpha$ es una constante relacionada con $r$, ajustar $\\alpha'$ es similar a sintonizar la tasa de aprendizaje si la inicialización está correctamente escalada. Por lo tanto, se fija $\\alpha'$ igual al primer $r$ que se prueba y no se sintoniza más; simplemente se usa $\\alpha$. Este escalado reduce la necesidad de retocar hiperparámetros. La forma final es:\n",
    "\n",
    "$$\n",
    "h = W_0 x + \\frac{\\alpha'}{r} BAx = W_0 x + \\alpha\\,BAx\n",
    "$$\n",
    "\n",
    "El siguiente ejemplo ilustra el proceso.\n",
    "\n",
    "$\n",
    "W_0 + BA = \n",
    "\\begin{bmatrix}\n",
    "w_{11} & w_{12} & w_{13} & w_{14} \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "w_{21} & w_{22} & w_{23} & w_{24} \\\\\\\\\\\\\n",
    "w_{31} & w_{32} & w_{33} & w_{34} \\\\\\\\\\\\\n",
    "w_{41} & w_{42} & w_{43} & w_{44} \\\\\\\\\\\\\n",
    "\\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "a_1 \\\\\\\\\\\\\n",
    "a_2 \\\\\\\\\\\\\n",
    "a_3 \\\\\\\\\\\\\n",
    "a_4 \\\\\\\\\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "b_1 & b_2 & b_3 & b_4 \\\\\\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Esto muestra el producto de las matrices $A$ y $B$, denotado $AB$, que se puede sumar a $W_0$. Sin embargo, la matriz resultante $W_0 + AB$ está limitada según las dimensiones de $A$ y $B$, debido al concepto de rango (rank)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Rango**\n",
    "\n",
    "El rango de una matriz es el número de dimensiones en las que \"viven\" sus filas (o columnas). \n",
    "\n",
    "Una matriz cuadrada se dice de **rango completo** (full rank) si su rango es igual al número de filas o columnas. Hagamos esta idea más intuitiva con un ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "import torchtext#; torchtext.disable_torchtext_deprecation_warning()\n",
    "from torchtext.vocab import build_vocab_from_iterator, GloVe, Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import Matrix, init_printing,Symbol\n",
    "\n",
    "from numpy.linalg import qr,eig,inv,matrix_rank,inv, norm\n",
    "from scipy.linalg import null_space\n",
    "from sympy import Matrix, init_printing,Symbol\n",
    "init_printing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_matrix_and_subspace(F):\n",
    "    # Asegura de que F tenga 3 filas para poder visualizar en 3D.\n",
    "    assert F.shape[0] == 3, \"La matriz F debe tener 3 filas para visualización en 3D.\"\n",
    "    \n",
    "    # Crea una figura y un eje 3D\n",
    "    ax = plt.figure().add_subplot(projection='3d')\n",
    "    \n",
    "    # Para cada columna de F, dibujar un vector desde el origen\n",
    "    for i in range(F.shape[1]):\n",
    "        ax.quiver(\n",
    "            0, 0, 0,                   # origen del vector\n",
    "            F[0, i], F[1, i], F[2, i], # componentes del vector\n",
    "            color='blue',\n",
    "            arrow_length_ratio=0.1,\n",
    "            label=f'Columna {i+1}'     # etiqueta para la leyenda\n",
    "        )\n",
    "\n",
    "    # Si F tiene exactamente 2 columnas, dibujar el plano que ellas generan\n",
    "    if F.shape[1] == 2:\n",
    "        # Calcula el vector normal al plano generado por las dos columnas\n",
    "        normal_vector = np.cross(F[:, 0], F[:, 1])\n",
    "        \n",
    "        # Crea una malla de puntos en XY para dibujar la superficie del plano\n",
    "        xx, yy = np.meshgrid(\n",
    "            np.linspace(-3, 3, 10),\n",
    "            np.linspace(-3, 3, 10)\n",
    "        )\n",
    "        # Ecuación del plano: normal · [x, y, z] = 0  =>  z = ...\n",
    "        if normal_vector[2] != 0:\n",
    "            zz = (-normal_vector[0] * xx - normal_vector[1] * yy) / normal_vector[2]\n",
    "        else:\n",
    "            zz = np.zeros_like(xx)\n",
    "        \n",
    "        # Dibuja la superficie del plano con transparencia\n",
    "        ax.plot_surface(xx, yy, zz, alpha=0.5, color='green', label='Plano generado')\n",
    "\n",
    "    # Configura límites de los ejes\n",
    "    ax.set_xlim([-3, 3])\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax.set_zlim([-3, 3])\n",
    "    \n",
    "    # Etiquetas de los ejes\n",
    "    ax.set_xlabel('$x_{1}$')\n",
    "    ax.set_ylabel('$x_{2}$')\n",
    "    ax.set_zlabel('$x_{3}$')\n",
    "    \n",
    "    # Muestra la figura\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_matrix_and_subspace(F):\n",
    "    # Asegura de que F tenga 3 filas para poder visualizar en 3D.\n",
    "    assert F.shape[0] == 3, \"La matriz F debe tener 3 filas para visualización en 3D.\"\n",
    "    \n",
    "    # Crea una figura y un eje 3D\n",
    "    ax = plt.figure().add_subplot(projection='3d')\n",
    "    \n",
    "    # Para cada columna de F, dibujar un vector desde el origen\n",
    "    for i in range(F.shape[1]):\n",
    "        ax.quiver(\n",
    "            0, 0, 0,                   # origen del vector\n",
    "            F[0, i], F[1, i], F[2, i], # componentes del vector\n",
    "            color='blue',\n",
    "            arrow_length_ratio=0.1,\n",
    "            label=f'Columna {i+1}'     # etiqueta para la leyenda\n",
    "        )\n",
    "\n",
    "    # Si F tiene exactamente 2 columnas, dibujar el plano que ellas generan\n",
    "    if F.shape[1] == 2:\n",
    "        # Calcula el vector normal al plano generado por las dos columnas\n",
    "        normal_vector = np.cross(F[:, 0], F[:, 1])\n",
    "        \n",
    "        # Crea una malla de puntos en XY para dibujar la superficie del plano\n",
    "        xx, yy = np.meshgrid(\n",
    "            np.linspace(-3, 3, 10),\n",
    "            np.linspace(-3, 3, 10)\n",
    "        )\n",
    "        # Ecuación del plano: normal · [x, y, z] = 0  =>  z = ...\n",
    "        if normal_vector[2] != 0:\n",
    "            zz = (-normal_vector[0] * xx - normal_vector[1] * yy) / normal_vector[2]\n",
    "        else:\n",
    "            zz = np.zeros_like(xx)\n",
    "        \n",
    "        # Dibuja la superficie del plano con transparencia\n",
    "        ax.plot_surface(xx, yy, zz, alpha=0.5, color='green', label='Plano generado')\n",
    "\n",
    "    # Configura límites de los ejes\n",
    "    ax.set_xlim([-3, 3])\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax.set_zlim([-3, 3])\n",
    "    \n",
    "    # Etiquetas de los ejes\n",
    "    ax.set_xlabel('$x_{1}$')\n",
    "    ax.set_ylabel('$x_{2}$')\n",
    "    ax.set_zlabel('$x_{3}$')\n",
    "    \n",
    "    # Muestra la figura\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "En el contexto de Low-Rank Adaptation  (LoRA), donde $B \\in \\mathbb{R}^{d \\times r}$, la matriz $B$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "B=torch.tensor([[1,0],[0,1],[0,0]]).numpy()\n",
    "\n",
    "Matrix(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta matriz de $3 \\times 2$ tiene columnas que generan un subespacio bidimensional en $\\mathbb{R}^3$. Específicamente, las columnas de $B$ son:\n",
    "\n",
    "- $\\mathbf{b}_1 = \\begin{bmatrix} 1 \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ 0 \\\\ 0 \\end{bmatrix}$\n",
    "- $\\mathbf{b}_2 = \\begin{bmatrix} 0 \\\\\\\\ 1 \\\\ 0 \\end{bmatrix}$\n",
    "\n",
    "Estos vectores columna son los vectores de la base estándar para el plano $xy$ en $\\mathbb{R}^3$ y, por lo tanto, generan el plano $xy$ mostrado en verde en la imagen siguiente. Multiplicar cualquiera de estos vectores columna (en azul) por un escalar siempre resulta en un punto que queda en ese mismo plano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " plot_matrix_and_subspace(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este escenario, los vectores, a pesar de tener cada uno tres componentes, pueden alcanzar cualquier punto del plano verde bidimensional representado en la imagen. Estos vectores abarcan el plano verde, que reside dentro de un subespacio bidimensional. \n",
    "\n",
    "La dimensión de este subespacio, también conocida como su \"rango\", es dos, lo que corresponde a la dimensionalidad del plano. Si el rango fuera tres, cualquier punto en el espacio 3D podría alcanzarse mediante alguna combinación de las columnas de \\$𝐵\\$. El rango de una matriz se puede determinar usando la función `matrix_rank` proporcionada por NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_rank(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí, trazas una matriz diferente donde la matriz abarca un plano distinto, pero el rango sigue siendo dos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_=torch.tensor([[1,0],[-2,1],[0,1]]).numpy()\n",
    "plot_matrix_and_subspace(B_)\n",
    "print(\"rank de B\",matrix_rank(B_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_rank(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí presentamos la matriz `A`. El rango de esta matriz también es dos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=torch.tensor([[1,1,-1,1,0],[-2,2,2,0,1]]).numpy()\n",
    "Matrix(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_rank(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Para las matrices $C = BA$, si $B$ y $A$ tienen ambas rango $r$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C=B@A\n",
    "Matrix(C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las columnas de $C$ tendrán el mismo rango que $B$. Además, el espacio generado por las columnas de $C$ será el mismo que el generado por las columnas de $B$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"rank de C\",matrix_rank(C))\n",
    "plot_matrix_and_subspace(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Entendiendo LoRA in PyTorch**\n",
    "\n",
    "LoRA (Low-Rank Adaptation) es relativamente sencillo de inicializar en PyTorch. Se inicializa LoRA con las dimensiones de la entrada (`in_dim`), $m$, la salida (`out_dim`), $n$, un rango (`rank`), $r$, y un factor de escala `alpha`. Los parámetros se inicializan de la siguiente forma:\n",
    "\n",
    "```\n",
    "self.A = torch.nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
    "self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "```\n",
    "\n",
    "El uso de `nn.Parameter` hace que estos valores sean parámetros aprendibles.\n",
    "\n",
    "En la función forward, LoRA usa la notación $BAx$. En PyTorch, el vector de entrada es una fila, por lo que la salida se convierte en $x^TA^TB^T$; a partir de ahora omitiremos la transposición. El paso forward se implementa como:\n",
    "\n",
    "```\n",
    "x = self.alpha * (x @ self.A @ self.B)\n",
    "```\n",
    "\n",
    "El uso de `nn.Parameter` hace que estos valores sean parámetros aprendibles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        self.A = torch.nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
    "        \n",
    "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.alpha * (x @ self.A @ self.B)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta clase `LinearWithLoRA` copia el modelo lineal original y crea un objeto `LoRALayer`.\n",
    "\n",
    "```\n",
    "self.linear = linear.to(device)\n",
    "self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        ).to(device)\n",
    "```\n",
    "\n",
    "Luego, en el método forward aplica tanto el modelo lineal original como el modelo LoRA a la entrada `x` y los suma: `self.linear(x) + self.lora(x)`. Esto corresponde a:\n",
    "\n",
    "$x W_0 + x A B$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearWithLoRA(torch.nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear.to(device)\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.linear(x) + self.lora(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Configuración adicionales**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importar las librerías requeridas\n",
    "\n",
    "El siguiente bloque importa todas las librerías necesarias:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import accumulate\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "torch.set_num_threads(1)\n",
    "from torch import nn\n",
    "import os\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator, GloVe, Vectors\n",
    "from torchtext.datasets import IMDB\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "import pickle\n",
    "\n",
    "from urllib.request import urlopen\n",
    "import io\n",
    "\n",
    "import tarfile\n",
    "import tempfile\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# También puedes usar esta sección para suprimir las advertencias generadas por tu código:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Funciones auxiliares**\n",
    "\n",
    "El siguiente código muestra algunas funciones de ayuda para graficar, guardar y cargar archivos. Estas funciones no son el foco principal del cuaderno, pero debes ejecutar estas celdas para poder usarlas más adelante.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(COST,ACC):\n",
    "    \"\"\"\n",
    "    Grafica la evolución de la pérdida total y la precisión por época.\n",
    "\n",
    "    Parámetros:\n",
    "        COST (list): Lista con el valor de la pérdida total en cada época.\n",
    "        ACC (list): Lista con la precisión en cada época.\n",
    "    \"\"\"\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    color = 'tab:red'\n",
    "    ax1.plot(COST, color=color)\n",
    "    ax1.set_xlabel('epoca', color=color)\n",
    "    ax1.set_ylabel('Perdida total', color=color)\n",
    "    ax1.tick_params(axis='y', color=color)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('accuracy', color=color)  \n",
    "    ax2.plot(ACC, color=color)\n",
    "    ax2.tick_params(axis='y', color=color)\n",
    "    fig.tight_layout() \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list_to_file(lst, filename):\n",
    "    \"\"\"\n",
    "    Guarda una lista en un archivo usando serialización con pickle.\n",
    "\n",
    "    Parámetros:\n",
    "        lst (list): La lista que se guardará.\n",
    "        filename (str): El nombre del archivo donde se guardará la lista.\n",
    "\n",
    "    Retorna:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(lst, file)\n",
    "\n",
    "def load_list_from_file(filename):\n",
    "    \"\"\"\n",
    "    Carga una lista desde un archivo usando deserialización con pickle.\n",
    "\n",
    "    Parámetros:\n",
    "        filename (str): El nombre del archivo desde el cual se cargará la lista.\n",
    "\n",
    "    Retorna:\n",
    "        list: La lista cargada.\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as file:\n",
    "        loaded_list = pickle.load(file)\n",
    "    return loaded_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Pipeline de datos**\n",
    "\n",
    "**Tokenizador**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for  _,text in data_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nota: los embeddings de GloVe normalmente se descargan usando:\n",
    "# glove_embedding = GloVe(name=\"6B\", dim=100)\n",
    "# Sin embargo, el servidor de GloVe frecuentemente está inactivo.\n",
    "# El código siguiente ofrece una solución alternativa.\n",
    "\n",
    "class GloVe_override(Vectors):\n",
    "    url = {\n",
    "        \"6B\": \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/tQdezXocAJMBMPfUJx_iUg/glove-6B.zip\",\n",
    "    }\n",
    "\n",
    "    def __init__(self, name=\"6B\", dim=100, **kwargs) -> None:\n",
    "        url = self.url[name]\n",
    "        name = \"glove.{}.{}d.txt\".format(name, str(dim))\n",
    "        #name = \"glove.{}/glove.{}.{}d.txt\".format(name, name, str(dim))\n",
    "        super(GloVe_override, self).__init__(name, url=url, **kwargs)\n",
    "\n",
    "class GloVe_override2(Vectors):\n",
    "    url = {\n",
    "        \"6B\": \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/tQdezXocAJMBMPfUJx_iUg/glove-6B.zip\",\n",
    "    }\n",
    "\n",
    "    def __init__(self, name=\"6B\", dim=100, **kwargs) -> None:\n",
    "        url = self.url[name]\n",
    "        #name = \"glove.{}.{}d.txt\".format(name, str(dim))\n",
    "        name = \"glove.{}/glove.{}.{}d.txt\".format(name, name, str(dim))\n",
    "        super(GloVe_override2, self).__init__(name, url=url, **kwargs)\n",
    "\n",
    "try:\n",
    "    glove_embedding = GloVe_override(name=\"6B\", dim=100)\n",
    "except:\n",
    "    try:\n",
    "        glove_embedding = GloVe_override2(name=\"6B\", dim=100)\n",
    "    except:\n",
    "        glove_embedding = GloVe(name=\"6B\", dim=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Importamos el conjunto de datos IMDB**\n",
    "\n",
    "El siguiente código carga y manejo del conjunto de datos IMDB. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/35t-FeC-2uN1ozOwPs7wFg.gz')\n",
    "tar = tarfile.open(fileobj=io.BytesIO(urlopened.read()))\n",
    "tempdir = tempfile.TemporaryDirectory()\n",
    "tar.extractall(tempdir.name)\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, root_dir, train=True):\n",
    "        \"\"\"\n",
    "        root_dir: El directorio base del conjunto de datos IMDB.\n",
    "        train: Indicado booleano que indica si se debe usar datos de entrenamiento o de prueba.\n",
    "        \"\"\"\n",
    "        self.root_dir = os.path.join(root_dir, \"train\" if train else \"test\")\n",
    "        self.neg_files = [os.path.join(self.root_dir, \"neg\", f) for f in os.listdir(os.path.join(self.root_dir, \"neg\")) if f.endswith('.txt')]\n",
    "        self.pos_files = [os.path.join(self.root_dir, \"pos\", f) for f in os.listdir(os.path.join(self.root_dir, \"pos\")) if f.endswith('.txt')]\n",
    "        self.files = self.neg_files + self.pos_files\n",
    "        self.labels = [0] * len(self.neg_files) + [1] * len(self.pos_files)\n",
    "        self.pos_inx=len(self.pos_files)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.files[idx]\n",
    "        label = self.labels[idx]\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        return label, content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código utiliza la clase `IMDBDataset` definida anteriormente para crear iteradores de los conjuntos de datos de entrenamiento y prueba. Luego, muestra 20 ejemplos del conjunto de entrenamiento:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = tempdir.name + '/' + 'imdb_dataset'\n",
    "train_iter = IMDBDataset(root_dir=root_dir, train=True)  # Para datos de entrenamiento\n",
    "test_iter = IMDBDataset(root_dir=root_dir, train=False)  # Para datos de prueba\n",
    "\n",
    "start=train_iter.pos_inx\n",
    "for i in range(-10,10):\n",
    "    print(train_iter[start+i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente fragmento define el mapeo de etiquetas numéricas a reseñas negativas y positivas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_label = {0: \" negative review\", 1: \"positive review\"}\n",
    "imdb_label[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, este código verifica que en el conjunto de entrenamiento existan exactamente dos clases:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "num_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código carga un tokenizador básico en inglés y define una función llamada `yield_tokens` que utiliza el tokenizador para descomponer datos de texto proporcionados por un iterador en tokens:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    \"\"\"Devuelve tokens para cada muestra de datos.\"\"\"\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código carga un modelo de embeddings de palabras preentrenado llamado GloVe en una variable llamada `glove_embedding`:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código construye un objeto vocabulario a partir de un modelo de incrustaciones de palabras GloVe preentrenado y establece el índice predeterminado en el token `<unk>`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe,vocab\n",
    "# Construye el vocabulario a partir de glove_embedding.stoi\n",
    "vocab = vocab(glove_embedding .stoi, 0,specials=('<unk>', '<pad>'))\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contemos el número de palabras en el vocabulario:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probemos la función `vocab`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab([\"age\",\"hello\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **División del conjunto de datos**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convierte los iteradores de entrenamiento y prueba en conjuntos de datos de estilo map.\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "# Determina el número de muestras para entrenamiento y validación (5 % para validación).\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "\n",
    "# Divide aleatoriamente el conjunto de entrenamiento en entrenamiento y validación.\n",
    "# El conjunto de entrenamiento tendrá el 95 % de las muestras y el de validación el 5 % restante.\n",
    "split_train_, split_valid_ = random_split(\n",
    "    train_dataset,\n",
    "    [num_train, len(train_dataset) - num_train]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Para simular el proceso como si se tuviera GPU, reducimos aún más el tamaño del conjunto de entrenamiento. Si deseas usar el conjunto IMDB completo, comenta o elimina las dos líneas siguientes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = int(len(train_dataset) * 0.05)\n",
    "split_train_, _ = random_split(split_train_, [num_train, len(split_train_) - num_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código verifica si hay una GPU compatible con CUDA disponible usando PyTorch. Si existe, asigna `device = \"cuda\"`, de lo contrario `device = \"cpu\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cargador de datos**\n",
    "\n",
    "El siguiente código prepara el pipeline de procesamiento de texto con el tokenizador y el vocabulario. \n",
    "\n",
    "La función `text_pipeline` primero tokeniza el texto de entrada y luego aplica `vocab` para obtener los índices de los tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_pipeline(x):\n",
    "    return vocab(tokenizer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En PyTorch, la función **`collate_fn`** se utiliza junto con los data loaders para personalizar la forma en que se crean los lotes a partir de muestras individuales. El código proporcionado define una función `collate_batch` en PyTorch, que se emplea con los data loaders para ajustar la creación de lotes a partir de muestras individuales. Esta función procesa un lote de datos, incluyendo etiquetas y secuencias de texto. \n",
    "\n",
    "Aplica la función `text_pipeline` para preprocesar el texto. Los datos resultantes se convierten en tensores de PyTorch y se devuelven como una tupla que contiene el tensor de etiquetas, el tensor de texto y un tensor de offsets que representa las posiciones iniciales de cada secuencia de texto dentro del tensor combinado.\n",
    "\n",
    "Además, la función se asegura de que los tensores generados se muevan al dispositivo especificado (por ejemplo, GPU) para un cálculo más eficiente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for _label, _text in batch:\n",
    "\n",
    "        label_list.append(_label)\n",
    "        text_list.append(torch.tensor(text_pipeline(_text), dtype=torch.int64))\n",
    "\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = pad_sequence(text_list, batch_first=True)\n",
    "\n",
    "    return label_list.to(device), text_list.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes convertir estos conjuntos de datos en data loaders aplicando `collate_fn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Comprobemos qué generan estos data loaders:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label,seqence=next(iter(valid_dataloader))\n",
    "label,seqence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Red neuronal**\n",
    "\n",
    "Este código define una clase llamada `TextClassifier` que representa un clasificador de texto sencillo que utiliza:\n",
    "\n",
    "* una capa de embedding,\n",
    "* una capa lineal oculta con activación ReLU,\n",
    "* y una capa lineal de salida.\n",
    "\n",
    "El constructor recibe los siguientes argumentos:\n",
    "\n",
    "* `num_class`: el número de clases a clasificar.\n",
    "* `freeze`: indica si se debe congelar la capa de embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, freeze=False):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        # Capa de embedding precargada con vectores GloVe; opcionalmente congelada\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            glove_embedding.vectors.to(device),\n",
    "            freeze=freeze\n",
    "        )\n",
    "        # Ejemplo de capa adicional: capa lineal intermedia\n",
    "        self.fc1 = nn.Linear(in_features=100, out_features=128)\n",
    "        # Activación ReLU tras la capa lineal\n",
    "        self.relu = nn.ReLU()\n",
    "        # Capa de salida que produce las puntuaciones (logits) para cada clase\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pasa la secuencia de índices por la capa de embedding\n",
    "        x = self.embedding(x)\n",
    "        # Aplica un pooling por media sobre la dimensión de secuencia\n",
    "        x = torch.mean(x, dim=1)\n",
    "        # Pasa los embeddings agrupados por la capa lineal intermedia…\n",
    "        x = self.fc1(x)\n",
    "        # …y aplica la activación ReLU\n",
    "        x = self.relu(x)\n",
    "        # Devuelve los logits finales para cada clase\n",
    "        return self.fc2(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Entrenamiento del modelo sobre todo el conjunto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo=TextClassifier(num_classes=2,freeze=True)\n",
    "modelo.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.eval()\n",
    "predicted_label=modelo(seqence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicted_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente función **`predict`** toma como entradas un texto, una secuencia de texto y un modelo. Utiliza un modelo preentrenado que se pasa como parámetro para predecir la etiqueta del texto para su clasificación en el conjunto de datos IMDB:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, text_pipeline, modelo):\n",
    "    with torch.no_grad():\n",
    "        text = torch.unsqueeze(torch.tensor(text_pipeline(text)),0).to(device)\n",
    "        modelo.to(device)\n",
    "        output = modelo(text)\n",
    "        return imdb_label[output.argmax(1).item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"I like sports and stuff\", text_pipeline, modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Podemos crear una función para evaluar la exactitud del modelo en un conjunto de datos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, modelo, device):\n",
    "    modelo.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for label, text in dataloader:\n",
    "            label, text = label.to(device), text.to(device)\n",
    "            outputs = modelo(text)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código evalúa el rendimiento del modelo (puede tardar unos 4 minutos en CPU). **Para mayor eficiencia, no ejecutaremos esta celda ahora**, pero puede descomentarse si se desea comprobar que el modelo sin entrenar no rinde mejor que el azar:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate(test_dataloader, modelo, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten en cuenta que el rendimiento actual del modelo no es mejor que el promedio. Este resultado es esperado, considerando que el modelo aún no ha recibido ningún entrenamiento.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Entrenamiento**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código define la función de entrenamiento que se utiliza para entrenar tu modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(modelo, optimizer, criterion, train_dataloader, valid_dataloader, epochs=100, model_name=\"mi_modeldrop\"):\n",
    "    cum_loss_list = []\n",
    "    acc_epoch = []\n",
    "    best_acc = 0\n",
    "    file_name = model_name\n",
    "    \n",
    "    for epoch in tqdm(range(1, epochs + 1)):\n",
    "        modelo.train()\n",
    "        cum_loss = 0\n",
    "        for _, (label, text) in enumerate(train_dataloader):            \n",
    "            optimizer.zero_grad()\n",
    "            predicted_label = modelo(text)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(modelo.parameters(), 0.1)\n",
    "            optimizer.step()\n",
    "            cum_loss += loss.item()\n",
    "        #print(\"Funcion de perdida:\", cum_loss)\n",
    "        cum_loss_list.append(cum_loss)\n",
    "        acc_val = evaluate(valid_dataloader, modelo, device)\n",
    "        acc_epoch.append(acc_val)\n",
    "        \n",
    "        if acc_val > best_acc:\n",
    "            best_acc = acc_val\n",
    "            print(f\"Nueva mejor exactituf: {acc_val:.4f}\")\n",
    "            #torch.save(modelo.state_dict(), f\"{model_name}.pth\")\n",
    "    \n",
    "    #save_list_to_file(cum_loss_list, f\"{model_name}_loss.pkl\")\n",
    "    #save_list_to_file(acc_epoch, f\"{model_name}_acc.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "El siguiente código establece la tasa de aprendizaje (LR) en 1, que determina el tamaño del paso con el que el optimizador actualiza los parámetros del modelo durante el entrenamiento. El criterio `CrossEntropyLoss` se utiliza para calcular la pérdida entre las salidas predichas por el modelo y las etiquetas reales. Esta función de pérdida se emplea comúnmente en tareas de clasificación multiclase.\n",
    "\n",
    "El optimizador elegido es Stochastic Gradient Descent (SGD), que ajusta los parámetros del modelo en función de los gradientes calculados respecto a la función de pérdida. El optimizador SGD utiliza la tasa de aprendizaje especificada para controlar el tamaño de las actualizaciones de los pesos.\n",
    "\n",
    "Además, se define un programador de tasa de aprendizaje mediante `StepLR`. Este programador ajusta la tasa de aprendizaje durante el entrenamiento, reduciéndola en un factor (`gamma`) de 0.1 después de cada época (paso) para mejorar la convergencia y afinar el rendimiento del modelo. Estos componentes forman, en conjunto, la configuración esencial para entrenar una red neuronal usando la tasa de aprendizaje, el criterio de pérdida, el optimizador y el programador de tasa de aprendizaje especificados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR=1\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(modelo.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se a preentrenado el modelo para 300 épocas con una GPU y se ha guardado para tu comodidad. Sin embargo, para ilustrar el proceso de entrenamiento, se ha incluido el siguiente código que entrena el modelo solo para dos épocas. Ten en cuenta que ha limitado el número de épocas a dos, ya que el entrenamiento en una CPU puede llevar mucho tiempo. Incluso con solo dos épocas, el siguiente código puede tardar aproximadamente un minuto en ejecutarse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"model_imdb_freeze_true2\"\n",
    "train_model(modelo, optimizer, criterion, train_dataloader, valid_dataloader, epochs=2, model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lugar de evaluar el modelo entrenado en 2 épocas, carguemos el modelo preentrenado que fue entrenado durante 300 épocas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/ZvhVWJU0flC7BmU1jjYxjg/model-imdb-freeze-true2.pth\n",
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/2RdN-JG4Rm5Gx3UNtOP4NA/model-imdb-freeze-true2-acc.pkl\n",
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/8qoGvWk0BdXRGoFAOT-dAw/model-imdb-freeze-true2-loss.pkl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grafiquemos el costo y la exactitud de cada época para el modelo preentrenado, que se entrenó para 300 épocas. El gráfico muestra que, con solo unas pocas épocas, la exactitud presenta una volatilidad significativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_loss_list=load_list_from_file(model_name.replace('_','-') + \"-loss.pkl\")\n",
    "acc_epoch=load_list_from_file(model_name.replace('_','-') + \"-acc.pkl\")\n",
    "plot(cum_loss_list,acc_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el modelo que se ha entrenado. Si deseas entrenar el modelo tu mismo comente estas líneas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.load_state_dict(torch.load(model_name.replace('_','-') + \".pth\", map_location=device))\n",
    "modelo.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se evalúa el modelo con los datos de prueba. El modelo preentrenado alcanza una exactitud del 66 %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(test_dataloader , modelo, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Aplicando LoRA**\n",
    "\n",
    "Para afinar con LoRA, primero carga un modelo **TextClassifier** preentrenado con LoRA (mientras congelas sus capas), carga su estado preentrenado desde un archivo y luego deshabilita las actualizaciones de gradiente para todos sus parámetros para evitar entrenamiento adicional.\n",
    "\n",
    "Aquí, cargare,ps un modelo que fue preentrenado en el conjunto de datos AG NEWS, el cual tiene 4 clases. \n",
    "\n",
    "Observa que cuando inicializas este modelo, estableces `num_classes` en 4. Además, el modelo AG\\_News preentrenado se entrenó con la capa de embedding sin congelar. Por lo tanto, inicializarás el modelo con `freeze=False`. Aunque estés inicializando el modelo con capas sin congelar y con un número incorrecto de clases para tu tarea, harás modificaciones más adelante que corregirán esto:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import io\n",
    "\n",
    "model_lora=TextClassifier(num_classes=4,freeze=False)\n",
    "model_lora.to(device)\n",
    "\n",
    "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/uGC04Pom651hQs1XrZ0NsQ/my-model-freeze-false.pth')\n",
    "\n",
    "stream = io.BytesIO(urlopened.read())\n",
    "state_dict = torch.load(stream, map_location=device)\n",
    "model_lora.load_state_dict(state_dict)\n",
    "\n",
    "# Aquí, congelas todas las capas:\n",
    "for parm in model_lora.parameters():\n",
    "    parm.requires_grad=False\n",
    "model_lora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten en cuenta que el bucle `for` en el código anterior congeló todas las capas de la red neuronal, incluyendo la capa de embedding.\n",
    "\n",
    "Además, observa que el modelo original resolvía un problema de clasificación con cuatro clases, mientras que el conjunto de datos IMDB solo tiene 2 clases. Para tener esto en cuenta, reemplazamos la capa final por una nueva capa lineal donde el número de salidas sea igual a 2:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lora.fc2=nn.Linear(in_features=128, out_features=2, bias=True).to(device)\n",
    "model_lora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos todos los módulos en el objeto:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tu tarea ahora es reemplazar la capa oculta por una capa LoRA. Puedes acceder a la capa oculta de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lora.fc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo siguiente reemplaza esta capa por una capa LoRA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lora.fc1=LinearWithLoRA(model_lora.fc1,rank=2, alpha=0.1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos nuevamente la capa oculta para asegurarnos de que efectivamente se haya convertido en una capa LoRA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lora.fc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto, entrenar el modelo es similar, con la única diferencia de que, salvo la capa de salida, solo los parámetros aprendibles\n",
    "`A` y `B` se actualizarán. El código para seleccionar los valores de `r` y `alpha`, que no se ejecuta, se proporciona aquí para tu conveniencia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lora.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details> \n",
    "<summary><b>Haz clic aquí para ver el código para seleccionar r y alpha</b></summary>\n",
    "\n",
    "```python \n",
    "ranks = [1, 2, 5, 10]\n",
    "alphas = [0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "\n",
    "results = []\n",
    "accuracy_old = 0\n",
    "# Itera sobre cada combinación de 'r' y 'alpha'\n",
    "for r in ranks:\n",
    "    for alpha in alphas:\n",
    "        print(f\"Pruebas con rank = {r} and alpha = {alpha}\")\n",
    "        model_name = f\"model_lora_rank{r}_alpha{alpha}_AGtoIBDM_final_adam_\"\n",
    "        \n",
    "        model_lora = TextClassifier(num_classes=4, freeze=False)\n",
    "        model_lora.to(device)\n",
    "        \n",
    "        urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/uGC04Pom651hQs1XrZ0NsQ/my-model-freeze-false.pth')\n",
    "        \n",
    "        stream = io.BytesIO(urlopened.read())\n",
    "        state_dict = torch.load(stream, map_location=device)\n",
    "        model_lora.load_state_dict(state_dict)\n",
    "        \n",
    "        for parm in model_lora.parameters():\n",
    "            parm.requires_grad = False\n",
    "        \n",
    "        model_lora.fc2 = nn.Linear(in_features=128, out_features=2, bias=True)\n",
    "        model_lora.fc1 = LinearWithLoRA(model_lora.fc1, rank=r, alpha=alpha)\n",
    "        optimizer = torch.optim.Adam(model_lora.parameters(), lr=LR)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n",
    "        \n",
    "        model_lora.to(device)\n",
    "        \n",
    "        train_model(model_lora, optimizer, criterion, train_dataloader, valid_dataloader, epochs=300, model_name=model_name)\n",
    "        \n",
    "        accuracy = evaluate(valid_dataloader, model_lora, device)\n",
    "        result = {\n",
    "            'rank': r,\n",
    "            'alpha': alpha,\n",
    "            'accuracy': accuracy\n",
    "        }\n",
    "\n",
    "        # Agrega el diccionario a la lista de resultados\n",
    "        results.append(result)\n",
    "        \n",
    "        if accuracy > accuracy_old:\n",
    "            print(f\"Pruebas con rank = {r} and alpha = {alpha}\")\n",
    "            print(f\"Exactitud: {accuracy} accuracy_old: {accuracy_old}\")\n",
    "            accuracy_old = accuracy\n",
    "            torch.save(model.state_dict(), f\"{model_name}.pth\")\n",
    "            save_list_to_file(cum_loss_list, f\"{model_name}_loss.pkl\")\n",
    "            save_list_to_file(acc_epoch, f\"{model_name}_acc.pkl\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, configuremos los componentes de entrenamiento para el modelo `model_lora`, definiendo una tasa de aprendizaje de 1, usando la pérdida de entropía cruzada como criterio, optimizando con descenso de gradiente estocástico (SGD) y programando la tasa de aprendizaje para que decaiga en un factor de 0.1 en cada época:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR=1\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_lora.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ha preentrenado un modelo usando un procedimiento idéntico durante 300 épocas y lo guardaste para tu conveniencia. Sin embargo, para que veas cómo funciona el entrenamiento en la práctica, ejecuta el siguiente código para entrenar el modelo solo durante 2 épocas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"model_lora_final2\"\n",
    "train_model(model_lora,optimizer, criterion, train_dataloader, valid_dataloader, epochs=2, model_name=model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lugar de evaluar el modelo que acabas de entrenar por 2 épocas, veamos el modelo LoRA preentrenado con 300 épocas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/JWPRb1RMhKLRMUWOKw9pxA/model-lora-final2.pth\n",
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/_dm02rLyTrwsXEQh2r32sQ/model-lora-final2-acc.pkl\n",
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/OZbVqKjoqOSIwnET8AB1KA/model-lora-final2-loss.pkl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo siguiente muestra la progresión del entrenamiento de este modelo durante 300 épocas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_loss_list=load_list_from_file(model_name.replace('_','-') + \"-loss.pkl\")\n",
    "acc_epoch=load_list_from_file(model_name.replace('_','-') + \"-acc.pkl\")\n",
    "plot(cum_loss_list,acc_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, carguemos realmente el modelo en `model_lora`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lora.load_state_dict(torch.load(model_name.replace('_','-') + \".pth\", map_location=device))\n",
    "model_lora.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y evaluemos su rendimiento en los datos de prueba:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtienes una mejora del 3% sobre un modelo entrenado desde cero al usar LoRA. Ten en cuenta que esto ocurre a pesar de que el modelo afinado con LoRA actualizó menos parámetros que el modelo entrenado desde cero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El atributo `model_lora.fc1` representa `LinearWithLoRA`, que contiene tanto la capa estándar `Linear` (`linear`) como una capa adicional `LoRA` (`lora`), la cual representa `LoRALayer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lora.fc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desde `model_lora.fc1.lora` puedes obtener los parámetros aprendibles **A** y **B**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B=model_lora.fc1.lora.B\n",
    "print(\"B\",B)\n",
    "print(\"\\n Numero de elementos en el tensor B\",B.numel())\n",
    "torch.save(B, 'B.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=model_lora.fc1.lora.A\n",
    "print(\"A\",A)\n",
    "print(\"\\n Numero de elementos en el tensor A\",A.numel())\n",
    "torch.save(A, 'A.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A y B tienen aproximadamente 450 parámetros. Si guardaras la capa lineal completa, tendrías 12,800 parámetros, lo que es alrededor de 28 veces más. \n",
    "\n",
    "Recuerda, este es posiblemente el modelo más simple que puedas tener."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Numero de elementos en el tensor A\",model_lora.fc1.linear.weight.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alfa y la capa de salida también se guardan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alfa_=model_lora.fc1.lora.alpha\n",
    "torch.save(alfa_, 'alfa_.pth')\n",
    "torch.save(model_lora.fc2.state_dict(), 'out_layer.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cargando el modelo**\n",
    "\n",
    "La principal ventaja de LoRA es que para el fine-tuning solo necesitas guardar los parámetros entrenables A y B, el factor α, y la capa de salida en tu ejemplo de clasificación.\n",
    "\n",
    "Los archivos guardados se convierten en tensores y la capa lineal, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.load('A.pth')\n",
    "print(\"A:\",A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = torch.load('B.pth')\n",
    "print(\"B:\",B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alfa_ = torch.load('alfa_.pth')\n",
    "alfa_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La capa de salida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer=nn.Linear(in_features=128, out_features=2, bias=True)\n",
    "output_layer.load_state_dict(torch.load('out_layer.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea el objeto del modelo y se cargan los parámetros preentrenados:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_load_lora = TextClassifier(num_classes=4,freeze=False)\n",
    "model_load_lora.to(device)\n",
    "\n",
    "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/uGC04Pom651hQs1XrZ0NsQ/my-model-freeze-false.pth')\n",
    "\n",
    "stream = io.BytesIO(urlopened.read())\n",
    "state_dict = torch.load(stream, map_location=device)\n",
    "model_load_lora.load_state_dict(state_dict)\n",
    "\n",
    "model_load_lora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se añade la capa LoRA a la capa oculta original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_load_lora.fc1=LinearWithLoRA(model_load_lora.fc1,rank=2, alpha=0.1)\n",
    "model_load_lora.fc2=nn.Linear(in_features=128, out_features=2, bias=True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se incorporan los parámetros obtenidos del fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_load_lora.fc1.lora.A=A\n",
    "model_load_lora.fc1.lora.B=B\n",
    "model_load_lora.fc1.lora.alpha=alfa_ \n",
    "model_load_lora.fc2=output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_load_lora.to(device)\n",
    "model_load_lora.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(test_dataloader , model_load_lora, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto confirma que el modelo se cargó correctamente. Aún obtienes una mejora del 3 % en exactitud!."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se muestra cómo hacer una predicción sobre el siguiente artículo utilizando la función **`predict`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article=\"\"\"This was a lacklustre movie with very little going for it. I was not impressed.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este contenido en Markdown genera un recuadro con fondo gris claro y padding. Contiene un encabezado `<h3>` que muestra el contenido de la variable `article` y un `<h4>` que indica la categoría predicha del artículo, proporcionada por la variable `result`. Los marcadores `{article}` y `{result}` se reemplazarán dinámicamente con los valores reales al renderizarse el Markdown.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict(article, model_load_lora, text_pipeline)\n",
    "\n",
    "markdown_content = f'''\n",
    "<div style=\"background-color: lightgray; padding: 10px;\">\n",
    "    <h3>{article}</h3>\n",
    "    <h4>The category of the news article: {result}</h4>\n",
    "</div>\n",
    "'''\n",
    "\n",
    "md(markdown_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Ejercicio: Aplicar LoRA a una red distinta para reconocimiento de letras**\n",
    "\n",
    "Partiendo de la clase `NNet` (diseñada originalmente para dígitos 0–9), tu tarea es:\n",
    "\n",
    "1. **Reemplazar la última capa** para que la red tenga 26 salidas (una por cada letra del alfabeto inglés).\n",
    "2. **Congelar todos los parámetros originales** y **aplicar LoRA** únicamente a la segunda capa lineal (`fc2`), usando rango 2 y factor de escala $\\alpha=0.1$.\n",
    "3. \n",
    "4. Verificar que sólo los parámetros de las capas LoRA y de la nueva capa de salida estén marcados para entrenamiento (`requires_grad=True`).\n",
    "\n",
    "**Código base con la sección de LoRA ya integrada**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Asumimos que LinearWithLoRA está definido/importado correctamente:\n",
    "# from lora import LinearWithLoRA\n",
    "\n",
    "class NNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNet, self).__init__()\n",
    "        # Capa de convolución C1: 1 canal de entrada, 6 de salida, kernel 5x5\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        # Capa de convolución C3: 6 canales de entrada, 16 de salida, kernel 5x5\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # Capa totalmente conectada F5: de 400 a 120 neuronas\n",
    "        self.fc1  = nn.Linear(16 * 5 * 5, 120)\n",
    "        # Capa totalmente conectada F6: de 120 a 84 neuronas\n",
    "        self.fc2  = nn.Linear(120, 84)\n",
    "        # Capa de salida original (se reemplazará después)\n",
    "        self.fc3  = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # C1 + ReLU -> tamaño (N, 6, 28, 28)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        # S2 Pooling 2x2 -> tamaño (N, 6, 14, 14)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        # C3 + ReLU -> tamaño (N, 16, 10, 10)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        # S4 Pooling 2x2 -> tamaño (N, 16, 5, 5)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        # Aplanamiento -> tamaño (N, 400)\n",
    "        x = torch.flatten(x, 1)\n",
    "        # F5 + ReLU -> tamaño (N, 120)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # F6 + ReLU -> tamaño (N, 84)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # Capa de salida -> tamaño (N, 26) después del reemplazo\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Instanciar y mover a dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_exercise = NNet().to(device)\n",
    "\n",
    "print('Modelo antes de aplicar LoRA:')\n",
    "print(model_exercise)\n",
    "print(\"\\n###############\\n\")\n",
    "\n",
    "# 1) Congela todos los parámetros originales\n",
    "for param in model_exercise.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 2) Reemplaza la capa final por una con 26 salidas (A–Z)\n",
    "model_exercise.fc3 = nn.Linear(in_features=84, out_features=26, bias=True).to(device)\n",
    "\n",
    "# 3) Aplica LoRA solo a la segunda capa lineal (fc2)\n",
    "### REEMPLAZAR AQUÍ ###\n",
    "# Sustituimos fc2 por una versión que incluye adaptadores LoRA\n",
    "model_exercise.fc2 = LinearWithLoRA(\n",
    "    original_linear = model_exercise.fc2,\n",
    "    rank            = 2,\n",
    "    alpha           = 0.1\n",
    ").to(device)\n",
    "### FIN REEMPLAZO ###\n",
    "\n",
    "print('Modelo después de aplicar LoRA:')\n",
    "print(model_exercise)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tu respuesta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "prev_pub_hash": "76d39c215db0c382687ee9cbd1add55a12e9d224c61198bd34619b63d8828b90"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
