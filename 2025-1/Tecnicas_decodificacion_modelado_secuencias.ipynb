{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0a1d661",
   "metadata": {},
   "source": [
    "### **Modelos secuencia a secuencia**\n",
    "\n",
    "Un modelo secuencia a secuencia (Seq2Seq) es una arquitectura de red neuronal diseñada para transformar una secuencia de entrada en una secuencia de salida, y es especialmente útil para tareas donde la longitud de las secuencias de entrada y salida puede diferir. \n",
    "\n",
    "Los modelos Seq2Seq son comúnmente utilizados en aplicaciones como traducción automática, resumen de texto, y generación de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c74e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# S: Símbolo que marca el inicio de la entrada de decodificación\n",
    "# E: Símbolo que marca el final de la salida de decodificación\n",
    "# P: Símbolo de relleno para completar secuencias cortas\n",
    "\n",
    "# Parámetros del modelo\n",
    "n_step = 5          # Longitud de las secuencias (pasos de tiempo)\n",
    "n_hidden = 128      # Tamaño del estado oculto de las RNNs\n",
    "\n",
    "# Definición del conjunto de caracteres y su mapeo numérico\n",
    "char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz']\n",
    "num_dic = {n: i for i, n in enumerate(char_arr)}\n",
    "\n",
    "# Datos de entrenamiento (pares de palabras)\n",
    "seq_data = [['man', 'women'], ['black', 'white'], ['king', 'queen'], ['girl', 'boy'], ['up', 'down'], ['high', 'low']]\n",
    "\n",
    "n_class = len(num_dic)   # Número de clases = número de caracteres posibles\n",
    "batch_size = len(seq_data)  # Número de pares de datos\n",
    "\n",
    "# Función para preparar el lote de entrenamiento\n",
    "def make_batch():\n",
    "    input_batch, output_batch, target_batch = [], [], []\n",
    "\n",
    "    for seq in seq_data:\n",
    "        for i in range(2):\n",
    "            seq[i] = seq[i] + 'P' * (n_step - len(seq[i]))  # Rellenar con 'P' si es necesario\n",
    "\n",
    "        input_seq = [num_dic[n] for n in seq[0]]              # Convertir caracteres a índices\n",
    "        output_seq = [num_dic[n] for n in ('S' + seq[1])]      # Agregar símbolo 'S' al inicio de la salida\n",
    "        target = [num_dic[n] for n in (seq[1] + 'E')]          # Agregar símbolo 'E' al final como target\n",
    "\n",
    "        input_batch.append(np.eye(n_class)[input_seq])         # One-hot encoding de entrada\n",
    "        output_batch.append(np.eye(n_class)[output_seq])       # One-hot encoding de salida\n",
    "        target_batch.append(target)                            # Target como índices (no one-hot)\n",
    "\n",
    "    input_batch = np.array(input_batch)\n",
    "    output_batch = np.array(output_batch)\n",
    "    target_batch = np.array(target_batch)\n",
    "\n",
    "    return torch.FloatTensor(input_batch), torch.FloatTensor(output_batch), torch.LongTensor(target_batch)\n",
    "\n",
    "# Función para preparar un lote de prueba dado un input_word\n",
    "def make_testbatch(input_word):\n",
    "    input_w = input_word + 'P' * (n_step - len(input_word))  # Rellenar si es necesario\n",
    "    input_seq = [num_dic[n] for n in input_w]\n",
    "    output_seq = [num_dic[n] for n in 'S' + 'P' * n_step]     # Salida inicializada con 'S' + relleno\n",
    "\n",
    "    input_batch = np.eye(n_class)[input_seq]\n",
    "    output_batch = np.eye(n_class)[output_seq]\n",
    "\n",
    "    input_batch = np.array(input_batch)\n",
    "    output_batch = np.array(output_batch)\n",
    "\n",
    "    return torch.FloatTensor(input_batch).unsqueeze(0), torch.FloatTensor(output_batch).unsqueeze(0)\n",
    "\n",
    "# Definición del modelo Seq2Seq\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        # Capa RNN para codificador\n",
    "        self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, num_layers=2, dropout=0.5)\n",
    "        # Capa RNN para decodificador\n",
    "        self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, num_layers=2, dropout=0.5)\n",
    "        # Capa totalmente conectada para predecir clases\n",
    "        self.fc = nn.Linear(n_hidden, n_class)\n",
    "\n",
    "    def forward(self, enc_input, enc_hidden, dec_input):\n",
    "        # Cambiar dimensiones: [n_step, batch_size, n_class]\n",
    "        enc_input = enc_input.transpose(0, 1)\n",
    "        dec_input = dec_input.transpose(0, 1)\n",
    "\n",
    "        # Codificar\n",
    "        _, enc_states = self.enc_cell(enc_input, enc_hidden)\n",
    "\n",
    "        # Decodificar\n",
    "        outputs, _ = self.dec_cell(dec_input, enc_states)\n",
    "\n",
    "        # Clasificación de la salida\n",
    "        modelo = self.fc(outputs)  # [n_step+1, batch_size, n_class]\n",
    "        return modelo\n",
    "\n",
    "# Inicializar modelo, función de pérdida y optimizador\n",
    "modelo = Seq2Seq()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(modelo.parameters(), lr=0.001)\n",
    "\n",
    "# Preparar los lotes de entrada/salida/target\n",
    "input_batch, output_batch, target_batch = make_batch()\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "for epoch in range(5000):\n",
    "    # Inicializar el estado oculto\n",
    "    hidden = torch.zeros(2, batch_size, n_hidden)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Propagación hacia adelante\n",
    "    output = modelo(input_batch, hidden, output_batch)\n",
    "    output = output.transpose(0, 1)  # [batch_size, n_step+1, n_class]\n",
    "\n",
    "    loss = 0\n",
    "    for i in range(batch_size):\n",
    "        # Calcular la pérdida para cada muestra\n",
    "        loss += criterion(output[i], target_batch[i])\n",
    "\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f'Época: {epoch + 1:04d}, costo = {loss.item():.6f}')\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Función para traducir una palabra nueva\n",
    "def translate(word):\n",
    "    # Preparar lotes\n",
    "    input_batch, output_batch = make_testbatch(word)\n",
    "    hidden = torch.zeros(2, 1, n_hidden)\n",
    "\n",
    "    # Propagación hacia adelante\n",
    "    output = modelo(input_batch, hidden, output_batch)\n",
    "    \n",
    "    # Obtener predicciones: tomar la clase con mayor probabilidad\n",
    "    predict = output.data.max(2, keepdim=True)[1]\n",
    "    decoded = [char_arr[i.item()] for i in predict.squeeze()]\n",
    "\n",
    "    # Detenerse en 'E' si aparece\n",
    "    if 'E' in decoded:\n",
    "        end = decoded.index('E')\n",
    "        translated = ''.join(decoded[:end])\n",
    "    else:\n",
    "        translated = ''.join(decoded)\n",
    "\n",
    "    return translated.replace('P', '')  # Eliminar los rellenos\n",
    "\n",
    "# Pruebas de traducción\n",
    "print('test')\n",
    "print('man ->', translate('man'))\n",
    "print('mans ->', translate('mans'))\n",
    "print('king ->', translate('king'))\n",
    "print('black ->', translate('black'))\n",
    "print('upp ->', translate('upp'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5ae88d",
   "metadata": {},
   "source": [
    "### **Greedy, beam search y selección aleatoria con temperatura**\n",
    "\n",
    "Podemos implementar las estrategias de decodificación greedy, beam search y selección aleatoria con temperatura, podemos modificar el código dado agregando funciones específicas para cada método de decodificación.\n",
    "\n",
    "**Estrategia greedy**\n",
    "\n",
    "La estrategia greedy selecciona el token con la mayor probabilidad en cada paso de decodificación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6b79a4-579b-4ed9-9573-b8bb2cb8caf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Parámetros principales\n",
    "n_step = 5          # Número de pasos de tiempo (longitud máxima de secuencias)\n",
    "n_hidden = 128      # Número de neuronas ocultas en las RNNs\n",
    "\n",
    "# Definición del conjunto de caracteres y mapeo de caracteres a índices\n",
    "char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz']\n",
    "num_dic = {n: i for i, n in enumerate(char_arr)}\n",
    "\n",
    "# Datos de entrenamiento: pares de secuencias\n",
    "seq_data = [['man', 'women'], ['black', 'white'], ['king', 'queen'], ['girl', 'boy'], ['up', 'down'], ['high', 'low']]\n",
    "\n",
    "n_class = len(num_dic)     # Número de clases = número total de caracteres\n",
    "batch_size = len(seq_data) # Tamaño del lote = número de pares\n",
    "\n",
    "# Función para preparar el lote de entrenamiento\n",
    "def make_batch():\n",
    "    input_batch, output_batch, target_batch = [], [], []\n",
    "\n",
    "    for seq in seq_data:\n",
    "        for i in range(2):\n",
    "            # Rellenar las secuencias con 'P' si son más cortas que n_step\n",
    "            seq[i] = seq[i] + 'P' * (n_step - len(seq[i]))\n",
    "\n",
    "        # Convertir caracteres a índices\n",
    "        input_seq = [num_dic[n] for n in seq[0]]\n",
    "        output_seq = [num_dic[n] for n in ('S' + seq[1])] # Agrega símbolo 'S' al inicio\n",
    "        target = [num_dic[n] for n in (seq[1] + 'E')]      # Agrega símbolo 'E' al final\n",
    "\n",
    "        # Codificar en one-hot\n",
    "        input_batch.append(np.eye(n_class)[input_seq])\n",
    "        output_batch.append(np.eye(n_class)[output_seq])\n",
    "        target_batch.append(target) # El target es una secuencia de índices (no one-hot)\n",
    "\n",
    "    # Convertir listas a tensores de PyTorch\n",
    "    input_batch = np.array(input_batch)\n",
    "    output_batch = np.array(output_batch)\n",
    "    target_batch = np.array(target_batch)\n",
    "\n",
    "    return torch.FloatTensor(input_batch), torch.FloatTensor(output_batch), torch.LongTensor(target_batch)\n",
    "\n",
    "# Función para preparar el lote de prueba para una palabra de entrada\n",
    "def make_testbatch(input_word):\n",
    "    input_w = input_word + 'P' * (n_step - len(input_word)) # Rellenar si es necesario\n",
    "    input_seq = [num_dic[n] for n in input_w]\n",
    "    output_seq = [num_dic[n] for n in 'S' + 'P' * n_step]   # Salida empieza con 'S' + relleno\n",
    "\n",
    "    input_batch = np.eye(n_class)[input_seq]\n",
    "    output_batch = np.eye(n_class)[output_seq]\n",
    "\n",
    "    # Convertir a tensores y agregar dimensión batch\n",
    "    return torch.FloatTensor(input_batch).unsqueeze(0), torch.FloatTensor(output_batch).unsqueeze(0)\n",
    "\n",
    "# Definición de la arquitectura del modelo Seq2Seq\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        # Codificador RNN de 2 capas con dropout\n",
    "        self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, num_layers=2, dropout=0.5)\n",
    "        # Decodificador RNN de 2 capas con dropout\n",
    "        self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, num_layers=2, dropout=0.5)\n",
    "        # Capa completamente conectada para predecir caracteres\n",
    "        self.fc = nn.Linear(n_hidden, n_class)\n",
    "\n",
    "    def forward(self, enc_input, enc_hidden, dec_input):\n",
    "        # Transponer para que el tiempo esté en la primera dimensión\n",
    "        enc_input = enc_input.transpose(0, 1) # [n_step, batch_size, n_class]\n",
    "        dec_input = dec_input.transpose(0, 1) # [n_step, batch_size, n_class]\n",
    "\n",
    "        # Propagación codificadora\n",
    "        _, enc_states = self.enc_cell(enc_input, enc_hidden)\n",
    "\n",
    "        # Propagación decodificadora\n",
    "        outputs, _ = self.dec_cell(dec_input, enc_states)\n",
    "\n",
    "        # Aplicar capa final para obtener distribución de clases\n",
    "        modelo = self.fc(outputs)\n",
    "\n",
    "        return modelo, enc_states\n",
    "\n",
    "# Inicialización del modelo, función de pérdida y optimizador\n",
    "modelo = Seq2Seq()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(modelo.parameters(), lr=0.001)\n",
    "\n",
    "# Preparar los lotes de entrada, salida y objetivos\n",
    "input_batch, output_batch, target_batch = make_batch()\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "for epoch in range(5000):\n",
    "    modelo.train() # Modo entrenamiento\n",
    "    hidden = torch.zeros(2, batch_size, n_hidden) # Estado oculto inicial\n",
    "\n",
    "    optimizer.zero_grad() # Reiniciar gradientes\n",
    "    output, _ = modelo(input_batch, hidden, output_batch)\n",
    "    output = output.transpose(0, 1) # [batch_size, n_step, n_class]\n",
    "\n",
    "    # Calcular pérdida sumando las pérdidas por elemento\n",
    "    loss = 0\n",
    "    for i in range(batch_size):\n",
    "        loss += criterion(output[i], target_batch[i])\n",
    "\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f'Época: {epoch + 1:04d}, costo = {loss.item():.6f}')\n",
    "\n",
    "    loss.backward() # Retropropagación\n",
    "    optimizer.step() # Actualización de parámetros\n",
    "\n",
    "# Función para traducir una nueva palabra usando decodificación greedy\n",
    "def translate(word):\n",
    "    modelo.eval() # Modo evaluación\n",
    "    with torch.no_grad(): # No calcular gradientes\n",
    "        input_w = word + 'P' * (n_step - len(word)) # Rellenar si es necesario\n",
    "        input_seq = [num_dic[n] for n in input_w]\n",
    "        input_batch = np.eye(n_class)[input_seq]\n",
    "        input_batch = torch.FloatTensor(input_batch).unsqueeze(0) # [1, n_step, n_class]\n",
    "\n",
    "        hidden = torch.zeros(2, 1, n_hidden) # Estado oculto inicial para prueba\n",
    "        enc_input = input_batch.transpose(0, 1)\n",
    "\n",
    "        # Codificar la entrada\n",
    "        _, enc_states = modelo.enc_cell(enc_input, hidden)\n",
    "\n",
    "        # Inicializar entrada del decodificador con símbolo 'S'\n",
    "        decoder_input = torch.zeros(1, 1, n_class)\n",
    "        decoder_input[0, 0, num_dic['S']] = 1.0\n",
    "\n",
    "        decoded = []\n",
    "        for _ in range(n_step + 1):\n",
    "            # Paso de decodificación\n",
    "            output, enc_states = modelo.dec_cell(decoder_input, enc_states)\n",
    "            output = modelo.fc(output.squeeze(0))  # [1, n_class]\n",
    "\n",
    "            _, topi = output.topk(1) # Elegir el índice de mayor probabilidad\n",
    "            next_token = topi.item()\n",
    "            char = char_arr[next_token]\n",
    "\n",
    "            if char == 'E': # Finalizar si se predice 'E'\n",
    "                break\n",
    "            if char != 'P': # Ignorar símbolos de relleno\n",
    "                decoded.append(char)\n",
    "\n",
    "            # Actualizar entrada del decodificador\n",
    "            decoder_input = torch.zeros(1, 1, n_class)\n",
    "            decoder_input[0, 0, next_token] = 1.0\n",
    "\n",
    "        translated = ''.join(decoded)\n",
    "        return translated\n",
    "\n",
    "# Pruebas de traducción\n",
    "print('Prueba de decodificación greedy:')\n",
    "print('man ->', translate('man'))\n",
    "print('mans ->', translate('mans'))\n",
    "print('king ->', translate('king'))\n",
    "print('black ->', translate('black'))\n",
    "print('upp ->', translate('upp'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae18962-1071-4a87-9f6b-63d27d47cced",
   "metadata": {},
   "source": [
    "**Estrategia beam search**\n",
    "\n",
    "Beam Search mantiene las mejores k secuencias en cada paso, lo que permite explorar múltiples caminos en la decodificación y seleccionar la secuencia más probable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f34097-8d29-4b32-9301-fca30e49c2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Parámetros principales\n",
    "n_step = 5          # Número de pasos de tiempo (longitud fija de secuencias)\n",
    "n_hidden = 128      # Tamaño del estado oculto en las RNNs\n",
    "\n",
    "# Definición de caracteres y mapeo a índices numéricos\n",
    "char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz']\n",
    "num_dic = {n: i for i, n in enumerate(char_arr)}\n",
    "\n",
    "# Datos de entrenamiento: pares de palabras\n",
    "seq_data = [['man', 'women'], ['black', 'white'], ['king', 'queen'], ['girl', 'boy'], ['up', 'down'], ['high', 'low']]\n",
    "\n",
    "n_class = len(num_dic)     # Número total de caracteres (clases)\n",
    "batch_size = len(seq_data) # Número de pares de datos\n",
    "\n",
    "# Función para preparar el lote de entrenamiento\n",
    "def make_batch():\n",
    "    input_batch, output_batch, target_batch = [], [], []\n",
    "\n",
    "    for seq in seq_data:\n",
    "        for i in range(2):\n",
    "            seq[i] = seq[i] + 'P' * (n_step - len(seq[i]))  # Rellenar con 'P' si la secuencia es más corta que n_step\n",
    "\n",
    "        input_seq = [num_dic[n] for n in seq[0]]              # Indices para la entrada\n",
    "        output_seq = [num_dic[n] for n in ('S' + seq[1])]      # 'S' como inicio de salida\n",
    "        target = [num_dic[n] for n in (seq[1] + 'E')]          # 'E' como final para el target\n",
    "\n",
    "        input_batch.append(np.eye(n_class)[input_seq])         # One-hot encoding de entrada\n",
    "        output_batch.append(np.eye(n_class)[output_seq])       # One-hot encoding de salida\n",
    "        target_batch.append(target)                            # Target como índices\n",
    "\n",
    "    # Convertir las listas a arrays de numpy y luego a tensores\n",
    "    input_batch = np.array(input_batch)\n",
    "    output_batch = np.array(output_batch)\n",
    "    target_batch = np.array(target_batch)\n",
    "\n",
    "    return torch.FloatTensor(input_batch), torch.FloatTensor(output_batch), torch.LongTensor(target_batch)\n",
    "\n",
    "# Función para preparar un lote de prueba (para inferencia)\n",
    "def make_testbatch(input_word):\n",
    "    input_w = input_word + 'P' * (n_step - len(input_word))  # Rellenar si es necesario\n",
    "    input_seq = [num_dic.get(n, num_dic['P']) for n in input_w]  # Convertir a índices\n",
    "    output_seq = [num_dic[n] for n in 'S' + 'P' * n_step]    # Salida comienza con 'S' y 'P's\n",
    "\n",
    "    input_batch = np.eye(n_class)[input_seq]\n",
    "    output_batch = np.eye(n_class)[output_seq]\n",
    "\n",
    "    return torch.FloatTensor(input_batch).unsqueeze(0), torch.FloatTensor(output_batch).unsqueeze(0)\n",
    "\n",
    "# Definición del modelo Seq2Seq\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, num_layers=2, dropout=0.5)  # Codificador RNN\n",
    "        self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, num_layers=2, dropout=0.5)  # Decodificador RNN\n",
    "        self.fc = nn.Linear(n_hidden, n_class)  # Capa completamente conectada para predecir caracteres\n",
    "\n",
    "    def forward(self, enc_input, enc_hidden, dec_input):\n",
    "        enc_input = enc_input.transpose(0, 1)  # [n_step, batch_size, n_class]\n",
    "        dec_input = dec_input.transpose(0, 1)  # [n_step, batch_size, n_class]\n",
    "\n",
    "        _, enc_states = self.enc_cell(enc_input, enc_hidden)   # Codificación\n",
    "        outputs, _ = self.dec_cell(dec_input, enc_states)      # Decodificación\n",
    "\n",
    "        modelo = self.fc(outputs)  # Salida final (predicción de caracteres)\n",
    "        return modelo, enc_states\n",
    "\n",
    "# Inicializar modelo, función de pérdida y optimizador\n",
    "modelo = Seq2Seq()\n",
    "criterion = nn.CrossEntropyLoss()                     # Función de pérdida de entropía cruzada\n",
    "optimizer = torch.optim.Adam(modelo.parameters(), lr=0.001)  # Optimizador Adam\n",
    "\n",
    "# Preparar los lotes de entrenamiento\n",
    "input_batch, output_batch, target_batch = make_batch()\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "for epoch in range(5000):\n",
    "    modelo.train()                                    # Modo entrenamiento\n",
    "    hidden = torch.zeros(2, batch_size, n_hidden)     # Inicializar estado oculto\n",
    "\n",
    "    optimizer.zero_grad()                             # Resetear gradientes\n",
    "    output, _ = modelo(input_batch, hidden, output_batch)  # Forward pass\n",
    "    output = output.transpose(0, 1)                   # [batch_size, n_step, n_class]\n",
    "\n",
    "    # Calcular pérdida acumulada para el batch\n",
    "    loss = 0\n",
    "    for i in range(batch_size):\n",
    "        loss += criterion(output[i], target_batch[i])\n",
    "\n",
    "    if (epoch + 1) % 1000 == 0:                        # Imprimir cada 1000 épocas\n",
    "        print(f'Época: {epoch + 1:04d}, costo = {loss.item():.6f}')\n",
    "\n",
    "    loss.backward()                                   # Backpropagation\n",
    "    optimizer.step()                                  # Actualizar parámetros\n",
    "\n",
    "# Función de decodificación usando beam search\n",
    "def translate_beam_search(word, beam_size=3, max_dec_steps=10):\n",
    "    modelo.eval()  # Cambiar a modo evaluación\n",
    "    with torch.no_grad():  # No calcular gradientes\n",
    "        input_w = word + 'P' * (n_step - len(word))  # Rellenar si necesario\n",
    "        input_seq = [num_dic.get(n, num_dic['P']) for n in input_w]\n",
    "        input_batch = np.eye(n_class)[input_seq]\n",
    "        input_batch = torch.FloatTensor(input_batch).unsqueeze(0)\n",
    "\n",
    "        hidden = torch.zeros(2, 1, n_hidden)          # Inicializar estado oculto\n",
    "        enc_input = input_batch.transpose(0, 1)\n",
    "\n",
    "        _, enc_states = modelo.enc_cell(enc_input, hidden)  # Codificación\n",
    "\n",
    "        # Inicializar haz de búsqueda con el símbolo 'S'\n",
    "        beam = [{\n",
    "            'sequence': [num_dic['S']],\n",
    "            'hidden': enc_states.clone(),\n",
    "            'score': 0.0\n",
    "        }]\n",
    "        completed_sequences = []\n",
    "\n",
    "        for _ in range(max_dec_steps):\n",
    "            new_beam = []\n",
    "            for seq in beam:\n",
    "                last_token = seq['sequence'][-1]\n",
    "\n",
    "                if last_token == num_dic['E']:  # Si se alcanza el final\n",
    "                    completed_sequences.append(seq)\n",
    "                    continue\n",
    "\n",
    "                # Crear entrada para decodificador\n",
    "                decoder_input = torch.zeros(1, 1, n_class)\n",
    "                decoder_input[0, 0, last_token] = 1.0\n",
    "\n",
    "                dec_output, dec_hidden = modelo.dec_cell(decoder_input, seq['hidden'])\n",
    "                logits = modelo.fc(dec_output.squeeze(0))\n",
    "                log_probs = nn.functional.log_softmax(logits, dim=1)\n",
    "\n",
    "                # Tomar top-k tokens más probables\n",
    "                topk_log_probs, topk_indices = torch.topk(log_probs, beam_size, dim=1)\n",
    "\n",
    "                # Crear nuevas secuencias candidatas\n",
    "                for i in range(beam_size):\n",
    "                    token = topk_indices[0, i].item()\n",
    "                    score = seq['score'] + topk_log_probs[0, i].item()\n",
    "                    new_seq = seq['sequence'] + [token]\n",
    "                    new_hidden = dec_hidden.clone()\n",
    "                    new_beam.append({\n",
    "                        'sequence': new_seq,\n",
    "                        'hidden': new_hidden,\n",
    "                        'score': score\n",
    "                    })\n",
    "\n",
    "            # Mantener solo las mejores beam_size secuencias\n",
    "            new_beam = sorted(new_beam, key=lambda x: x['score'], reverse=True)\n",
    "            beam = new_beam[:beam_size]\n",
    "\n",
    "            # Terminar si todas las secuencias han llegado a 'E'\n",
    "            if len(completed_sequences) >= beam_size:\n",
    "                break\n",
    "\n",
    "        if not completed_sequences:  # Si no hay completas, usar actuales\n",
    "            completed_sequences = beam\n",
    "\n",
    "        completed_sequences = sorted(completed_sequences, key=lambda x: x['score'], reverse=True)\n",
    "        best_sequence = completed_sequences[0]['sequence']\n",
    "\n",
    "        # Decodificar la mejor secuencia\n",
    "        decoded = [char_arr[i] for i in best_sequence]\n",
    "\n",
    "        if 'E' in decoded:\n",
    "            end = decoded.index('E')\n",
    "            decoded = decoded[:end]\n",
    "\n",
    "        translated = ''.join([char for char in decoded if char not in ['S', 'P']])\n",
    "        return translated\n",
    "\n",
    "# Prueba de la decodificación beam search\n",
    "print('\\nPrueba de decodificación beam search:')\n",
    "print('man ->', translate_beam_search('man'))\n",
    "print('mans ->', translate_beam_search('mans'))\n",
    "print('king ->', translate_beam_search('king'))\n",
    "print('black ->', translate_beam_search('black'))\n",
    "print('upp ->', translate_beam_search('upp'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e705b50c-920d-4c30-89fe-0a2bcf1f7d36",
   "metadata": {},
   "source": [
    "**Selección aleatoria con temperatura**\n",
    "\n",
    "La selección aleatoria con temperatura ajusta las probabilidades de los tokens antes de muestrear de la distribución, permitiendo más exploración en la generación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeff0976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Parámetros principales\n",
    "n_step = 5          # Número de pasos de tiempo (longitud fija de la secuencia)\n",
    "n_hidden = 128      # Tamaño del estado oculto de las RNNs\n",
    "\n",
    "# Definición del conjunto de caracteres y su mapeo a índices numéricos\n",
    "char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz']\n",
    "num_dic = {n: i for i, n in enumerate(char_arr)}\n",
    "\n",
    "# Datos de entrenamiento: pares de palabras\n",
    "seq_data = [['man', 'women'], ['black', 'white'], ['king', 'queen'], ['girl', 'boy'], ['up', 'down'], ['high', 'low']]\n",
    "\n",
    "n_class = len(num_dic)      # Número total de caracteres/clases\n",
    "batch_size = len(seq_data)  # Número de pares de datos en el lote\n",
    "\n",
    "# Función para crear el lote de entrenamiento\n",
    "def make_batch():\n",
    "    input_batch, output_batch, target_batch = [], [], []\n",
    "\n",
    "    for seq in seq_data:\n",
    "        for i in range(2):\n",
    "            # Rellenar las palabras con 'P' hasta alcanzar n_step caracteres\n",
    "            seq[i] = seq[i] + 'P' * (n_step - len(seq[i]))\n",
    "\n",
    "        # Convertir caracteres a índices\n",
    "        input_seq = [num_dic[n] for n in seq[0]]\n",
    "        output_seq = [num_dic[n] for n in ('S' + seq[1])]  # Añadir 'S' al inicio de la secuencia de salida\n",
    "        target = [num_dic[n] for n in (seq[1] + 'E')]      # Añadir 'E' al final de la secuencia objetivo\n",
    "\n",
    "        # One-hot encoding de entrada y salida\n",
    "        input_batch.append(np.eye(n_class)[input_seq])\n",
    "        output_batch.append(np.eye(n_class)[output_seq])\n",
    "        target_batch.append(target)  # El objetivo es una lista de índices (no one-hot)\n",
    "\n",
    "    # Convertir listas a tensores de PyTorch\n",
    "    input_batch = np.array(input_batch)\n",
    "    output_batch = np.array(output_batch)\n",
    "    target_batch = np.array(target_batch)\n",
    "\n",
    "    return torch.FloatTensor(input_batch), torch.FloatTensor(output_batch), torch.LongTensor(target_batch)\n",
    "\n",
    "# Función para crear lote de prueba a partir de una palabra de entrada\n",
    "def make_testbatch(input_word):\n",
    "    input_w = input_word + 'P' * (n_step - len(input_word))  # Rellenar si es necesario\n",
    "    input_seq = [num_dic[n] for n in input_w]\n",
    "    output_seq = [num_dic[n] for n in 'S' + 'P' * n_step]    # Salida inicializada con 'S' seguido de 'P'\n",
    "\n",
    "    input_batch = np.eye(n_class)[input_seq]\n",
    "    output_batch = np.eye(n_class)[output_seq]\n",
    "\n",
    "    return torch.FloatTensor(input_batch).unsqueeze(0), torch.FloatTensor(output_batch).unsqueeze(0)\n",
    "\n",
    "# Definición del modelo Seq2Seq\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        # RNN para codificación\n",
    "        self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, num_layers=2, dropout=0.5)\n",
    "        # RNN para decodificación\n",
    "        self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, num_layers=2, dropout=0.5)\n",
    "        # Capa totalmente conectada para predecir clases\n",
    "        self.fc = nn.Linear(n_hidden, n_class)\n",
    "\n",
    "    def forward(self, enc_input, enc_hidden, dec_input):\n",
    "        # Cambiar orden de dimensiones para las RNNs\n",
    "        enc_input = enc_input.transpose(0, 1)  # [n_step, batch_size, n_class]\n",
    "        dec_input = dec_input.transpose(0, 1)  # [n_step, batch_size, n_class]\n",
    "\n",
    "        _, enc_states = self.enc_cell(enc_input, enc_hidden)   # Propagación en codificador\n",
    "        outputs, _ = self.dec_cell(dec_input, enc_states)      # Propagación en decodificador\n",
    "\n",
    "        modelo = self.fc(outputs)  # Aplicar capa final de predicción\n",
    "        return modelo\n",
    "\n",
    "# Inicializar modelo, función de pérdida y optimizador\n",
    "modelo = Seq2Seq()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(modelo.parameters(), lr=0.001)\n",
    "\n",
    "# Preparar los lotes\n",
    "input_batch, output_batch, target_batch = make_batch()\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "for epoch in range(5000):\n",
    "    hidden = torch.zeros(2, batch_size, n_hidden)  # Estado oculto inicial (2 capas)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = modelo(input_batch, hidden, output_batch)\n",
    "    output = output.transpose(0, 1)  # [batch_size, n_step+1, n_class]\n",
    "\n",
    "    # Cálculo de la pérdida acumulada sobre el lote\n",
    "    loss = 0\n",
    "    for i in range(batch_size):\n",
    "        loss += criterion(output[i], target_batch[i])\n",
    "\n",
    "    if (epoch + 1) % 1000 == 0:  # Mostrar el costo cada 1000 épocas\n",
    "        print(f'Época: {epoch + 1:04d}, costo = {loss.item():.6f}')\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Estrategias de decodificación\n",
    "\n",
    "# Decodificación greedy (elige siempre el token de mayor probabilidad)\n",
    "def greedy_decode(input_batch, hidden, output_batch):\n",
    "    output = modelo(input_batch, hidden, output_batch)\n",
    "    predict = output.data.max(2, keepdim=True)[1]\n",
    "    return predict\n",
    "\n",
    "# Decodificación usando Beam Search\n",
    "def beam_search_decode(input_batch, hidden, output_batch, beam_width=3):\n",
    "    output = modelo(input_batch, hidden, output_batch)\n",
    "    output = F.softmax(output, dim=2)  # Convertir logits a probabilidades\n",
    "\n",
    "    sequences = [[list(), 1.0]]  # Inicializar con secuencia vacía y score 1.0\n",
    "    for row in output.squeeze(1):\n",
    "        all_candidates = []\n",
    "        for seq, score in sequences:\n",
    "            for j in range(len(row)):\n",
    "                candidate = [seq + [j], score * row[j].item()]\n",
    "                all_candidates.append(candidate)\n",
    "        ordered = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)\n",
    "        sequences = ordered[:beam_width]  # Mantener las mejores beam_width secuencias\n",
    "\n",
    "    best_sequence = sequences[0][0]\n",
    "    return torch.tensor(best_sequence, dtype=torch.long).view(1, -1, 1)\n",
    "\n",
    "# Muestreo aleatorio con temperatura\n",
    "def random_sample_with_temperature(output, temperature=1.0):\n",
    "    output = output.div(temperature).exp()\n",
    "    probs = F.softmax(output, dim=-1)\n",
    "    return torch.multinomial(probs.view(-1, probs.size(-1)), 1).view(output.size(0), output.size(1), -1)\n",
    "\n",
    "# Decodificación usando muestreo con temperatura\n",
    "def decode_with_temperature(input_batch, hidden, output_batch, temperature=1.0):\n",
    "    output = modelo(input_batch, hidden, output_batch)\n",
    "    sampled_output = random_sample_with_temperature(output, temperature)\n",
    "    return sampled_output\n",
    "\n",
    "# Función general para traducir una palabra usando diferentes estrategias\n",
    "def translate(word, strategy='greedy', beam_width=3, temperature=1.0):\n",
    "    input_batch, output_batch = make_testbatch(word)\n",
    "    hidden = torch.zeros(2, 1, n_hidden)\n",
    "\n",
    "    if strategy == 'greedy':\n",
    "        predict = greedy_decode(input_batch, hidden, output_batch)\n",
    "    elif strategy == 'beam_search':\n",
    "        predict = beam_search_decode(input_batch, hidden, output_batch, beam_width)\n",
    "    elif strategy == 'temperature':\n",
    "        predict = decode_with_temperature(input_batch, hidden, output_batch, temperature)\n",
    "\n",
    "    decoded = [char_arr[i] for i in predict.squeeze()]\n",
    "    if 'E' in decoded:  # Cortar la secuencia si se encuentra el final 'E'\n",
    "        end = decoded.index('E')\n",
    "        translated = ''.join(decoded[:end])\n",
    "    else:\n",
    "        translated = ''.join(decoded)\n",
    "\n",
    "    return translated.replace('P', '')  # Eliminar caracteres de relleno\n",
    "\n",
    "# Comparativa entre estrategias de decodificación\n",
    "words = ['man', 'mans', 'king', 'black', 'upp']\n",
    "\n",
    "print('Comparativa de decodificación:\\n')\n",
    "for word in words:\n",
    "    print(f'Palabra: {word}')\n",
    "    print(f'Greedy: {translate(word, strategy=\"greedy\")}')\n",
    "    print(f'Beam Search: {translate(word, strategy=\"beam_search\", beam_width=3)}')\n",
    "    print(f'Temperatura (T=1.0): {translate(word, strategy=\"temperature\", temperature=1.0)}')\n",
    "    print('---------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9d8228",
   "metadata": {},
   "source": [
    "### **Ejercicios**\n",
    "\n",
    "1 .Implementa un mecanismo de atención en el modelo Seq2Seq para mejorar la traducción.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Añade una capa de atención al modelo Seq2Seq.\n",
    "- Modifica el método forward para incorporar la atención.\n",
    "- Entrena el modelo y compara el rendimiento con el modelo original.\n",
    "\n",
    "Pistas:\n",
    "\n",
    "- Utiliza la clase nn.Linear para calcular los pesos de atención.\n",
    "- Multiplica los pesos de atención con los estados ocultos del codificador para obtener el contexto.\n",
    "- Concatena el contexto con la entrada del decodificador en cada paso de tiempo.\n",
    "\n",
    "2 . Compara el rendimiento de las estrategias de decodificación Greedy, Beam Search y Temperatura en diferentes configuraciones de entrenamiento.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Entrena el modelo con diferentes tamaños de conjunto de datos y configuraciones de hiperparámetros (por ejemplo, diferentes tamaños de hidden_size y num_layers).\n",
    "- Aplica las tres estrategias de decodificación a cada modelo entrenado.\n",
    "- Evalúa y compara la calidad de las traducciones utilizando métricas como la precisión y el BLEU score.\n",
    "\n",
    "Pistas:\n",
    "\n",
    "- Usa conjuntos de datos grandes y pequeños para ver cómo cambia el rendimiento.\n",
    "- Experimenta con diferentes beam_width y temperatures.\n",
    "\n",
    "3 . Implementa una variante de Beam Search que penalice secuencias más largas para evitar repeticiones innecesarias.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Modifica la función beam_search_decode para incluir una penalización de longitud.\n",
    "- Ajusta el cálculo de las puntuaciones de las secuencias para penalizar las secuencias más largas.\n",
    "- Compara los resultados con el Beam Search estándar.\n",
    "\n",
    "Pistas:\n",
    "\n",
    "- Multiplica la puntuación de cada secuencia por una función de penalización basada en su longitud.\n",
    "- Puedes usar una función de penalización lineal o exponencial.\n",
    "\n",
    "4 . Implementa una estrategia de decodificación por temperatura que ajuste dinámicamente la temperatura en cada paso de tiempo.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Modifica la función decode_with_temperature para ajustar la temperatura en cada paso de tiempo.\n",
    "- Implementa una función que disminuya la temperatura a medida que avanza la decodificación, incentivando exploración al principio y explotación al final.\n",
    "- Evalúa el impacto de la temperatura dinámica en la calidad de las traducciones.\n",
    "\n",
    "Pistas:\n",
    "\n",
    "- Usa una función de decremento lineal o exponencial para la temperatura.\n",
    "- Compara los resultados con una temperatura fija.\n",
    "\n",
    "5 . Analiza la complejidad computacional y el rendimiento de las diferentes estrategias de decodificación.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Mide el tiempo de ejecución de las estrategias Greedy, Beam Search y Temperatura para diferentes tamaños de vocabulario y longitudes de secuencia.\n",
    "- Analiza cómo cambia la complejidad computacional con respecto a beam_width y temperature.\n",
    "- Discute los trade-offs entre la calidad de la traducción y el tiempo de ejecución.\n",
    "\n",
    "Pistas:\n",
    "\n",
    "- Usa la biblioteca time para medir el tiempo de ejecución.\n",
    "- Realiza pruebas con diferentes configuraciones y grafica los resultados.\n",
    "\n",
    "\n",
    "6 .Mejora la generalización del modelo incorporando técnicas de regularización y dropout.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Añade capas de dropout adicionales al modelo Seq2Seq.\n",
    "- Implementa técnicas de regularización como L2.\n",
    "- Entrena el modelo con estas técnicas y compara el rendimiento con el modelo original.\n",
    "\n",
    "Pistas:\n",
    "\n",
    "- Usa nn.Dropout en las capas RNN y totalmente conectadas.\n",
    "- Ajusta los hiperparámetros de regularización y dropout para encontrar la configuración óptima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efe5f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
