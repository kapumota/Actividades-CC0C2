{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1700247a",
   "metadata": {},
   "source": [
    "## **Fundamentos del aprendizaje por refuerzo**\n",
    "\n",
    "El aprendizaje por refuerzo (RL, por sus siglas en inglés) es un paradigma del aprendizaje automático donde un agente aprende a tomar decisiones mediante la interacción con su entorno, recibiendo recompensas o penalizaciones según sus acciones. \n",
    "\n",
    "A diferencia del aprendizaje supervisado, donde el aprendizaje se basa en un conjunto de datos etiquetados, RL se basa en la retroalimentación recibida a partir de la experiencia directa. \n",
    "\n",
    "#### **Elementos básicos del aprendizaje por refuerzo**\n",
    "\n",
    "En RL, los principales elementos son el agente, el entorno, los estados, las acciones y las recompensas. Estos elementos se relacionan de la siguiente manera:\n",
    "\n",
    "1. **Agente**: Es el tomador de decisiones que interactúa con el entorno. El agente recibe información del entorno, toma decisiones (acciones) y aprende de las consecuencias de esas decisiones.\n",
    "\n",
    "2. **Entorno**: Es el mundo con el que interactúa el agente. El entorno responde a las acciones del agente y proporciona nuevas observaciones y recompensas.\n",
    "\n",
    "3. **Estados (s)**: Representan la situación actual del entorno. Un estado contiene toda la información necesaria para describir la situación en un momento dado.\n",
    "\n",
    "4. **Acciones (a)**: Son las decisiones o movimientos que el agente puede realizar en un estado dado. El conjunto de todas las posibles acciones se denota como A.\n",
    "\n",
    "5. **Recompensas (r)**: Son señales de retroalimentación que indican el valor de una acción en un estado específico. El objetivo del agente es maximizar la recompensa total a lo largo del tiempo.\n",
    "\n",
    "La interacción entre estos elementos puede describirse como una serie de pasos, donde el agente percibe un estado del entorno, selecciona una acción, recibe una recompensa y transita a un nuevo estado.\n",
    "\n",
    "#### **Procesos de decisión de Markov (MDPs)**\n",
    "\n",
    "Los procesos de decisión de Markov (MDPs) son una herramienta matemática utilizada para modelar problemas de RL. Un MDP se define por los siguientes componentes:\n",
    "\n",
    "1. **Conjunto de estados (S)**: Todos los posibles estados en los que el agente puede encontrarse.\n",
    "2. **Conjunto de acciones (A)**: Todas las posibles acciones que el agente puede tomar.\n",
    "3. **Función de transición (P)**: Describe la probabilidad de transición de un estado a otro, dado una acción. $P(s'|s,a)$ representa la probabilidad de moverse al estado $s'$ desde el estado $s$ tomando la acción $a$.\n",
    "4. **Función de recompensa (R)**: Define la recompensa esperada al realizar una acción en un estado particular. $R(s,a)$ es la recompensa inmediata recibida al realizar la acción $a$ en el estado $s$.\n",
    "5. **Factor de descuento ($\\gamma$)**: Es un valor entre 0 y 1 que determina la importancia de las recompensas futuras. Un factor de descuento cercano a 0 hace que el agente se enfoque en recompensas inmediatas, mientras que un valor cercano a 1 da más peso a las recompensas futuras.\n",
    "\n",
    "Los MDPs permiten formalizar el problema de RL y proporcionar una base para diseñar y analizar algoritmos.\n",
    "\n",
    "#### **Políticas y funciones de valor**\n",
    "\n",
    "En RL, una política ($\\pi$) es una estrategia que el agente sigue para decidir qué acciones tomar en cada estado. Una política puede ser determinista ($\\pi(s) = a$) o estocástica ($\\pi(a|s)$), donde la acción es elegida con una cierta probabilidad.\n",
    "\n",
    "Las funciones de valor son herramientas clave para evaluar la calidad de los estados y las acciones bajo una política determinada. Hay dos tipos principales de funciones de valor:\n",
    "\n",
    "1. **Función de valor del estado (V)**: $V^\\pi(s)$ es el valor esperado de las recompensas futuras comenzando desde el estado $s$ y siguiendo la política $\\pi$. Se define como:\n",
    "   $$\n",
    "   V^\\pi(s) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_{t+1} \\mid s_t = s \\right]\n",
    "   $$\n",
    "\n",
    "2. **Función de valor de la acción (Q)**: $Q^\\pi(s,a)$ es el valor esperado de las recompensas futuras al tomar la acción $a$ en el estado $s$ y luego seguir la política $\\pi$. Se define como:\n",
    "   $$\n",
    "   Q^\\pi(s,a) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_{t+1} \\mid s_t = s, a_t = a \\right]\n",
    "   $$\n",
    "\n",
    "El objetivo del agente es encontrar la política óptima ($\\pi^*$) que maximice estas funciones de valor.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d10036",
   "metadata": {},
   "source": [
    "#### **Métodos basados en el valor**\n",
    "\n",
    "Los métodos basados en el valor se centran en aprender una función de valor que estima la calidad de los estados y acciones. Los dos algoritmos más representativos en esta categoría son Q-Learning y SARSA.\n",
    "\n",
    "**Q-Learning**:\n",
    "Q-Learning es un algoritmo off-policy que busca aprender la función de valor de acción $Q(s, a)$. La actualización de Q-Learning se basa en la ecuación de Bellman:\n",
    "\n",
    "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)]$$\n",
    "\n",
    "Aquí, $s$ es el estado actual, $a$ es la acción tomada, $r$ es la recompensa recibida, $s'$ es el nuevo estado, $\\alpha$ es la tasa de aprendizaje, y $\\gamma$ es el factor de descuento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f09d66d",
   "metadata": {},
   "source": [
    "**SARSA**:\n",
    "    \n",
    "SARSA (State-Action-Reward-State-Action) es un algoritmo on-policy que actualiza la función de valor de acción utilizando la acción actual y la siguiente acción seleccionada por la política. La actualización de SARSA se basa en la ecuación:\n",
    "\n",
    "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma Q(s', a') - Q(s, a)]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e520365",
   "metadata": {},
   "source": [
    "#### **Métodos basados en la política**\n",
    "\n",
    "Los métodos basados en la política se enfocan en aprender directamente una política que mapea estados a acciones, sin necesidad de una función de valor intermedia. Los dos algoritmos más comunes en esta categoría son REINFORCE y Actor-Critic.\n",
    "\n",
    "**REINFORCE**:\n",
    "REINFORCE es un algoritmo de gradiente de política que ajusta los parámetros de la política para maximizar la recompensa esperada. La actualización de la política se basa en la ecuación:\n",
    "\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\log \\pi_\\theta(a|s) G_t$$\n",
    "\n",
    "Donde $G_t$ es la recompensa acumulada desde el tiempo $t$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f819b60",
   "metadata": {},
   "source": [
    "**Actor-Critic**:\n",
    "El método Actor-Critic combina una red de política (actor) y una red de valor (critic). El actor actualiza la política basándose en la ventaja estimada por el crítico. La actualización de la política sigue:\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\log \\pi_\\theta(a|s) A(s, a)$$\n",
    "\n",
    "Y la actualización de la función de valor sigue:\n",
    "\n",
    "$$\\phi \\leftarrow \\phi + \\beta \\nabla_\\phi (r + \\gamma V_\\phi(s') - V_\\phi(s))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3119edd3",
   "metadata": {},
   "source": [
    "### **Métodos híbridos**\n",
    "\n",
    "Los métodos híbridos combinan enfoques basados en el valor y en la política para aprovechar las fortalezas de ambos. Entre los algoritmos híbridos más avanzados se encuentran DDPG, PPO y A3C.\n",
    "\n",
    "**DDPG (Deep Deterministic Policy Gradient)**:\n",
    "DDPG es un algoritmo off-policy que combina DQN y el actor-critic. Utiliza una red de actor para seleccionar acciones y una red crítico para evaluar la calidad de esas acciones. También emplea una red de destino para estabilizar el entrenamiento.\n",
    "\n",
    "**PPO (Proximal Policy Optimization)**:\n",
    "PPO es un algoritmo de política basada en la proximidad que busca mejorar la estabilidad del entrenamiento limitando el tamaño del paso de actualización. La función objetivo de PPO es:\n",
    "\n",
    "$$ L^{CLIP}(\\theta) = \\mathbb{E}_t \\left[ \\min \\left( \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)} A_t, \\text{clip}\\left( \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}, 1 - \\epsilon, 1 + \\epsilon \\right) A_t \\right) \\right]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad29401",
   "metadata": {},
   "source": [
    "**A3C (Asynchronous Advantage Actor-Critic)**:\n",
    "\n",
    "A3C es un algoritmo de aprendizaje asincrónico donde múltiples agentes independientes aprenden simultáneamente y actualizan una política global. Utiliza la misma estructura que el actor-critic, pero distribuye el entrenamiento en múltiples instancias del entorno.\n",
    "\n",
    "En resumen, los métodos de aprendizaje por refuerzo abarcan una variedad de enfoques, cada uno con sus propias fortalezas y desafíos. Desde los métodos basados en el valor como Q-Learning y SARSA, hasta los métodos basados en la política como REINFORCE y Actor-Critic, y finalmente los métodos híbridos como DDPG, PPO y A3C, todos juegan un papel crucial en el desarrollo de agentes inteligentes capaces de tomar decisiones en entornos complejos y dinámicos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f36640",
   "metadata": {},
   "source": [
    "### **Métodos de Monte Carlo: simulación y estimación de valores**\n",
    "\n",
    "Los métodos de Monte Carlo son una clase de algoritmos computacionales que dependen de muestreos aleatorios repetidos para obtener resultados numéricos. Se utilizan principalmente en optimización, integración numérica y generación de muestras de una distribución de probabilidad. En el contexto del aprendizaje por refuerzo, los métodos de Monte Carlo se usan para estimar el valor esperado de una política dada, promediando las recompensas observadas a lo largo de múltiples episodios.\n",
    "\n",
    "**Simulación y estimación de valores con Monte Carlo**\n",
    "\n",
    "En el aprendizaje por refuerzo, la simulación mediante métodos de Monte Carlo implica ejecutar varios episodios de interacción entre el agente y el entorno para recopilar datos sobre las recompensas obtenidas. A partir de estas simulaciones, se pueden calcular las estimaciones de valor. Estas estimaciones se basan en la premisa de que, a largo plazo, las recompensas acumuladas reflejan el valor esperado de un estado o una acción.\n",
    "\n",
    "Para estimar los valores de una política, se sigue un procedimiento que incluye:\n",
    "\n",
    "1. **Generación de episodios:** Se generan múltiples episodios siguiendo la política actual, donde un episodio es una secuencia de estados, acciones y recompensas que termina en un estado terminal.\n",
    "2. **Cálculo de retornos:** Para cada estado visitado en un episodio, se calcula el retorno, que es la suma de recompensas futuras descontadas.\n",
    "3. **Promedio de retornos:** Se promedian los retornos de todos los episodios en los que se visitó un estado específico para obtener una estimación de su valor.\n",
    "\n",
    "Una característica fundamental de los métodos de Monte Carlo es que requieren episodios completos, lo que significa que sólo se actualizan los valores al final de un episodio. Esto puede ser una limitación en entornos donde los episodios son largos o no terminan.\n",
    "\n",
    "**Métodos de diferencias temporales (TD): TD($\\lambda$) y n-step TD**\n",
    "\n",
    "Los métodos de diferencias temporales combinan las ventajas del aprendizaje Monte Carlo y el aprendizaje dinámico, actualizando las estimaciones de valores en función de las diferencias temporales, es decir, la diferencia entre las estimaciones de valor consecutivas.\n",
    "\n",
    "**TD($\\lambda$)**\n",
    "\n",
    "TD($\\lambda$) es una técnica que unifica los métodos TD y Monte Carlo mediante el uso de trazas de elegibilidad, que son variables que asignan crédito a los estados y acciones visitados recientemente. $\\lambda$ es un parámetro que controla la ponderación de las actualizaciones.\n",
    "\n",
    "1. **Trazas de elegibilidad:** Se utilizan para dar crédito a los estados visitados recientemente. A medida que el agente se mueve a través del entorno, las trazas de elegibilidad se actualizan, decayendo con cada paso.\n",
    "2. **Actualización de valores:** Las actualizaciones de valores se realizan no solo en función del estado actual y el siguiente estado, sino también en función de los estados anteriores, ponderados por sus trazas de elegibilidad.\n",
    "\n",
    "La fórmula general de actualización para TD($\\lambda$) es:\n",
    "\n",
    "$$V(s) \\leftarrow V(s) + \\alpha \\delta_t e_t(s)$$\n",
    "donde:\n",
    "- $\\alpha$ es la tasa de aprendizaje,\n",
    "- $\\delta_t$ es el error de TD,\n",
    "- $e_t(s)$ es la traza de elegibilidad.\n",
    "\n",
    "**n-step TD**\n",
    "\n",
    "El método n-step TD extiende la idea básica de TD al utilizar recompensas de los siguientes n pasos en lugar de solo la recompensa inmediata y el valor del siguiente estado. Este método actualiza los valores basándose en n pasos futuros de interacción con el entorno.\n",
    "\n",
    "1. **Recompensas acumuladas:** Para cada estado, se acumulan las recompensas de los próximos n pasos.\n",
    "2. **Actualización de valores:** Los valores se actualizan utilizando esta suma de recompensas, proporcionando un equilibrio entre la actualización a corto plazo (TD(0)) y la actualización a largo plazo (Monte Carlo).\n",
    "\n",
    "**Exploración vs. explotación: estrategias epsilon-greedy, Upper Confidence Bound (UCB)**\n",
    "\n",
    "En el aprendizaje por refuerzo, un desafío clave es balancear la exploración de nuevas acciones con la explotación de las acciones conocidas que proporcionan las mayores recompensas. Este dilema se conoce como el trade-off exploración-explotación.\n",
    "\n",
    "**Estrategia epsilon-greedy**\n",
    "\n",
    "La estrategia epsilon-greedy es una de las técnicas más simples y efectivas para gestionar este trade-off. En esta estrategia:\n",
    "\n",
    "1. **Exploración:** Con una probabilidad ($\\epsilon$), el agente selecciona una acción al azar, lo que permite explorar nuevas acciones.\n",
    "2. **Explotación:** Con una probabilidad (1 - $\\epsilon$), el agente selecciona la acción con el mayor valor esperado (explota el conocimiento actual).\n",
    "\n",
    "El parámetro ($\\epsilon$) se puede ajustar durante el entrenamiento, a menudo comenzando con un valor alto para fomentar la exploración y disminuyéndolo gradualmente para favorecer la explotación a medida que el agente aprende más sobre el entorno.\n",
    "\n",
    "**Upper Confidence Bound (UCB)**\n",
    "\n",
    "El método Upper Confidence Bound es otra estrategia para balancear exploración y explotación, utilizando una forma más teórica y matemática basada en la teoría de la toma de decisiones en condiciones de incertidumbre.\n",
    "\n",
    "1. **Valor de confianza:** UCB asigna a cada acción un valor de confianza que aumenta con la incertidumbre de la estimación del valor de esa acción.\n",
    "2. **Selección de acciones:** El agente selecciona la acción con el valor de confianza más alto, lo que favorece las acciones con altos valores esperados y aquellas que han sido menos exploradas.\n",
    "\n",
    "La fórmula general para el valor de confianza en UCB es:\n",
    "$$Q(a) + c \\sqrt{\\frac{\\ln(t)}{N(a)}}$$\n",
    "donde:\n",
    "- $Q(a)$ es el valor estimado de la acción $a$,\n",
    "- $c$ es un parámetro que controla el grado de exploración,\n",
    "- $t$ es el número total de selecciones de acciones,\n",
    "- $N(a)$ es el número de veces que la acción $a$ ha sido seleccionada.\n",
    "\n",
    "UCB proporciona un marco matemáticamente riguroso para el trade-off exploración-explotación, asegurando que cada acción sea seleccionada un número suficiente de veces para obtener estimaciones precisas de su valor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5ec7ac",
   "metadata": {},
   "source": [
    "UCB-V es una variante del algoritmo UCB que considera la varianza en la estimación de las recompensas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c766cb",
   "metadata": {},
   "source": [
    "### **Deep Q-Networks (DQN)**\n",
    "\n",
    "Deep Q-Networks (DQN) representan una evolución significativa en el campo del Aprendizaje por Refuerzo (RL), combinando técnicas tradicionales de RL con redes neuronales profundas. Este enfoque fue popularizado por la investigación de DeepMind, donde se demostró que una red neuronal podía aprender a jugar videojuegos de Atari a nivel humano.\n",
    "\n",
    "En los métodos tradicionales de RL, la función Q se representa mediante tablas (tabular methods) que mapean cada par estado-acción a un valor Q. Sin embargo, este enfoque no es escalable a entornos con grandes espacios de estados y acciones. Aquí es donde entran las redes neuronales: en lugar de almacenar valores Q explícitamente, una red neuronal se entrena para aproximar la función Q.\n",
    "\n",
    "La arquitectura de un DQN es bastante simple: se utiliza una red neuronal con varias capas ocultas que toma el estado del entorno como entrada y produce un valor Q para cada posible acción. La actualización de los pesos de la red neuronal se realiza mediante un proceso de backpropagation, donde el objetivo es minimizar el error entre el valor Q estimado y el valor Q objetivo, el cual se define mediante la ecuación de Bellman."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eb5c39",
   "metadata": {},
   "source": [
    "### **Redes Actor-Critic**\n",
    "El enfoque Actor-Critic combina las ventajas de los métodos basados en políticas y los basados en valor. En lugar de tener una única red que aprenda la política o el valor Q, se utilizan dos redes: una para la política (actor) y otra para el valor (critic).\n",
    "\n",
    "La red Actor se encarga de seleccionar acciones según una política aprendida, mientras que la red Critic evalúa estas acciones proporcionando una estimación del valor Q. \n",
    "\n",
    "El Actor mejora su política utilizando las críticas del Critic, haciendo que este enfoque sea más estable y eficiente en comparación con los métodos puramente basados en políticas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2fd17c",
   "metadata": {},
   "source": [
    "### **Algoritmos avanzados**\n",
    "\n",
    "**Proximal Policy Optimization (PPO)**\n",
    "\n",
    "PPO es uno de los algoritmos más robustos y populares en el aprendizaje por refuerzo. La idea principal detrás de PPO es limitar el cambio en la política en cada actualización para mantener la estabilidad y eficiencia del entrenamiento. PPO emplea una técnica llamada \"clipping\" para evitar cambios demasiado grandes en la política."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299beda7",
   "metadata": {},
   "source": [
    "#### **Asynchronous Advantage Actor-Critic (A3C)**\n",
    "\n",
    "A3C es otro algoritmo avanzado que utiliza múltiples agentes (hilos) que interactúan con el entorno de forma paralela. Cada agente tiene su propia copia de la red neuronal y actualiza los parámetros de la red global de forma asíncrona. Este enfoque mejora la eficiencia del entrenamiento y la estabilidad del modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ac7d94",
   "metadata": {},
   "source": [
    "Estos enfoques avanzados han demostrado ser altamente efectivos para resolver problemas complejos en el campo del aprendizaje por refuerzo, proporcionando un marco robusto y eficiente para la toma de decisiones autónoma en una variedad de entornos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
