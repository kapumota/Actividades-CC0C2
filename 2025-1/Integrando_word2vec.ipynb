{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Integrando Word2Vec**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Configuración**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este cuaderno, utilizarás las siguientes librerías:\n",
    "\n",
    "*   [`torch`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) para construir modelos de redes neuronales y preparar los datos.\n",
    "*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) para operaciones matemáticas.\n",
    "*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) para visualizar los datos.\n",
    "*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) para herramientas adicionales de graficación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Instalación de librerías requeridas**\n",
    "<h5 style=\"color:red;\">Después de instalar las siguientes librerías, por favor REINICIA EL KERNEL y ejecuta todas las celdas.</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todas las librerías requeridas para este cuaderno se enumeran a continuación.\n",
    "#!mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1\n",
    "# Nota: Si tu entorno no soporta \"!mamba install\", utiliza \"!pip install\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim #4.2.0\n",
    "#!pip install portalocker>=2.0.0\n",
    "#!pip install -Uq torchtext #\n",
    "#!pip install -Uq torch\n",
    "#!pip install -Uq torchdata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importación de librerías requeridas\n",
    "\n",
    "_Se recomienda que importes todas las librerías necesarias en un solo lugar (aquí):_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from IPython.core.display import display, SVG\n",
    "\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import logging\n",
    "#from gensim.models import Word2Vec\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.vocab import GloVe, vocab\n",
    "from torchdata.datapipes.iter import IterableWrapper, Mapper\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# También puedes usar esta sección para suprimir advertencias generadas por tu código:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se define una función para graficar los embeddings de palabras en un espacio 2D.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(word_embeddings, vocab=vocab):\n",
    "\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    word_embeddings_2d = tsne.fit_transform(word_embeddings)\n",
    "\n",
    "    # Graficar los resultados con las etiquetas del vocabulario\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    for i, word in enumerate(vocab.get_itos()):  # asumiendo que vocab.itos devuelve la lista de palabras en tu vocabulario\n",
    "        plt.scatter(word_embeddings_2d[i, 0], word_embeddings_2d[i, 1])\n",
    "        plt.annotate(word, (word_embeddings_2d[i, 0], word_embeddings_2d[i, 1]))\n",
    "\n",
    "    plt.xlabel(\"componente t-SNE 1\")\n",
    "    plt.ylabel(\"componente t-SNE 2\")\n",
    "    plt.title(\"Embeddings de palabras visualizados con t-SNE\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se define una función que retorne palabras similares a una palabra específica calculando la distancia coseno.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta función retorna las palabras más similares a una palabra objetivo calculando la distancia coseno entre vectores\n",
    "def find_similar_words(word, word_embeddings, top_k=5):\n",
    "    if word not in word_embeddings:\n",
    "        print(\"Palabra no encontrada en los embeddings.\")\n",
    "        return []\n",
    "\n",
    "    # Obtén el embedding para la palabra dada\n",
    "    target_embedding = word_embeddings[word]\n",
    "\n",
    "    # Calcula las similitudes coseno entre la palabra objetivo y todas las demás\n",
    "    similarities = {}\n",
    "    for w, embedding in word_embeddings.items():\n",
    "        if w != word:\n",
    "            similarity = torch.dot(target_embedding, embedding) / (\n",
    "                torch.norm(target_embedding) * torch.norm(embedding)\n",
    "            )\n",
    "            similarities[w] = similarity.item()\n",
    "\n",
    "    # Ordena las similitudes en orden descendente\n",
    "    sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Retorna las top k palabras similares\n",
    "    most_similar_words = [w for w, _ in sorted_similarities[:top_k]]\n",
    "    return most_similar_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se define una función que entrene el modelo word2vec con datos simples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(modelo, dataloader, criterion, optimizer, num_epochs=1000):\n",
    "    \"\"\"\n",
    "    Entrena el modelo durante el número especificado de épocas.\n",
    "    \n",
    "    Args:\n",
    "        modelo: El modelo de PyTorch a entrenar.\n",
    "        dataloader: DataLoader que proporciona los datos para entrenamiento.\n",
    "        criterion: Función de pérdida.\n",
    "        optimizer: Optimizador para actualizar los pesos del modelo.\n",
    "        num_epochs: Número de épocas para entrenar el modelo.\n",
    "\n",
    "    Returns:\n",
    "        modelo: El modelo entrenado.\n",
    "        epoch_losses: Lista de pérdidas promedio para cada época.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Lista para almacenar la pérdida en cada época\n",
    "    epoch_losses = []\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # Almacena la pérdida acumulada para la época actual\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Uso de tqdm para una barra de progreso\n",
    "        for idx, samples in enumerate(dataloader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Comprueba si el modelo contiene una capa EmbeddingBag\n",
    "            if any(isinstance(module, nn.EmbeddingBag) for _, module in modelo.named_modules()):\n",
    "                target, context, offsets = samples\n",
    "                predicted = modelo(context, offsets)\n",
    "            \n",
    "            # Comprueba si el modelo contiene una capa Embedding\n",
    "            elif any(isinstance(module, nn.Embedding) for _, module in modelo.named_modules()):\n",
    "                target, context = samples\n",
    "                predicted = modelo(context)\n",
    "                \n",
    "            loss = criterion(predicted, target)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(modelo.parameters(), 0.1)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Agrega la pérdida promedio de la época\n",
    "        epoch_losses.append(running_loss / len(dataloader))\n",
    "    \n",
    "    return modelo, epoch_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Word2Vec**\n",
    "\n",
    "Word2Vec es una familia de métodos que transforma palabras en vectores numéricos, posicionando palabras similares cerca unas de otras en un espacio definido por estos números. De esta manera, puedes cuantificar y analizar las relaciones entre palabras matemáticamente. Por ejemplo, palabras como \"cat\" y \"kitten\" o \"cat\" y \"dog\" tienen vectores que están cercanos, mientras que una palabra como \"book\" se posiciona más lejos en este espacio vectorial.\n",
    "\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/Words.png\" alt=\"Ejemplo de Word2Vec\" class=\"bg-primary\" width=\"400px\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **GloVe**\n",
    "\n",
    "GloVe, por otro lado, es otro algoritmo popular para aprender embeddings de palabras.  A diferencia de word2vec, que se basa en predecir palabras de contexto/objetivo, GloVe se enfoca en capturar las estadísticas globales de co-ocurrencia de palabras en todo el corpus. Construye una matriz de co-ocurrencia que representa la frecuencia con la que las palabras aparecen juntas en el texto. La matriz luego se factoriza para obtener los embeddings de palabras. Por ejemplo, si \"Man\" y \"King\" coocurren muchas veces, sus vectores serán similares.\n",
    "\n",
    "El modelo GloVe sigue un enfoque fundamental al construir una gran matriz de co-ocurrencia de palabra-contexto que contiene pares de `(palabra, contexto)`. Cada entrada en esta matriz representa la frecuencia con la que una palabra aparece dentro de un contexto dado, que puede ser una secuencia de palabras. El objetivo del modelo es utilizar técnicas de factoración de matrices para aproximar esta matriz de co-ocurrencia. El proceso se ilustra en el siguiente diagrama:\n",
    "\n",
    "1. Crear una matriz de co-ocurrencia de palabra-contexto: El modelo comienza generando una matriz que captura la información de co-ocurrencia de las palabras y sus contextos circundantes. Cada elemento de la matriz representa cuán a menudo un par específico de palabra y contexto coocurre en los datos de entrenamiento.\n",
    "\n",
    "2. Aplicar factoración de matrices: A continuación, el modelo GloVe aplica métodos de factoración de matrices para aproximar la matriz de co-ocurrencia de palabra-contexto. El objetivo es descomponer la matriz original en representaciones de dimensión inferior que capturen las relaciones semánticas entre palabras y contextos.\n",
    "\n",
    "3. Obtener embeddings de palabra y contexto: Al factorizar la matriz de co-ocurrencia, el modelo obtiene embeddings para palabras y contextos. Estos embeddings son representaciones numéricas que codifican el significado semántico y las relaciones de las palabras y los contextos.\n",
    "\n",
    "Para lograr esto, generalmente puedes comenzar inicializando WF (Word-Feature matrix) y FC (Feature-Context matrix) con pesos aleatorios. Luego, realizarás una operación de multiplicación entre estas matrices para obtener WC' (una aproximación de WC), y evaluar su similitud con WC. Este proceso se repite varias veces utilizando el descenso de gradiente estocástico (SGD) para minimizar el error (WC' - WC).\n",
    "\n",
    "Una vez completado el entrenamiento, la matriz resultante WF  proporciona embeddings de palabras o representaciones vectoriales para cada palabra (el vector verde en el diagrama). La dimensionalidad de los vectores de embedding se puede determinar previamente estableciendo el valor de `F` a un número específico de dimensiones, lo que permite una representación compacta de la semántica de las palabras.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/matrix%20fact.png\" alt=\"Matriz de co-ocurrencia\" class=\"bg-primary\" width=\"600px\">\n",
    "\n",
    "La principal ventaja de GloVe es que puede incorporar tanto estadísticas globales como información de contexto local. Esto resulta en embeddings de palabras que no solo capturan las relaciones semánticas entre palabras, sino que también preservan ciertas relaciones sintácticas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Crear y entrenar modelos word2vec**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data = \"\"\"I wish I was little bit taller\n",
    "I wish I was a baller\n",
    "She wore a small black dress to the party\n",
    "The dog chased a big red ball in the park\n",
    "He had a huge smile on his face when he won the race\n",
    "The tiny kitten played with a fluffy toy mouse\n",
    "The team celebrated their victory with a grand parade\n",
    "She bought a small, delicate necklace for her sister\n",
    "The mountain peak stood majestic and tall against the clear blue sky\n",
    "The toddler took small, careful steps as she learned to walk\n",
    "The house had a spacious backyard with a big swimming pool\n",
    "He felt a sense of accomplishment after completing the challenging puzzle\n",
    "The chef prepared a delicious, flavorful dish using fresh ingredients\n",
    "The children played happily in the small, cozy room\n",
    "The book had an enormous impact on readers around the world\n",
    "The wind blew gently, rustling the leaves of the tall trees\n",
    "She painted a beautiful, intricate design on the small canvas\n",
    "The concert hall was filled with thousands of excited fans\n",
    "The garden was adorned with colorful flowers of all sizes\n",
    "I hope to achieve great success in my chosen career path\n",
    "The skyscraper towered above the city, casting a long shadow\n",
    "He gazed in awe at the breathtaking view from the mountaintop\n",
    "The artist created a stunning masterpiece with bold brushstrokes\n",
    "The baby took her first steps, a small milestone that brought joy to her parents\n",
    "The team put in a tremendous amount of effort to win the championship\n",
    "The sun set behind the horizon, painting the sky in vibrant colors\n",
    "The professor gave a fascinating lecture on the history of ancient civilizations\n",
    "The house was filled with laughter and the sound of children playing\n",
    "She received a warm, enthusiastic welcome from the audience\n",
    "The marathon runner had incredible endurance and determination\n",
    "The child's eyes sparkled with excitement upon opening the gift\n",
    "The ship sailed across the vast ocean, guided by the stars\n",
    "The company achieved remarkable growth in a short period of time\n",
    "The team worked together harmoniously to complete the project\n",
    "The puppy wagged its tail, expressing its happiness and affection\n",
    "She wore a stunning gown that made her feel like a princess\n",
    "The building had a grand entrance with towering columns\n",
    "The concert was a roaring success, with the crowd cheering and clapping\n",
    "The baby took a tiny bite of the sweet, juicy fruit\n",
    "The athlete broke a new record, achieving a significant milestone in her career\n",
    "The sculpture was a masterpiece of intricate details and craftsmanship\n",
    "The forest was filled with towering trees, creating a sense of serenity\n",
    "The children built a small sandcastle on the beach, their imaginations running wild\n",
    "The mountain range stretched as far as the eye could see, majestic and awe-inspiring\n",
    "The artist's brush glided smoothly across the canvas, creating a beautiful painting\n",
    "She received a small token of appreciation for her hard work and dedication\n",
    "The orchestra played a magnificent symphony that moved the audience to tears\n",
    "The flower bloomed in vibrant colors, attracting butterflies and bees\n",
    "The team celebrated their victory with a big, extravagant party\n",
    "The child's laughter echoed through the small room, filling it with joy\n",
    "The sunflower stood tall, reaching for the sky with its bright yellow petals\n",
    "The city skyline was dominated by tall buildings and skyscrapers\n",
    "The cake was adorned with a beautiful, elaborate design for the special occasion\n",
    "The storm brought heavy rain and strong winds, causing widespread damage\n",
    "The small boat sailed peacefully on the calm, glassy lake\n",
    "The artist used bold strokes of color to create a striking and vivid painting\n",
    "The couple shared a passionate kiss under the starry night sky\n",
    "The mountain climber reached the summit after a long and arduous journey\n",
    "The child's eyes widened in amazement as the magician performed his tricks\n",
    "The garden was filled with the sweet fragrance of blooming flowers\n",
    "The basketball player made a big jump and scored a spectacular slam dunk\n",
    "The cat pounced on a small mouse, displaying its hunting instincts\n",
    "The mansion had a grand entrance with a sweeping staircase and chandeliers\n",
    "The raindrops fell gently, creating a rhythmic patter on the roof\n",
    "The baby took a big step forward, encouraged by her parents' applause\n",
    "The actor delivered a powerful and emotional performance on stage\n",
    "The butterfly fluttered its delicate wings, mesmerizing those who watched\n",
    "The company launched a small-scale advertising campaign to test the market\n",
    "The building was constructed with strong, sturdy materials to withstand earthquakes\n",
    "The singer's voice was powerful and resonated throughout the concert hall\n",
    "The child built a massive sandcastle with towers, moats, and bridges\n",
    "The garden was teeming with a variety of small insects and buzzing bees\n",
    "The athlete's muscles were well-developed and strong from years of training\n",
    "The sun cast long shadows as it set behind the mountains\n",
    "The couple exchanged heartfelt vows in a beautiful, intimate ceremony\n",
    "The dog wagged its tail vigorously, a sign of excitement and happiness\n",
    "The baby let out a tiny giggle, bringing joy to everyone around\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se prepara los datos a tokenizar y se crea un vocabulario a partir de ellos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Obtener tokenizador\n",
    "tokenizer = get_tokenizer('basic_english')  # Esto usa el tokenizador básico en inglés. Puedes elegir otro.\n",
    "\n",
    "# Paso 2: Tokenizar oraciones\n",
    "def tokenize_data(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield tokenizer(sentence)\n",
    "\n",
    "tokenized_toy_data = tokenizer(toy_data)\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(tokenize_data(tokenized_toy_data), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos cómo se ve una oración después de la tokenización y de la numeración:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba\n",
    "sample_sentence = \"I wish I was  a baller\"\n",
    "tokenized_sample = tokenizer(sample_sentence)\n",
    "encoded_sample = [vocab[token] for token in tokenized_sample]\n",
    "print(\"Muestra codificada:\", encoded_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se escribe una función para aplicar la numeración a todos los tokens:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda tokens: [vocab[token] for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Bolsa continua de palabras (CBOW)**\n",
    "\n",
    "Para el modelo CBOW, se utiliza un \"contexto\" para predecir una palabra objetivo. El \"contexto\" es típicamente un conjunto de palabras circundantes. Por ejemplo, si una ventana de contexto es de tamaño 2, entonces se toma dos palabras antes y dos palabras después de la palabra objetivo como contexto. La palabra objetivo se muestra en rojo y el contexto en azul:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\">\n",
    "    <tr>\n",
    "        <th>Paso de tiempo</th>\n",
    "        <th>Frase</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td><span style=\"color:blue;\">I wish</span> <span style=\"color:red;\">I</span> <span style=\"color:blue;\">was  little </span></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>2</td>\n",
    "        <td><span style=\"color:blue;\">wish I</span> <span style=\"color:red;\">was</span> <span style=\"color:blue;\">little bit </span></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>3</td>\n",
    "        <td><span style=\"color:blue;\">I was</span> <span style=\"color:red;\">little</span> <span style=\"color:blue;\">  bit taller</span></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>4</td>\n",
    "        <td><span style=\"color:blue;\">was little</span> <span style=\"color:red;\">bit</span> <span style=\"color:blue;\"> taller I</span></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>5</td>\n",
    "        <td><span style=\"color:blue;\">little bit</span> <span style=\"color:red;\">taller</span> <span style=\"color:blue;\"> I wish</span></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>6</td>\n",
    "        <td><span style=\"color:blue;\">bit taller</span> <span style=\"color:red;\">I</span> <span style=\"color:blue;\">wish I</span></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>7</td>\n",
    "        <td><span style=\"color:blue;\">taller I</span> <span style=\"color:red;\">wish</span> <span style=\"color:blue;\">I was</span></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>8</td>\n",
    "        <td><span style=\"color:blue;\">I wish</span> <span style=\"color:red;\">I</span> <span style=\"color:blue;\">was a</span></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>9</td>\n",
    "        <td><span style=\"color:blue;\">wish I</span> <span style=\"color:red;\">was</span> <span style=\"color:blue;\">a baller</span></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede desplazar la secuencia y crear los datos de entrenamiento:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "\n",
    "cobow_data = []\n",
    "for i in range(1, len(tokenized_toy_data) - CONTEXT_SIZE):\n",
    "    context = (\n",
    "        [tokenized_toy_data[i - j - 1] for j in range(CONTEXT_SIZE)]\n",
    "        + [tokenized_toy_data[i + j + 1] for j in range(CONTEXT_SIZE)]\n",
    "    )\n",
    "    target = tokenized_toy_data[i]\n",
    "    cobow_data.append((context, target))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprimimos un ejemplo, mostrando tanto las palabras de contexto (por ejemplo, `['wish', 'i', 'was', 'little']`) como la palabra objetivo (por ejemplo, `'i'`):\n",
    "\n",
    "<table border=\"1\">\n",
    "<tr>\n",
    "        <th>Paso de tiempo</th>\n",
    "        <th>Frase</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td><span style=\"color:blue;\">I wish</span> <span style=\"color:red;\">I</span> <span style=\"color:blue;\">was  little </span></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cobow_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se imprime la siguiente muestra, mostrando tanto el contexto (por ejemplo, `['i', 'wish', 'little', 'bit']`) como la palabra objetivo: `'was'`\n",
    "\n",
    "<table border=\"1\">\n",
    "    <tr>\n",
    "        <th>Paso de tiempo</th>\n",
    "        <th>Frase</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>2</td>\n",
    "        <td><span style=\"color:blue;\">wish I</span> <span style=\"color:red;\">was</span> <span style=\"color:blue;\"> little bit</span></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cobow_data[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se encuentra los embeddings que guíen al modelo para predecir las siguientes probabilidades. $P(w_t| w_{t-2},w_{t-1},w_{t+1},w_{t+2})$ es la probabilidad de $w_t$, la palabra objetivo, condicionada a la ocurrencia de las palabras de contexto $w_{t-2},w_{t-1},w_{t+1},w_{t+2}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>\\( P(w_t| w_{t-2},w_{t-1},w_{t+1},w_{t+2}) \\)</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\\( P(w_1 | w_{-1},w_0,w_2,w_3) = P(I | \\text{I wish, was little}) \\)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>\\( P(w_2 | w_0,w_1,w_3,w_4) = P(was | \\text{wish I, little bit}) \\)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>\\( P(w_3 | w_1,w_2,w_4,w_5) = P(little | \\text{I was, bit taller}) \\)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>\\( P(w_4 | w_2,w_3,w_5,w_6) = P(bit | \\text{was little, taller I}) \\)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>\\( P(w_5 | w_3,w_4,w_6,w_7) = P(taller | \\text{little bit, I wish}) \\)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>\\( P(w_6 | w_4,w_5,w_7,w_8) = P(I | \\text{bit taller, wish I}) \\)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>\\( P(w_7 | w_5,w_6,w_8,w_9) = P(wish | \\text{taller I, I was}) \\)</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `collate_batch` procesa lotes de datos, convirtiendo el texto (contexto y palabra objetivo) a formato numérico usando el vocabulario y organizándolos para el entrenamiento del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    target_list, context_list, offsets = [], [], [0]\n",
    "    for _context, _target in batch:\n",
    "        \n",
    "        target_list.append(vocab[_target])  \n",
    "        processed_context = torch.tensor(text_pipeline(_context), dtype=torch.int64)\n",
    "        context_list.append(processed_context)\n",
    "        offsets.append(processed_context.size(0))\n",
    "    target_list = torch.tensor(target_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    context_list = torch.cat(context_list)\n",
    "    return target_list.to(device), context_list.to(device), offsets.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se selecciona las primeras 10 muestras de la lista `cobow_data` y se procesa usando la función `collate_batch`. Los resultados son las palabras objetivo tokenizadas (`target_list`), las palabras de contexto (`context_list`) y los índices iniciales de contexto para cada muestra (`offsets`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list, context_list, offsets = collate_batch(cobow_data[0:10])\n",
    "print(f\"target_list (palabras objetivo tokenizadas): {target_list} , context_list (palabras de contexto): {context_list} , offsets (índices de inicio para cada contexto): {offsets}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea un objeto ```DataLoader```:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64  # tamaño del lote para entrenamiento\n",
    "\n",
    "dataloader_cbow = DataLoader(\n",
    "    cobow_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "print(dataloader_cbow) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo CBOW que se muestra a continuación comienza con una capa `EmbeddingBag`, que toma una lista de índices de palabras de contexto (de longitud variable) y produce el promedio de los embeddings de dichas palabras. Este embedding se pasa luego a través de una capa lineal que reduce su dimensión a `embed_dim/2`. \n",
    "\n",
    "Tras aplicar la activación ReLU, la salida se procesa con otra capa lineal, transformándola para que coincida con el tamaño del vocabulario, permitiendo así que el modelo prediga la probabilidad de cualquier palabra del vocabulario como palabra objetivo. \n",
    "El flujo general va desde los índices de las palabras de contexto hasta predecir la palabra central en el enfoque de CBOW.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    # Inicializar el modelo CBOW\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        \n",
    "        super(CBOW, self).__init__()\n",
    "         # Definir la capa de embedding usando nn.EmbeddingBag\n",
    "        # Esta capa produce el promedio de los embeddings de las palabras de contexto\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        # Definir la primera capa lineal con tamaño de entrada embed_dim y tamaño de salida embed_dim//2\n",
    "        self.linear1 = nn.Linear(embed_dim, embed_dim//2)\n",
    "        # Definir la capa completamente conectada con tamaño de entrada embed_dim//2 y tamaño de salida vocab_size\n",
    "        self.fc = nn.Linear(embed_dim//2, vocab_size)\n",
    "        \n",
    "\n",
    "        self.init_weights()\n",
    "    # Inicializa los pesos de los parámetros del modelo\n",
    "    def init_weights(self):\n",
    "        # Inicializa los pesos de la capa de embedding\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        # Inicializa los pesos de la capa completamente conectada\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        # Inicializa los sesgos de la capa completamente conectada a ceros\n",
    "        self.fc.bias.data.zero_()\n",
    "        \n",
    "\n",
    "    def forward(self, texto, offsets):\n",
    "        # Pasa el texto de entrada y los offsets a través de la capa de embedding\n",
    "        out = self.embedding(texto, offsets)\n",
    "        # Aplica la función de activación ReLU a la salida de la primera capa lineal\n",
    "        out = torch.relu(self.linear1(out))\n",
    "        # Pasa la salida de la activación ReLU a través de la capa completamente conectada\n",
    "        return self.fc(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea una instancia del modelo CBOW:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "emsize = 24\n",
    "model_cbow = CBOW(vocab_size, emsize, vocab_size).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se define la función de pérdida, el optimizador y el scheduler para el entrenamiento:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 5  # tasa de aprendizaje\n",
    "\n",
    "# Define el criterio CrossEntropyLoss. Es comúnmente usado para tareas de clasificación multiclase.\n",
    "# Este criterio combina la función softmax con la pérdida de log-verosimilitud negativa.\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define el optimizador utilizando descenso de gradiente estocástico (SGD).\n",
    "# Optimiza los parámetros del modelo model_cbow, obtenidos con model_cbow.parameters().\n",
    "# La tasa de aprendizaje (lr) determina el tamaño del paso en cada actualización de los parámetros.\n",
    "optimizer = torch.optim.SGD(model_cbow.parameters(), lr=LR)\n",
    "\n",
    "# Define un scheduler para la tasa de aprendizaje.\n",
    "# El scheduler StepLR ajusta la tasa de aprendizaje durante el entrenamiento.\n",
    "# Multiplica la tasa de aprendizaje por gamma cada 1 época (aquí, 1.0).\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se entrena el modelo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cbow, epoch_losses = train_model(model_cbow, dataloader_cbow, criterion, optimizer, num_epochs=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graficamos los valores de pérdida a lo largo del entrenamiento:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_losses)\n",
    "plt.xlabel(\"épocas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los pesos del modelo son los embeddings de palabras reales. Se pueden cargar en un arreglo de numpy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = model_cbow.embedding.weight.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisamos el vector embebido para una palabra de ejemplo. Observamos la dimensión de este vector, que es igual a `emsize = 24` que se definió anteriormente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'baller'\n",
    "word_index = vocab.get_stoi()[word]  # obtener el índice de la palabra en el vocabulario\n",
    "print(word_embeddings[word_index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos comprobar si los embeddings representan las similitudes entre las palabras. Para ello, a efectos de visualización, se necesita reducir la dimensión de los embeddings para mapear el espacio de embeddings a un espacio 2D. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embeddings(word_embeddings, vocab=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al examinar las proyecciones de t-SNE, se evidencia que, pese a la inevitable pérdida de información por la reducción de dimensiones y las limitaciones de un conjunto de datos pequeño, las palabras con significados similares se agrupan. Por ejemplo, palabras como 'bright' y 'shadow' se encuentran próximas cerca del punto (-15, -15) en los componentes 1 y 2. De igual forma, 'cat', 'dog' y 'mouse' se agrupan alrededor del punto (5, 5), y 'sailed' junto con 'wind' pueden encontrarse próximos al punto (5, -8).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Modelo skip-gram**\n",
    "\n",
    "El modelo skip-gram es una de las dos arquitecturas principales utilizadas en word2vec, una técnica popular para aprender embeddings de palabras. En el modelo Skip-gram, el objetivo es predecir las palabras circundantes (contexto) a partir de una palabra central (objetivo). La idea principal es que las palabras que aparecen en contextos similares tienden a tener significados similares. Considera este ejemplo:\n",
    "\n",
    "**I was little bit taller,**\n",
    "\n",
    "Suponiendo una ventana de contexto de tamaño 2, las palabras en rojo representan el contexto, mientras que la palabra resaltada en azul indica la palabra objetivo:\n",
    "\n",
    "**<span style=\"color:red;\"> I was</span> <span style=\"color:blue;\">little</span> <span style=\"color:red;\">bit taller, </span>**\n",
    "\n",
    "##### **Ventanas en el modelo skip-gram:**\n",
    "\n",
    "Entrenar el modelo skip-gram usando el contexto real puede ser computacionalmente costoso. Esto se debe a que implica predecir probabilidades para cada palabra del vocabulario en cada posición del contexto, a diferencia del CBOW, que solo predice las probabilidades para la palabra objetivo. Para mitigar esto, se emplean varias técnicas de aproximación.\n",
    "\n",
    "Una técnica común es dividir el contexto completo en partes más pequeñas y predecirlas una a una. Esto no solo simplifica la tarea de predicción, sino que también ayuda a un mejor entrenamiento, ya que proporciona múltiples ejemplos de entrenamiento a partir de un único par `(contexto, objetivo)`.\n",
    "\n",
    "##### **Usando la primera fila de la tabla como ejemplo:**\n",
    "\n",
    "Para el ejemplo anterior, la palabra objetivo es **\"little\"**. El contexto completo para esta palabra objetivo es:\n",
    "\n",
    "**I was bit taller**\n",
    "\n",
    "En la aproximación, en lugar de usar el contexto completo para predecir la palabra objetivo, se debe dividir. En este ejemplo hay cuatro aproximaciones:\n",
    "\n",
    "1. Aproximación 1: **I**\n",
    "2. Aproximación 2: **was**\n",
    "3. Aproximación 3: **bit**\n",
    "4. Aproximación 4: **taller**\n",
    "\n",
    "Para cada aproximación, el modelo skip-gram tratará de predecir la palabra objetivo \"little\" usando solo esa parte del contexto. Esto significa que, para la primera aproximación, el modelo intentará predecir \"little\" usando únicamente la palabra \"I\". Para la segunda, usará únicamente \"was\", y así sucesivamente.\n",
    "\n",
    "En conclusión, el modelo skip-gram busca comprender las relaciones entre las palabras prediciendo el contexto a partir de una palabra dada. Técnicas de aproximación, como la ilustrada, ayudan a simplificar el proceso de entrenamiento y hacerlo más eficiente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\">\n",
    "    <tr>\n",
    "        <th>Contexto completo con objetivo</th>\n",
    "        <th>Palabra objetivo</th>\n",
    "        <th>Contexto original del objetivo</th>\n",
    "        <th>Approximacion 1</th>\n",
    "        <th>Approximacion 2</th>\n",
    "        <th>Approximacion 3</th>\n",
    "        <th>Approximacion 4</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><span style=\"color:red;\"> I was</span> <span style=\"color:blue;\">little</span> <span style=\"color:red;\">bit taller, </span></td>\n",
    "        <td>little</td>\n",
    "        <td> I was bit taller,</td>\n",
    "        <td>I</td>\n",
    "        <td>was</td>\n",
    "        <td>bit</td>\n",
    "        <td>taller,</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><span style=\"color:red;\"> was little</span> <span style=\"color:blue;\">bit</span> <span style=\"color:red;\">taller, I </span></td>\n",
    "        <td>bit</td>\n",
    "        <td>was little taller, I</td>\n",
    "        <td>was</td>\n",
    "        <td>little</td>\n",
    "        <td>taller,</td>\n",
    "        <td>I</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><span style=\"color:red;\">little bit</span> <span style=\"color:blue;\">taller,</span> <span style=\"color:red;\">I wish </span></td>\n",
    "        <td>taller,</td>\n",
    "        <td>little bit I wish</td>\n",
    "        <td>little</td>\n",
    "        <td>bit</td>\n",
    "        <td>I</td>\n",
    "        <td>wish</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><span style=\"color:red;\"> bit taller,</span> <span style=\"color:blue;\">I</span> <span style=\"color:red;\">wish I </span></td>\n",
    "        <td>I</td>\n",
    "        <td>bit taller, wish I</td>\n",
    "        <td>bit</td>\n",
    "        <td>taller,</td>\n",
    "        <td>wish</td>\n",
    "        <td>I</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><span style=\"color:red;\"> taller, I</span> <span style=\"color:blue;\">wish</span> <span style=\"color:red;\">I was </span></td>\n",
    "        <td>wish</td>\n",
    "        <td>taller, I I was</td>\n",
    "        <td>taller,</td>\n",
    "        <td>I</td>\n",
    "        <td>I</td>\n",
    "        <td>was</td>\n",
    "    </tr>\n",
    "    <!-- More rows can be added in a similar pattern for other words in the phrase. -->\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo es optimizar las probabilidades condicionales para obtener embeddings, optimizar las probabilidades condicionales para obtener embeddings de alta calidad. \n",
    "\n",
    "> La única diferencia con respecto al CBOW estándar es la estructura de las probabilidades condicionales $P(w_{t+j}| w_{t})$ para una ventana de $j=-2,-1,..,1,2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\">\n",
    "    <tr>\n",
    "        <th>j</th>\n",
    "        <th>Palabra objetivo t=3 </th>\n",
    "        <th>Palabra de contexto</th>\n",
    "        <th>Probabilidad</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "         <th>-1</th>\n",
    "        <td>little</td>\n",
    "        <td>was</td>\n",
    "        <td> P(was | little)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "         <th>1</th>\n",
    "        <td>little</td>\n",
    "        <td>bit</td>\n",
    "        <td>P(bit | little)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "         <th>2</th>\n",
    "        <td>little</td>\n",
    "        <td>taller</td>\n",
    "        <td>P(taller | little) </td>\n",
    "    </tr>\n",
    "    <!-- Repeat rows for each context word for each target word -->\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En contraste con la notación estándar en probabilidad condicional, donde la variable dependiente se representa generalmente como la objetivo, la terminología actual invierte esta convención.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este código construye un conjunto de datos para el modelo skip-gram a partir de datos tokenizados, en el que para cada palabra (objetivo) se recogen las palabras circundantes dentro de una ventana especificada (contexto) definida por `CONTEXT_SIZE`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define el tamaño de la ventana para el contexto alrededor de la palabra objetivo.\n",
    "CONTEXT_SIZE = 2\n",
    "\n",
    "# Inicializa una lista vacía para almacenar los pares (objetivo, contexto).\n",
    "skip_data = []\n",
    "\n",
    "# Itera sobre cada palabra en tokenized_toy_data, excluyendo las primeras y últimas palabras según CONTEXT_SIZE.\n",
    "for i in range(CONTEXT_SIZE, len(tokenized_toy_data) - CONTEXT_SIZE):\n",
    "\n",
    "    # Para la palabra en la posición i, el contexto comprende las palabras anteriores (de acuerdo al CONTEXT_SIZE)\n",
    "    # y las palabras siguientes. Se recoge el contexto en una lista.\n",
    "    context = (\n",
    "        [tokenized_toy_data[i - j - 1] for j in range(CONTEXT_SIZE)]  # Palabras precedentes\n",
    "        + [tokenized_toy_data[i + j + 1] for j in range(CONTEXT_SIZE)]  # Palabras siguientes\n",
    "    )\n",
    "\n",
    "    # La palabra en la posición i se toma como objetivo.\n",
    "    target = tokenized_toy_data[i]\n",
    "\n",
    "    # Agrega el par (objetivo, contexto) a la lista skip_data.\n",
    "    skip_data.append((target, context))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede aplicar una ventana al modelo skip-gram(windowing):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_data_ = [[(sample[0], word) for word in sample[1]] for sample in skip_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y obtener pares `(objetivo, contexto)`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_data_flat = [item for items in skip_data_ for item in items]\n",
    "skip_data_flat[8:28]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea una función `collate` para numerar los pares `(objetivo, contexto)`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    target_list, context_list = [], []\n",
    "    for _context, _target in batch:\n",
    "        \n",
    "        target_list.append(vocab[_target]) \n",
    "        context_list.append(vocab[_context])\n",
    "        \n",
    "    target_list = torch.tensor(target_list, dtype=torch.int64)\n",
    "    context_list = torch.tensor(context_list, dtype=torch.int64)\n",
    "    return target_list.to(device), context_list.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(skip_data_flat, batch_size=BATCH_SIZE, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se revisa una muestra de lote (batch) del par `(objetivo, contexto)` después de la colación:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(dataloader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí, se define la red skip-gram.\n",
    "\n",
    "La capa de embeddings se define utilizando `nn.Embedding`, que crea embeddings para las palabras del vocabulario según el tamaño y la dimensión especificados.\n",
    "La capa `fc` es una capa completamente conectada con dimensión de entrada `embed_dim` y dimensión de salida `vocab_size`.\n",
    "\n",
    "En el método forward, el texto de entrada se pasa por la capa de embeddings para obtener los vectores de las palabras. La salida de la capa de embeddings se pasa luego por la capa fc.\n",
    "Se aplica la activación ReLU a la salida de la capa fc. Finalmente, se retorna la salida.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram_Model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(SkipGram_Model, self).__init__()\n",
    "        # Definir la capa de embeddings\n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embed_dim\n",
    "        )\n",
    "        \n",
    "        # Definir la capa completamente conectada\n",
    "        self.fc = nn.Linear(in_features=embed_dim, out_features=vocab_size)\n",
    "\n",
    "    def forward(self, texto):\n",
    "        # Realiza el pase forward\n",
    "        # Pasa el texto de entrada por la capa de embeddings\n",
    "        out = self.embeddings(texto)\n",
    "        \n",
    "        # Pasa la salida de la capa de embeddings por la capa fc\n",
    "        # Aplica la función de activación ReLU\n",
    "        out = torch.relu(out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea una instancia del modelo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emsize = 24\n",
    "model_sg = SkipGram_Model(vocab_size, emsize).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se entrenar el modelo con datos simples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 5  # tasa de aprendizaje\n",
    "#BATCH_SIZE = 64  # tamaño del lote para el entrenamiento\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_sg.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sg, epoch_losses = train_model(model_sg, dataloader, criterion, optimizer, num_epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede graficar los embeddings de palabras reduciendo sus dimensiones con t-SNE:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = model_sg.embeddings.weight.detach().cpu().numpy()\n",
    "plot_embeddings(word_embeddings, vocab=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando se elige entre CBOW o Skip-gram, la mejor opción suele depender de las especificidades de la tarea y de los datos. Si el conjunto de datos es pequeño pero se necesita una buena representación de palabras poco frecuentes, skip-gram podría ser la mejor opción. Si lo que más importa es la eficiencia computacional y las palabras poco frecuentes no son tan importantes, CBOW podría ser suficiente. Cabe mencionar que, para conjuntos de datos muy pequeños, las ventajas de los embeddings neurales pueden ser limitadas y métodos más simples o el uso de embeddings preentrenados podrían ser más efectivos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Aplicando embeddings de palabras preentrenados**\n",
    "\n",
    "#### Cargar el modelo Stanford GloVe\n",
    "\n",
    "El **transfer learning**, en particular mediante el uso de embeddings preentrenados, es una piedra angular en el NLP moderno. Este enfoque aprovecha el conocimiento adquirido en una tarea, generalmente aprendido con conjuntos de datos masivos, y lo aplica a otra tarea, a menudo más especializada. La principal ventaja es doble: se evita la necesidad de enormes recursos computacionales para aprender desde cero y se inyecta una base de entendimiento lingüístico en el modelo. Al usar embeddings que ya han capturado patrones y asociaciones complejas del lenguaje, incluso modelos con poca exposición a datos específicos pueden comportarse de manera sorprendentemente sofisticada, haciendo del transfer learning un atajo estratégico para mejorar el rendimiento en el NLP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el modelo GloVe preentrenado de Stanford:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede especificar el nombre del modelo y la dimensión del embedding: `GloVe(name='GloVe_model_name', dim=300)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creando una instancia de la versión 6B del modelo Glove()\n",
    "glove_vectors_6B = GloVe(name ='6B')  # puedes especificar el modelo con el siguiente formato: GloVe(name='840B', dim=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creando otra instancia de un modelo Glove() más grande\n",
    "#glove_vectors_840B = GloVe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se debe continuar con el modelo 6B ya que es más liviano. Podemos cargar distintos modelos GloVe preentrenados desde torch utilizando ```torch.nn.Embedding.from_pretrained```. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga los pesos preentrenados del modelo GloVe en una capa de embedding de PyTorch\n",
    "embeddings_Glove6B = torch.nn.Embedding.from_pretrained(glove_vectors_6B.vectors, freeze=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observa los vectores de embedding de este gran modelo preentrenado para las palabras de tu corpus:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un arreglo que retorne el índice de cada palabra en el vocabulario del modelo GloVe:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = glove_vectors_6B.stoi  # Mapeo del vocabulario a índices\n",
    "word_to_index['team']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos el vector de embedding para una palabra:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_Glove6B.weight[word_to_index['team']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisamos qué tan bien el modelo GloVe captura la similitud entre palabras:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un arreglo de palabras de ejemplo\n",
    "words = [\n",
    "    \"taller\",\n",
    "    \"short\",\n",
    "    \"black\",\n",
    "    \"white\",\n",
    "    \"dress\",\n",
    "    \"pants\",\n",
    "    \"big\",\n",
    "    \"small\",\n",
    "    \"red\",\n",
    "    \"blue\",\n",
    "    \"smile\",\n",
    "    \"frown\",\n",
    "    \"race\",\n",
    "    \"stroll\",\n",
    "    \"tiny\",\n",
    "    \"huge\",\n",
    "    \"soft\",\n",
    "    \"rough\",\n",
    "    \"team\",\n",
    "    \"individual\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea un diccionario de palabras y sus embeddings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict_Glove6B = {}\n",
    "for word in words:\n",
    "    # Obtén el índice de la palabra para acceder a su embedding\n",
    "    embedding_vector = embeddings_Glove6B.weight[word_to_index[word]]\n",
    "    if embedding_vector is not None:\n",
    "        # Se omitirán aquellas palabras que no se encuentren en el índice de embeddings\n",
    "        embedding_dict_Glove6B[word] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los embeddings preentrenados para las palabras de ejemplo, veamos si el modelo es capaz de capturar la similitud entre ellas calculando la distancia entre palabras:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama a la función para encontrar palabras similares\n",
    "target_word = \"small\"\n",
    "top_k = 2\n",
    "similar_words = find_similar_words(target_word, embedding_dict_Glove6B, top_k)\n",
    "\n",
    "# Imprime las palabras similares\n",
    "print(\"{} palabras más similares a {}:\".format(top_k, target_word), similar_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Verifica que el modelo GloVe preentrenado realiza un buen trabajo capturando la similitud entre palabras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Clasificación de textos utilizando embeddings de palabras preentrenados**\n",
    "\n",
    "Usemos los embeddings preentrenados para clasificar datos textuales en distintos temas:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, debemos construir un vocabulario a partir del modelo GloVe preentrenado:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe, vocab\n",
    "# Construir el vocabulario a partir de glove_vectors\n",
    "# vocab(ordered_dict: Dict, min_freq: int = 1, specials: Optional[List[str]] = None)\n",
    "vocab = vocab(glove_vectors_6B.stoi, 0, specials=('<unk>', '<pad>'))\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab([\"<unk>\", \"Hello\", \"hello\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, debemos tokenizar el texto. Para ello podemos usar tokenizadores preentrenados de torch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir tokenizador\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "# Definir funciones para procesar el texto y las etiquetas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crean particiones (splits) del conjunto de datos `AG_NEWS()` para entrenamiento, validación y prueba:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide el conjunto de datos en iteradores de entrenamiento y prueba.\n",
    "train_iter, test_iter = AG_NEWS()\n",
    "\n",
    "# Convierte los iteradores de entrenamiento y prueba a datasets de estilo mapa.\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "# Determina el número de muestras a usar para entrenamiento y validación (5% para validación).\n",
    "num_train = int(len(train_dataset) * 0.85)\n",
    "\n",
    "# Separa aleatoriamente el dataset de entrenamiento en conjuntos de entrenamiento y validación usando `random_split`.\n",
    "# El conjunto de entrenamiento contendrá el 85% de las muestras y el de validación el 15% restante.\n",
    "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se definen las etiquetas de clase:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir etiquetas de clase\n",
    "ag_news_label = {1: \"World\", 2: \"Sports\", 3: \"Business\", 4: \"Sci/Tec\"}\n",
    "'''ag_news_label[y]'''\n",
    "num_class = len(set([label for (label, text) in train_iter]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agrupamos los datos en lotes (`collate data`):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_pipeline(x):\n",
    "    x = x.lower()  # se necesita porque el vocabulario está en minúsculas\n",
    "    return vocab(tokenizer(x))\n",
    "\n",
    "def label_pipeline(x):\n",
    "    return int(x) - 1\n",
    "\n",
    "# Crea la etiqueta, el texto y los offsets para cada lote de datos\n",
    "# 'text' es el texto concatenado de todos los datos del lote\n",
    "# necesitas los offsets (índices finales) para separar los textos y predecir sus etiquetas\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea los dataLoaders para las particiones de entrenamiento, validación y prueba:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label, text, offsets = next(iter(train_dataloader))\n",
    "print(label, text, offsets)\n",
    "label.shape, text.shape, offsets.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea el modelo clasificador:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(glove_vectors_6B.vectors, freeze=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text)\n",
    "        # Obtienes el promedio de los embeddings de las palabras en el texto\n",
    "        means = []\n",
    "        for i in range(1, len(offsets)):\n",
    "            text_tmp = embedded[offsets[i-1]:offsets[i]]\n",
    "            means.append(text_tmp.mean(0))\n",
    "\n",
    "        return self.fc(torch.stack(means))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se define una función de evaluación para calcular la precisión del modelo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader):\n",
    "    modelo.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = modelo(text, offsets)\n",
    "\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea una instancia del modelo y se comprueba su capacidad de predicción antes de entrenar:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir hiperparámetros\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 300\n",
    "# Inicializar el modelo\n",
    "modelo = TextClassificationModel(vocab_size, embedding_dim, num_class).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(test_dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se entrena el modelo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_TextClassification(modelo, dataloader, criterion, optimizer, epochs=10):\n",
    "    \n",
    "    cum_loss_list = []\n",
    "    acc_epoch = []\n",
    "    acc_old = 0\n",
    "\n",
    "    for epoch in tqdm(range(1, EPOCHS + 1)):\n",
    "        modelo.train()\n",
    "        cum_loss = 0\n",
    "        for idx, (label, text, offsets) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            predicted_label = modelo(text, offsets)\n",
    "            \n",
    "            loss = criterion(predicted_label, label)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(modelo.parameters(), 0.1)\n",
    "            optimizer.step()\n",
    "            cum_loss += loss.item()\n",
    "\n",
    "        cum_loss_list.append(cum_loss / len(train_dataloader))\n",
    "        accu_val = evaluate(valid_dataloader)\n",
    "        acc_epoch.append(accu_val)\n",
    "\n",
    "        if accu_val > acc_old:\n",
    "            acc_old = accu_val\n",
    "            torch.save(modelo.state_dict(), 'model.pth')\n",
    "            \n",
    "    return modelo, cum_loss_list, acc_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hiperparámetros\n",
    "LR = 0.1\n",
    "EPOCHS = 10\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(modelo.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "\n",
    "modelo, cum_loss_list, acc_epoch = train_TextClassification(modelo, train_dataloader, criterion, optimizer, EPOCHS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se grafica la pérdida y la precisión del modelo entrenado:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot(COST, ACC):\n",
    "    fig, ax1 = plt.subplots()\n",
    "    color = 'tab:red'\n",
    "    ax1.plot(COST, color=color)\n",
    "    ax1.set_xlabel('época', color=color)\n",
    "    ax1.set_ylabel('pérdida total', color=color)\n",
    "    ax1.tick_params(axis='y', color=color)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('precisión', color=color)  # ya se ha definido la etiqueta del eje x con ax1\n",
    "    ax2.plot(ACC, color=color)\n",
    "    ax2.tick_params(axis='y', color=color)\n",
    "    fig.tight_layout()  # para que la etiqueta del eje y derecho no se corte\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(cum_loss_list, acc_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se evalúa el modelo en el conjunto de datos de prueba:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(test_dataloader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "prev_pub_hash": "d8b55d962b308a81277ef91e9a81d5e951c13e7d48f1d3bf55c8cd47d33c8370"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
