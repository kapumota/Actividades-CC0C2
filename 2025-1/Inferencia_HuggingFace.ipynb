{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Carga de modelos e inferencia con Hugging Face**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuración\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este cuaderno, usaremos las siguientes librerías:\n",
    "\n",
    "* [`torch`](https://pytorch.org/) para aprendizaje profundo y modelado de redes neuronales.\n",
    "* [`transformers`](https://huggingface.co/transformers/) para acceder a modelos preentrenados y realizar diversas tareas de NLP con facilidad.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalación de las librerías requeridas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch\n",
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importación de las librerías necesarias\n",
    "\n",
    "*Se recomienda importar todas las librerías requeridas en un solo lugar (aquí):*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Puedes usar esta sección para suprimir advertencias generadas por tu código:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Clasificación de texto con DistilBERT**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cargar el modelo y el tokenizador**\n",
    "\n",
    "Primero, inicializamos un tokenizador y un modelo para análisis de sentimiento usando DistilBERT afinado en el conjunto de datos SST-2. \n",
    "Esta configuración es útil para tareas donde necesitas clasificar rápidamente el sentimiento de un fragmento de texto con un modelo transformer eficiente y preentrenado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del tokenizador y del modelo\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "modelo = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocesamiento del texto de entrada\n",
    "\n",
    "Tokeniza el texto de entrada y conviértelo a un formato adecuado para el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texto simple\n",
    "text = \"Congratulations! You've won a free ticket to the Bahamas. Reply WIN to claim.\"\n",
    "\n",
    "# Tokenización del texto de entrada\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los IDs de los tokens son los índices de los tokens en el vocabulario. El campo `attention_mask` es esencial para procesar correctamente las secuencias con padding, asegurando un cálculo eficiente y manteniendo el rendimiento del modelo. \n",
    "\n",
    "Incluso cuando no hay tokens enmascarados, ayuda al modelo a diferenciar entre contenido real y relleno, lo cual es crítico para un procesamiento preciso y eficiente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizar la inferencia\n",
    "\n",
    "El contexto `torch.no_grad()` se utiliza para desactivar el cálculo de gradientes. Esto reduce el consumo de memoria y acelera el cálculo, ya que no se necesitan gradientes durante la inferencia (es decir, cuando no estás entrenando el modelo). \n",
    "\n",
    "La sintaxis `**inputs` se usa para desempaquetar un diccionario de argumentos de palabra clave en Python, en el contexto de `model(**inputs)`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza la inferencia\n",
    "with torch.no_grad():\n",
    "    outputs = modelo(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma es pasar `input_ids` y `attention_mask` como parámetros separados:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelo(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtener los logits\n",
    "\n",
    "Los logits son las predicciones crudas y no normalizadas del modelo. Extraigámoslos de la salida del modelo para procesarlos posteriormente, como determinar la clase predicha o calcular probabilidades.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs.logits\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postprocesamiento de la salida\n",
    "\n",
    "Convierte los logits en probabilidades y obtén la clase predicha:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convierte logits en probabilidades\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "# Obtiene la clase predicha\n",
    "predicted_class = torch.argmax(probs, dim=-1)\n",
    "\n",
    "# Mapea la clase predicha a la etiqueta\n",
    "labels = [\"NEGATIVE\", \"POSITIVE\"]\n",
    "predicted_label = labels[predicted_class]\n",
    "\n",
    "print(f\"Etiqueta predicha: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Generación de texto con GPT-2** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargar el tokenizador\n",
    "\n",
    "Carga el tokenizador preentrenado de GPT-2. El tokenizador convierte el texto en tokens que el modelo puede entender.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del tokenizador y del modelo\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga el modelo GPT-2 con una cabecera de modelado de lenguaje. El modelo genera texto basándose en los tokens de entrada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga el tokenizador y el modelo\n",
    "\n",
    "modelo = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocesamiento del texto de entrada\n",
    "\n",
    "Tokeniza el texto de entrada y conviértelo a un formato adecuado para el modelo, obteniendo los IDs de los tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt = \"Once upon a time\"\n",
    "\n",
    "# Tokeniza el texto de entrada\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Realiza la inferencia**\n",
    "\n",
    "Genera texto usando el modelo\n",
    "\n",
    "`inputs:` IDs de token de entrada provenientes del tokenizador\n",
    "\n",
    "`attention_mask:` Máscara que indica a qué tokens debe prestar atención\n",
    "\n",
    "`pad_token_id:` ID del token de relleno, establecido como el ID de fin de secuencia\n",
    "\n",
    "`max_length:` Longitud máxima de las secuencias generadas\n",
    "\n",
    "`num_return_sequence:` Número de secuencias a generar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera texto\n",
    "output_ids = modelo.generate(\n",
    "    inputs.input_ids, \n",
    "    attention_mask=inputs.attention_mask,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_length=50, \n",
    "    num_return_sequences=1\n",
    ")\n",
    "\n",
    "output_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "o\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "with torch.no_grad():\n",
    "    outputs = modelo(**inputs) \n",
    "\n",
    "outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postprocesamiento de la salida\n",
    "\n",
    "Decodifica los tokens generados para obtener el texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decodifica el texto generado\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Función `pipeline()` de Hugging Face**\n",
    "\n",
    "La función `pipeline()` de la librería `transformers` de Hugging Face es una API de alto nivel diseñada para simplificar el uso de modelos preentrenados en distintas tareas de procesamiento de lenguaje natural (NLP). \n",
    "\n",
    "Abstrae las complejidades de la carga de modelos, la tokenización, la inferencia y el postprocesamiento, permitiendo a los usuarios realizar tareas complejas de NLP con sólo unas pocas líneas de código.\n",
    "\n",
    "#### Definición\n",
    "\n",
    "```python\n",
    "transformers.pipeline(\n",
    "    task: str,\n",
    "    model: Optional = None,\n",
    "    config: Optional = None,\n",
    "    tokenizer: Optional = None,\n",
    "    feature_extractor: Optional = None,\n",
    "    framework: Optional = None,\n",
    "    revision: str = 'main',\n",
    "    use_fast: bool = True,\n",
    "    model_kwargs: Dict[str, Any] = None,\n",
    "    **kwargs\n",
    ")\n",
    "```\n",
    "\n",
    "#### Parámetros\n",
    "\n",
    "* **task**: `str`\n",
    "\n",
    "  * La tarea a realizar, como `\"text-classification\"`, `\"text-generation\"`, `\"question-answering\"`, etc.\n",
    "  * Ejemplo: `\"text-classification\"`\n",
    "\n",
    "* **model**: `Optional`\n",
    "\n",
    "  * El modelo a usar. Puede ser un identificador de modelo en el hub de Hugging Face (cadena), la ruta a un directorio con archivos de modelo, o una instancia de modelo ya cargada.\n",
    "  * Ejemplo: `\"distilbert-base-uncased-finetuned-sst-2-english\"`\n",
    "\n",
    "* **config**: `Optional`\n",
    "\n",
    "  * La configuración a utilizar. Puede ser una cadena, la ruta a un directorio, o un objeto de configuración ya cargado.\n",
    "  * Ejemplo: `{\"output_attentions\": True}`\n",
    "\n",
    "* **tokenizer**: `Optional`\n",
    "\n",
    "  * El tokenizador a usar. Puede ser una cadena, la ruta a un directorio, o una instancia de tokenizador ya cargada.\n",
    "  * Ejemplo: `\"bert-base-uncased\"`\n",
    "\n",
    "* **feature\\_extractor**: `Optional`\n",
    "\n",
    "  * El extractor de características para tareas que lo requieran (por ejemplo, procesamiento de imágenes).\n",
    "  * Ejemplo: `\"facebook/detectron2\"`\n",
    "\n",
    "* **framework**: `Optional`\n",
    "\n",
    "  * El framework a usar, `\"pt\"` para PyTorch o `\"tf\"` para TensorFlow. Si no se especifica, se infiere automáticamente.\n",
    "  * Ejemplo: `\"pt\"`\n",
    "\n",
    "* **revision**: `str`, valor por defecto `'main'`\n",
    "\n",
    "  * La versión específica del modelo a usar (rama, etiqueta o hash de commit).\n",
    "  * Ejemplo: `\"v1.0\"`\n",
    "\n",
    "* **use\\_fast**: `bool`, valor por defecto `True`\n",
    "\n",
    "  * Indica si se debe usar la versión rápida del tokenizador si está disponible.\n",
    "  * Ejemplo: `True`\n",
    "\n",
    "* **model\\_kwargs**: `Dict[str, Any]`, valor por defecto `None`\n",
    "\n",
    "  * Argumentos adicionales de palabra clave que se pasan al modelo durante la inicialización.\n",
    "  * Ejemplo: `{\"output_hidden_states\": True}`\n",
    "\n",
    "* **kwargs**: `Any`\n",
    "\n",
    "  * Argumentos adicionales que se pasan a los componentes del pipeline.\n",
    "\n",
    "#### Tipos de tareas\n",
    "\n",
    "La función `pipeline()` admite una amplia gama de tareas de NLP. A continuación, algunas de las más comunes:\n",
    "\n",
    "1. **Clasificación de texto**: `text-classification`\n",
    "\n",
    "   * **Propósito**: Clasificar texto en categorías predefinidas.\n",
    "   * **Casos de uso**: Análisis de sentimiento, detección de spam, clasificación de temas.\n",
    "\n",
    "2. **Generación de texto**: `text-generation`\n",
    "\n",
    "   * **Propósito**: Generar texto coherente a partir de un prompt dado.\n",
    "   * **Casos de uso**: Escritura creativa, generación de diálogos, completado de historias.\n",
    "\n",
    "3. **Pregunta-Respuesta**: `question-answering`\n",
    "\n",
    "   * **Propósito**: Responder preguntas basadas en un contexto dado.\n",
    "   * **Casos de uso**: Sistemas de Q\\&A, recuperación de información en documentos.\n",
    "\n",
    "4. **Reconocimiento de Entidades Nombradas (NER)**: `ner` (o `token-classification`)\n",
    "\n",
    "   * **Propósito**: Identificar y clasificar entidades nombradas (personas, organizaciones, ubicaciones) en el texto.\n",
    "   * **Casos de uso**: Extracción de información estructurada de texto no estructurado.\n",
    "\n",
    "5. **Resumen**: `summarization`\n",
    "\n",
    "   * **Propósito**: Resumir textos largos en resúmenes más cortos y coherentes.\n",
    "   * **Casos de uso**: Resumen de documentos, resumen de noticias.\n",
    "\n",
    "6. **Traducción**: `translation_xx_to_yy` (por ejemplo, `translation_en_to_fr`)\n",
    "\n",
    "   * **Propósito**: Traducir texto de un idioma a otro.\n",
    "   * **Casos de uso**: Traducción de idiomas, aplicaciones multilingües.\n",
    "\n",
    "7. **Rellenar máscara**: `fill-mask`\n",
    "\n",
    "   * **Propósito**: Predecir palabras enmascaradas dentro de una oración (útil para modelado de lenguaje enmascarado).\n",
    "   * **Casos de uso**: Tareas de modelado de lenguaje, análisis de predicciones del modelo.\n",
    "\n",
    "8. **Clasificación sin entrenamiento previo (Zero-Shot)**: `zero-shot-classification`\n",
    "\n",
    "   * **Propósito**: Clasificar texto en categorías sin datos de entrenamiento específicos para esas categorías.\n",
    "   * **Casos de uso**: Tareas de clasificación flexibles y adaptables.\n",
    "\n",
    "9. **Extracción de características**: `feature-extraction`\n",
    "\n",
    "   * **Propósito**: Extraer representaciones (estados ocultos) del texto.\n",
    "   * **Casos de uso**: Tareas posteriores que requieran representaciones de texto, como clustering, cálculo de similitud o entrenamiento de modelos personalizados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejemplo 1: Clasificación de texto usando `pipeline()`**\n",
    "\n",
    "En este ejemplo, usaremos la función `pipeline()` para realizar clasificación de texto. Cargaremos un modelo preentrenado de clasificación de texto y lo usaremos para clasificar un texto de ejemplo.\n",
    "\n",
    "#### **Cargar el modelo de clasificación de texto**\n",
    "\n",
    "Inicializamos el pipeline para la tarea `text-classification`, especificando el modelo `\"distilbert-base-uncased-finetuned-sst-2-english\"`. Este modelo está afinado para análisis de sentimiento.\n",
    "\n",
    "#### **Clasificar el texto de ejemplo**\n",
    "\n",
    "Usamos `classifier` para clasificar el texto de ejemplo:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga un modelo genérico de clasificación de texto\n",
    "classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# Clasifica un texto de ejemplo\n",
    "result = classifier(\"Congratulations! You've won a free ticket to the Bahamas. Reply WIN to claim.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salida\n",
    "\n",
    "La salida será una lista de diccionarios, donde cada diccionario contiene:\n",
    "\n",
    "* `label`: La etiqueta predicha (por ejemplo, `\"POSITIVE\"` o `\"NEGATIVE\"`).\n",
    "* `score`: La puntuación de confianza de la predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejemplo 2: Detección de idioma usando `pipeline()`**\n",
    "\n",
    "En este ejemplo, usaremos la función `pipeline()` para realizar detección de idioma. Cargaremos un modelo preentrenado de detección de idioma y lo usaremos para identificar el idioma de un texto de muestra.\n",
    "\n",
    "#### **Cargar el modelo de detección de idioma**\n",
    "\n",
    "Inicializamos el pipeline para la tarea `text-classification`, especificando el modelo `\"papluca/xlm-roberta-base-language-detection\"`. Este modelo está afinado para detección de idioma.\n",
    "\n",
    "#### **Detectar el idioma del texto de ejemplo**\n",
    "\n",
    "Usamos `classifier` para detectar el idioma del texto:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"papluca/xlm-roberta-base-language-detection\")\n",
    "result = classifier(\"Bonjour, comment ça va?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salida\n",
    "\n",
    "La salida será una lista de diccionarios, donde cada diccionario contiene:\n",
    "\n",
    "* `label`: La etiqueta de idioma predicha (por ejemplo, `\"fr\"` para francés).\n",
    "* `score`: La puntuación de confianza de la predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejemplo 3: Generación de texto usando `pipeline()`**\n",
    "\n",
    "En este ejemplo, usaremos la función `pipeline()` para realizar generación de texto. Cargaremos un modelo preentrenado de generación de texto y lo usaremos para generar texto a partir de un prompt dado.\n",
    "\n",
    "#### **Inicializa el pipeline de generación de texto**\n",
    "\n",
    "Inicializamos el pipeline para la tarea `text-generation`, especificando el modelo `\"gpt2\"`. GPT-2 es un modelo conocido para tareas de generación de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa el pipeline de generación de texto con GPT-2\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Generar texto a partir de un prompt dado**\n",
    "\n",
    "Usamos el generador para producir texto a partir del prompt: \"Once upon a time\". Especificamos `max_length=50` y `truncation=True` para limitar el texto generado a 50 tokens, y `num_return_sequences=1` para generar una única secuencia. \n",
    "\n",
    "La función `generator` devuelve el texto generado, que luego se imprime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera texto a partir de un prompt dado\n",
    "prompt = \"Once upon a time\"\n",
    "result = generator(prompt, max_length=50, num_return_sequences=1, truncation=True)\n",
    "\n",
    "# Imprime el texto generado\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salida\n",
    "\n",
    "La salida será una lista de diccionarios, donde cada diccionario contiene:\n",
    "\n",
    "* `generated_text`: El texto generado a partir del prompt de entrada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejemplo 4: Generación de texto con T5 usando `pipeline()`**\n",
    "\n",
    "En este ejemplo, usaremos la función `pipeline()` para realizar generación de texto-a-texto con el modelo T5. Cargaremos un modelo T5 preentrenado y lo usarás para traducir una frase de inglés a francés a partir de un prompt dado.\n",
    "\n",
    "#### Inicializa el pipeline de generación de texto-a-texto\n",
    "\n",
    "Inicializamos el pipeline para la tarea `text2text-generation`, especificando el modelo `\"t5-small\"`. T5 es un modelo versátil que puede realizar varias tareas de generación de texto-a-texto, incluida la traducción.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa el pipeline de generación de texto-a-texto con T5\n",
    "generator = pipeline(\"text2text-generation\", model=\"t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Genera texto basado en un prompt dado:\n",
    "\n",
    "Usamos el generador para traducir una oración del inglés al francés basándonos en el prompt: \"translate English to French: How are you?\". \n",
    "\n",
    "Especificamos `max_length=50` para limitar el texto generado a 50 tokens y `num_return_sequences=1` para generar una única secuencia. La función `generator` devuelve el texto traducido, que luego se imprime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera el texto basado en un prompt dado\n",
    "prompt = \"translate English to French: How are you?\"\n",
    "result = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "\n",
    "# Imprime el texto traducido\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salida\n",
    "\n",
    "La salida será una lista de diccionarios, donde cada diccionario contiene:\n",
    "\n",
    "* `generated_text`: El texto generado (traducido) a partir del prompt de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beneficios de usar `pipeline()`\n",
    "\n",
    "* **Reducción de código repetitivo**: Simplifica el código necesario para realizar tareas de PLN.\n",
    "* **Mejora de la legibilidad**: Hace que el código sea más claro y expresivo.\n",
    "* **Eficiencia de tiempo**: Ahorra tiempo al encargarse automáticamente de la carga del modelo, tokenización, inferencia y postprocesamiento.\n",
    "* **API consistente**: Ofrece una interfaz uniforme para distintas tareas, lo que facilita la experimentación y el prototipado rápido.\n",
    "* **Manejo automático del framework**: Gestiona automáticamente el framework subyacente (TensorFlow o PyTorch).\n",
    "\n",
    "#### Cuándo usar `pipeline()`\n",
    "\n",
    "* **Prototipado rápido**: Cuando necesitas prototipar rápidamente una aplicación de NLP o experimentar con distintos modelos.\n",
    "* **Tareas sencillas**: Para realizar tareas comunes de PLN que están bien soportadas por `pipeline()`.\n",
    "* **Despliegue**: Al desplegar modelos de NLP en entornos donde la simplicidad y facilidad de uso son cruciales.\n",
    "\n",
    "#### Cuándo evitar `pipeline()`\n",
    "\n",
    "* **Tareas personalizadas**: Cuando requieres llevar a cabo tareas muy específicas que `pipeline()` no soporta bien.\n",
    "* **Optimización de rendimiento**: Si necesitas control detallado sobre el modelo y el proceso de tokenización para optimizar el rendimiento o casos de uso particulares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejercicio: tarea de rellenar máscaras con BERT usando `pipeline()`**\n",
    "\n",
    "En este ejercicio usarás la función `pipeline()` para realizar una tarea de fill-mask con el modelo BERT. Cargarás un modelo BERT preentrenado y lo emplearás para predecir la palabra enmascarada en una frase dada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Instrucciones**\n",
    "\n",
    "1. **Inicializa** el pipeline de fill-mask con el modelo BERT.\n",
    "2. **Crea** un prompt que incluya un token enmascarado.\n",
    "3. **Genera** texto rellenando el token enmascarado.\n",
    "4. **Imprime** el texto resultante con las predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "prev_pub_hash": "580f657bfe790433f8d5be264eaea1f8bbd0547d40345696853f98af925da491"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
