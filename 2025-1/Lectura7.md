### Sobre los transformers

Desde su aparición, la arquitectura Transformer ha supuesto un cambio radical en el campo del procesamiento de lenguaje natural, al permitir que los modelos, en lugar de procesar las secuencias de forma estrictamente secuencial, como en las redes recurrentes, puedan atender a todas 
las posiciones de la entrada en paralelo. En esencia, un Transformer recibe una secuencia de embeddings de palabras a las que se les suma una codificación posicional que incorpora información sobre la posición de cada elemento dentro de la secuencia. 

De este modo, aunque el modelo carezca de bucles internos, no pierde la noción del orden original de los tokens. A partir de estas representaciones enriquecidas, el Transformer construye su famoso mecanismo de atención, que ha demostrado ser capaz de capturar relaciones contextuales de largo alcance entre palabras de un modo más efectivo y escalable que las arquitecturas previas.

La **codificación posicional** es esencial para dotar al Transformer de sensibilidad al orden. Una estrategia clásica consiste en generar, para cada posición $i$ y cada dimensión $k$ del espacio de embeddings, valores senoidales y cosenoidales de la forma

$$
\text{PE}(i,2k) = \sin\bigl(i / 10000^{2k/d}\bigr),\quad
\text{PE}(i,2k+1) = \cos\bigl(i / 10000^{2k/d}\bigr),
$$

donde $d$ es la dimensión total del embedding. Alternativamente, se pueden aprender vectores de posición entrenándolos junto al resto de parámetros. En ambos casos, las codificaciones posicionales se suman a los embeddings de palabras, haciendo que cada vector posea información tanto semántica como posicional.

El **mecanismo de atención** se basa en tres matrices,queries ($Q$), keys ($K$) y values ($V$), que se obtienen mediante proyecciones lineales de las representaciones de entrada. Con ellas, se calcula una matriz de atención que determina la importancia de cada posición en la secuencia para cada otra posición. De forma intuitiva, podemos aplicar este mecanismo a los embeddings de las palabras para "preguntar" a cada token qué otras partes del texto son más relevantes para su representación. Gracias a esto, se capturan de manera dinámica y contextual las relaciones entre palabras, superando limitaciones de ventanas de contexto reducidas.

En concreto, la **atención de producto punto escalado** implementa una serie de multiplicaciones matriciales que combinan las queries y las keys, dividiendo luego por la raíz cuadrada de la dimensión de las keys (un factor de escala que estabiliza el entrenamiento), y aplicando después una función softmax para normalizar las puntuaciones. Opcionalmente, se utiliza una máscara que impide atender posiciones no deseadas,por ejemplo, en el decodificador, para que en el entrenamiento autoregresivo no se tenga acceso a futuros tokens. El resultado de la distribución de atención se multiplica por la matriz de values, generando la salida final que incorpora tanto la información local del token como el contexto global de la secuencia.

Más allá de la simple predicción del siguiente token en tareas de modelado de lenguaje, la arquitectura Transformer permite **integrar capas de atención** dentro de soluciones de clasificación de texto, manteniendo el contexto completo de la frase. Al añadir una cabecera de clasificación sobre el token especial \[CLS], por ejemplo, es posible aprovechar la riqueza contextual construida por la red para asignar etiquetas de sentimiento, categorías de intención o cualquier otra etiqueta supervisada, sin perder la coherencia informacional de la secuencia original.

La variante **encoder–decoder** de los Transformers está especialmente diseñada para tareas de traducción automática. En ella, el codificador lee la secuencia de origen completa, aplicando varias capas de auto-atención y redes feed-forward, mientras que el decodificador genera la secuencia destino paso a paso, atendiéndose tanto a sus propios tokens previos (auto-atención enmascarada) como a la representación del codificador (*cross-attention*). Gracias a que ambos procesos se pueden paralelizar por posición, la eficiencia es muy superior a la de los modelos recurrentes: todas las posiciones de entrada y salida se procesan simultáneamente.

El **decodificador**, piedra angular de los modelos de generación de texto, opera de forma autoregresiva, prediciendo el siguiente token condicionado a todos los anteriores generados. Modelos como GPT, LLaMA o Granite son variantes puramente de decodificador; en ellos, durante el entrenamiento se emplea *teacher forcing* (se introducen siempre los tokens correctos del paso anterior), mientras que en la fase de inferencia el sistema enmascara las posiciones futuras, forzando la generación de forma estrictamente autoregresiva. En la práctica, las implementaciones en PyTorch de estos modelos se basan en el módulo `nn.Transformer`, con pequeñas adaptaciones para la gestión de máscaras y la arquitectura del head de predicción.

En el caso de **BERT**, se aprovecha únicamente la parte del codificador, potenciando la atención bidireccional. Durante el preentrenamiento, BERT lleva a cabo dos tareas complementarias: por un lado, en el **Masked Language Modeling** se enmascaran tokens aleatorios en la entrada y el modelo debe predecirlos correctamente; por otro lado, en la **Next Sentence Prediction** el sistema recibe un par de oraciones y debe decidir si la segunda prosigue lógicamente a la primera. De este modo, la arquitectura de BERT, al incorporar atención sobre toda la secuencia desde ambos lados, aprende representaciones altamente contextualizadas. Posteriormente, mediante fine-tuning, puede ajustarse a tareas tan diversas como resumen automático, respuesta a preguntas o análisis de sentimiento, bastando añadir un pequeño head específico y actualizar los pesos con los datos de la tarea.

La **preparación de datos** para estos modelos exige inicializar un tokenizador adecuado, por ejemplo invocando `get_tokenizer('bert-base-uncased')` en PyTorch o Hugging Face, y definir un conjunto de símbolos especiales,\[CLS], \[SEP], \[MASK], \[PAD],asignándoles los índices correspondientes en el vocabulario. El tokenizador fragmenta el texto en subunidades (WordPiece o SentencePiece), genera las secuencias de IDs y, en paralelo, construye máscaras de atención que indiquen al modelo qué posiciones son relevantes y cuáles deben ignorarse.

En la fase de decodificación de un transformer encoder–decoder, el método recibe dos entradas fundamentales: la **secuencia destino** y la **"memoria"** procedente del codificador. La secuencia destino se incrusta mediante el mismo embedding que la fuente y se le aplica la codificación posicional. A continuación, pasa por un bloque de atención enmascarada, luego por la atención cruzada que relaciona cada posición destino con todas las posiciones de la secuencia codificada y, finalmente, por capas feed-forward. De este modo, el Transformer gestiona de forma integrada los procesos de codificación y decodificación, manteniendo la coherencia contextual en todo momento.

Durante la generación de texto para tareas de traducción, la **atención cruzada** resulta especialmente crítica, ya que calcula las puntuaciones de atención entre cada posición del destino y todas las posiciones de la fuente, permitiendo al modelo consultar directamente la información relevante de la entrada en cada paso de la salida. Al combinar múltiples cabeceras de atención en paralelo, el sistema es capaz de atender simultáneamente a distintos tipos de relaciones: por un lado, patrones léxicos cortos; por otro, correspondencias semánticas de largo alcance.

El entrenamiento de estos grandes modelos Transformer suele ser **intensivo en recursos**; sin embargo, existen varias técnicas para optimizar la eficiencia y reducir costes sin sacrificar la calidad. Una de las más sencillas de implementar es la **acumulación de gradientes**: en lugar de procesar batches muy grandes que excedan la memoria de la GPU, se procesan varios mini-batches de forma secuencial, acumulando los gradientes sin actualizar los pesos hasta haber procesado un número prefijado de ellos. De esta forma, se simula el efecto de un batch grande, reduciendo el uso de memoria y, al mismo tiempo, estabilizando las actualizaciones al computar gradientes menos ruidosos.

Otra estrategia muy extendida es el **entrenamiento en precisión mixta** (mixed-precision training). Consiste en realizar gran parte de las operaciones matriciales en media precisión (FP16), lo que reduce drásticamente el consumo de memoria y acelera los cálculos en hardware moderno optimizado para estos formatos. Para evitar problemas de desbordamiento o pérdida de información en gradientes muy pequeños, se utiliza un ***loss scaler*** que ajusta dinámicamente la escala de la pérdida al retropropagar y luego reescala los gradientes antes de actualizar los pesos. Frameworks como PyTorch ofrecen utilidades integradas (`torch.cuda.amp`) que automatizan gran parte del proceso.

Cuando hablamos de modelos que no caben en la memoria de un solo dispositivo, o cuando buscamos repartir la carga de forma óptima, entran en juego las técnicas de **entrenamiento distribuido**. En **data parallelism**, cada GPU recibe un fragmento distinto del batch de datos, calcula la pérdida y los gradientes, y luego sincroniza estos gradientes entre dispositivos antes de actualizar los parámetros (por ejemplo, usando `DistributedDataParallel` en PyTorch). Por su parte, el **model parallelism** reparte las capas del propio modelo entre distintas GPUs, de modo que cada dispositivo almacena solo una parte de la arquitectura. En escenarios de extremo tamaño, se combinan ambas estrategias, o se implementan variantes más sofisticadas como el *tensor parallelism* y el *pipeline parallelism*, que fragmentan las operaciones a nivel de matriz y por etapas de la red respectivamente.

Finalmente, la elección de un **optimizador eficiente** puede marcar la diferencia entre un entrenamiento que converge en pocas épocas y uno que no logra alcanzar el rendimiento deseado. Dos de los optimizadores más populares en el entrenamiento de Transformers son **AdamW** y **LAMB**. AdamW incorpora un término de decaimiento de peso desacoplado de la tasa de aprendizaje, mejorando la generalización y evitando la sobreacumulación de regularización L2. LAMB, por su parte, fue diseñado para escalar a grandes *batch sizes*: calcula un ratio de confianza capa por capa ("layer-wise adaptive moments"), ajustando las actualizaciones de forma que cada parte de la red reciba el tamaño de paso óptimo, lo cual acelera la convergencia en conjuntos de datos extensos sin comprometer la estabilidad.

Conjugar estos avances ,codificación posicional, atención multi-cabecera, arquitecturas encoder–decoder, preentrenamiento de BERT, generación autoregresiva, junto con técnicas como acumulación de gradientes, mixed-precision, entrenamiento distribuido y optimizadores especializados, permite entrenar Transformers monumentales en plazos razonables y con recursos moderados, abriendo la puerta a aplicaciones más innovadoras y escalables en el ámbito del procesamiento de lenguaje natural y más allá.
