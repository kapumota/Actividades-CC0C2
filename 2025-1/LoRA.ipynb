{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LoRA con PyTorch**\n",
    "\n",
    "LoRA (Low-Rank Adaptation) es una t√©cnica de **afinado de par√°metros eficientes** para grandes modelos de lenguaje (LLM) que permite adaptar un modelo preentrenado a una nueva tarea sin necesidad de actualizar o almacenar todos sus billones de par√°metros. A continuaci√≥n se detalla algunas caracter√≠sticas importantes:\n",
    "\n",
    "\n",
    "- `Eficiencia en las actualizaciones de par√°metros:` LoRA introduce solo una peque√±a fracci√≥n de par√°metros adicionales comparado con el n√∫mero total de par√°metros de un modelo grande. Esto hace que el proceso de entrenamiento sea m√°s r√°pido y menos intensivo en recursos, ya que se necesitan actualizar menos par√°metros durante la retropropagaci√≥n.\n",
    "\n",
    "- `Preservaci√≥n del conocimiento preentrenado:` Al mantener la mayor√≠a de los pesos del modelo fijos y ajustarlos √∫nicamente mediante matrices de baja dimensi√≥n, LoRA ayuda a conservar las ricas representaciones que el modelo aprendi√≥ durante el preentrenamiento. Esto es especialmente beneficioso para tareas que no requieren desviaciones dr√°sticas del comportamiento aprendido en el preentrenamiento.\n",
    "\n",
    "- `Personalizaci√≥n para tareas espec√≠ficas:` A pesar de las actualizaciones m√≠nimas, los cambios introducidos por LoRA son lo suficientemente significativos como para adaptar el modelo a tareas concretas. Esto permite afinar modelos grandes en tareas especializadas sin necesidad de un reentrenamiento extenso.\n",
    "\n",
    "- `Reducci√≥n del sobreajuste:` Debido a que solo se adaptan un n√∫mero limitado de par√°metros, el riesgo de sobreajuste es menor en comparaci√≥n con el ajuste fino completo del modelo, especialmente al adaptar el modelo a conjuntos de datos m√°s peque√±os.\n",
    "\n",
    "- `Escalabilidad:` LoRA escala bien con el tama√±o del modelo. A medida que los modelos se vuelven m√°s grandes, el incremento relativo en el n√∫mero de par√°metros introducidos por LoRA se hace a√∫n m√°s peque√±o, lo que lo convierte en una opci√≥n especialmente atractiva para adaptar modelos muy grandes.\n",
    "\n",
    "- `Compatibilidad y simplicidad:` El m√©todo puede aplicarse f√°cilmente a diferentes tipos de redes neuronales, especialmente a aquellas basadas en la arquitectura Transformer. No requiere cambios importantes en la arquitectura existente, lo que simplifica su integraci√≥n en pipelines ya establecidos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Adaptaci√≥n de baja dimensi√≥n (LoRA)**\n",
    "\n",
    "PyTorch y la librer√≠a Hugging Face ofrecen herramientas s√≥lidas para la manipulaci√≥n de modelos con LoRA, pero no son muy intuitivas. \n",
    "\n",
    "En esta secci√≥n, profundizaremos en la construcci√≥n de una implementaci√≥n de LoRA (Low-Rank Adaptation) desde cero usando PyTorch. LoRA es un m√©todo general, pero se aplica com√∫nmente a la capa de atenci√≥n. Por simplicidad, en este cuaderno lo aplicaremos a una red neuronal. Esta decisi√≥n se toma porque acceder a los par√°metros de *atenci√≥n* en el m√≥dulo Encoder de PyTorch puede resultar complejo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **LoRA**\n",
    "\n",
    "1. Para cualquier capa arbitraria de una red, tienes el modelo con par√°metros preentrenados $W_0$, que son los par√°metros del modelo. Si solo consideras los par√°metros de atenci√≥n de cada capa, al menos $4 \\times m \\times n$ por capa. En muchos modelos, esto puede alcanzar billones de par√°metros aprendibles. Cada vez que afinamos con un nuevo conjunto de datos, tendr√≠as que almacenar billones de par√°metros.\n",
    "\n",
    "2. $\\Delta W$ representa dos matrices $B$ y $A$, donde $B$ y $A$ est√°n constri√±idas de manera que $B \\in \\mathbb{R}^{m \\times r}$, $A \\in \\mathbb{R}^{r \\times n}$, y $r \\le \\min(m,n)$. El n√∫mero total de par√°metros en $A$ y $B$ es mucho menor que en $W_1$ y mucho m√°s f√°cil de almacenar.\n",
    "\n",
    "$$\n",
    "W_1 \\approx W_0 + \\Delta W = W_0 + BA\n",
    "$$\n",
    "\n",
    "3. Para entrenar y predecir, en el pase hacia adelante $W_0$ se mantiene constante:\n",
    "\n",
    "$$\n",
    "h = W_0 x + BAx\n",
    "$$\n",
    "\n",
    "Para escalar $\\Delta W$ por $\\tfrac{\\alpha'}{r}$, donde $\\alpha$ es una constante relacionada con $r$, ajustar $\\alpha'$ es similar a sintonizar la tasa de aprendizaje si la inicializaci√≥n est√° correctamente escalada. Por lo tanto, se fija $\\alpha'$ igual al primer $r$ que se prueba y no se sintoniza m√°s; simplemente se usa $\\alpha$. Este escalado reduce la necesidad de retocar hiperpar√°metros. La forma final es:\n",
    "\n",
    "$$\n",
    "h = W_0 x + \\frac{\\alpha'}{r} BAx = W_0 x + \\alpha\\,BAx\n",
    "$$\n",
    "\n",
    "El siguiente ejemplo ilustra el proceso.\n",
    "\n",
    "$\n",
    "W_0 + BA = \n",
    "\\begin{bmatrix}\n",
    "w_{11} & w_{12} & w_{13} & w_{14} \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "w_{21} & w_{22} & w_{23} & w_{24} \\\\\\\\\\\\\n",
    "w_{31} & w_{32} & w_{33} & w_{34} \\\\\\\\\\\\\n",
    "w_{41} & w_{42} & w_{43} & w_{44} \\\\\\\\\\\\\n",
    "\\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "a_1 \\\\\\\\\\\\\n",
    "a_2 \\\\\\\\\\\\\n",
    "a_3 \\\\\\\\\\\\\n",
    "a_4 \\\\\\\\\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "b_1 & b_2 & b_3 & b_4 \\\\\\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Esto muestra el producto de las matrices $A$ y $B$, denotado $AB$, que se puede sumar a $W_0$. Sin embargo, la matriz resultante $W_0 + AB$ est√° limitada seg√∫n las dimensiones de $A$ y $B$, debido al concepto de rango (rank)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Rango**\n",
    "\n",
    "El rango de una matriz es el n√∫mero de dimensiones en las que \"viven\" sus filas (o columnas). \n",
    "\n",
    "Una matriz cuadrada se dice de **rango completo** (full rank) si su rango es igual al n√∫mero de filas o columnas. Hagamos esta idea m√°s intuitiva con un ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "import torchtext#; torchtext.disable_torchtext_deprecation_warning()\n",
    "from torchtext.vocab import build_vocab_from_iterator, GloVe, Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import Matrix, init_printing,Symbol\n",
    "\n",
    "from numpy.linalg import qr,eig,inv,matrix_rank,inv, norm\n",
    "from scipy.linalg import null_space\n",
    "from sympy import Matrix, init_printing,Symbol\n",
    "init_printing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_matrix_and_subspace(F):\n",
    "    # Asegura de que F tenga 3 filas para poder visualizar en 3D.\n",
    "    assert F.shape[0] == 3, \"La matriz F debe tener 3 filas para visualizaci√≥n en 3D.\"\n",
    "    \n",
    "    # Crea una figura y un eje 3D\n",
    "    ax = plt.figure().add_subplot(projection='3d')\n",
    "    \n",
    "    # Para cada columna de F, dibujar un vector desde el origen\n",
    "    for i in range(F.shape[1]):\n",
    "        ax.quiver(\n",
    "            0, 0, 0,                   # origen del vector\n",
    "            F[0, i], F[1, i], F[2, i], # componentes del vector\n",
    "            color='blue',\n",
    "            arrow_length_ratio=0.1,\n",
    "            label=f'Columna {i+1}'     # etiqueta para la leyenda\n",
    "        )\n",
    "\n",
    "    # Si F tiene exactamente 2 columnas, dibujar el plano que ellas generan\n",
    "    if F.shape[1] == 2:\n",
    "        # Calcula el vector normal al plano generado por las dos columnas\n",
    "        normal_vector = np.cross(F[:, 0], F[:, 1])\n",
    "        \n",
    "        # Crea una malla de puntos en XY para dibujar la superficie del plano\n",
    "        xx, yy = np.meshgrid(\n",
    "            np.linspace(-3, 3, 10),\n",
    "            np.linspace(-3, 3, 10)\n",
    "        )\n",
    "        # Ecuaci√≥n del plano: normal ¬∑ [x, y, z] = 0  =>  z = ...\n",
    "        if normal_vector[2] != 0:\n",
    "            zz = (-normal_vector[0] * xx - normal_vector[1] * yy) / normal_vector[2]\n",
    "        else:\n",
    "            zz = np.zeros_like(xx)\n",
    "        \n",
    "        # Dibuja la superficie del plano con transparencia\n",
    "        ax.plot_surface(xx, yy, zz, alpha=0.5, color='green', label='Plano generado')\n",
    "\n",
    "    # Configura l√≠mites de los ejes\n",
    "    ax.set_xlim([-3, 3])\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax.set_zlim([-3, 3])\n",
    "    \n",
    "    # Etiquetas de los ejes\n",
    "    ax.set_xlabel('$x_{1}$')\n",
    "    ax.set_ylabel('$x_{2}$')\n",
    "    ax.set_zlabel('$x_{3}$')\n",
    "    \n",
    "    # Muestra la figura\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_matrix_and_subspace(F):\n",
    "    # Asegura de que F tenga 3 filas para poder visualizar en 3D.\n",
    "    assert F.shape[0] == 3, \"La matriz F debe tener 3 filas para visualizaci√≥n en 3D.\"\n",
    "    \n",
    "    # Crea una figura y un eje 3D\n",
    "    ax = plt.figure().add_subplot(projection='3d')\n",
    "    \n",
    "    # Para cada columna de F, dibujar un vector desde el origen\n",
    "    for i in range(F.shape[1]):\n",
    "        ax.quiver(\n",
    "            0, 0, 0,                   # origen del vector\n",
    "            F[0, i], F[1, i], F[2, i], # componentes del vector\n",
    "            color='blue',\n",
    "            arrow_length_ratio=0.1,\n",
    "            label=f'Columna {i+1}'     # etiqueta para la leyenda\n",
    "        )\n",
    "\n",
    "    # Si F tiene exactamente 2 columnas, dibujar el plano que ellas generan\n",
    "    if F.shape[1] == 2:\n",
    "        # Calcula el vector normal al plano generado por las dos columnas\n",
    "        normal_vector = np.cross(F[:, 0], F[:, 1])\n",
    "        \n",
    "        # Crea una malla de puntos en XY para dibujar la superficie del plano\n",
    "        xx, yy = np.meshgrid(\n",
    "            np.linspace(-3, 3, 10),\n",
    "            np.linspace(-3, 3, 10)\n",
    "        )\n",
    "        # Ecuaci√≥n del plano: normal ¬∑ [x, y, z] = 0  =>  z = ...\n",
    "        if normal_vector[2] != 0:\n",
    "            zz = (-normal_vector[0] * xx - normal_vector[1] * yy) / normal_vector[2]\n",
    "        else:\n",
    "            zz = np.zeros_like(xx)\n",
    "        \n",
    "        # Dibuja la superficie del plano con transparencia\n",
    "        ax.plot_surface(xx, yy, zz, alpha=0.5, color='green', label='Plano generado')\n",
    "\n",
    "    # Configura l√≠mites de los ejes\n",
    "    ax.set_xlim([-3, 3])\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax.set_zlim([-3, 3])\n",
    "    \n",
    "    # Etiquetas de los ejes\n",
    "    ax.set_xlabel('$x_{1}$')\n",
    "    ax.set_ylabel('$x_{2}$')\n",
    "    ax.set_zlabel('$x_{3}$')\n",
    "    \n",
    "    # Muestra la figura\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "En el contexto de Low-Rank Adaptation  (LoRA), donde $B \\in \\mathbb{R}^{d \\times r}$, la matriz $B$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "B=torch.tensor([[1,0],[0,1],[0,0]]).numpy()\n",
    "\n",
    "Matrix(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta matriz de $3 \\times 2$ tiene columnas que generan un subespacio bidimensional en $\\mathbb{R}^3$. Espec√≠ficamente, las columnas de $B$ son:\n",
    "\n",
    "- $\\mathbf{b}_1 = \\begin{bmatrix} 1 \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ 0 \\\\ 0 \\end{bmatrix}$\n",
    "- $\\mathbf{b}_2 = \\begin{bmatrix} 0 \\\\\\\\ 1 \\\\ 0 \\end{bmatrix}$\n",
    "\n",
    "Estos vectores columna son los vectores de la base est√°ndar para el plano $xy$ en $\\mathbb{R}^3$ y, por lo tanto, generan el plano $xy$ mostrado en verde en la imagen siguiente. Multiplicar cualquiera de estos vectores columna (en azul) por un escalar siempre resulta en un punto que queda en ese mismo plano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " plot_matrix_and_subspace(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este escenario, los vectores, a pesar de tener cada uno tres componentes, pueden alcanzar cualquier punto del plano verde bidimensional representado en la imagen. Estos vectores abarcan el plano verde, que reside dentro de un subespacio bidimensional. \n",
    "\n",
    "La dimensi√≥n de este subespacio, tambi√©n conocida como su \"rango\", es dos, lo que corresponde a la dimensionalidad del plano. Si el rango fuera tres, cualquier punto en el espacio 3D podr√≠a alcanzarse mediante alguna combinaci√≥n de las columnas de \\$ùêµ\\$. El rango de una matriz se puede determinar usando la funci√≥n `matrix_rank` proporcionada por NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_rank(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqu√≠, trazas una matriz diferente donde la matriz abarca un plano distinto, pero el rango sigue siendo dos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_=torch.tensor([[1,0],[-2,1],[0,1]]).numpy()\n",
    "plot_matrix_and_subspace(B_)\n",
    "print(\"rank de B\",matrix_rank(B_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_rank(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqu√≠ presentamos la matriz `A`. El rango de esta matriz tambi√©n es dos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=torch.tensor([[1,1,-1,1,0],[-2,2,2,0,1]]).numpy()\n",
    "Matrix(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_rank(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Para las matrices $C = BA$, si $B$ y $A$ tienen ambas rango $r$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C=B@A\n",
    "Matrix(C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las columnas de $C$ tendr√°n el mismo rango que $B$. Adem√°s, el espacio generado por las columnas de $C$ ser√° el mismo que el generado por las columnas de $B$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"rank de C\",matrix_rank(C))\n",
    "plot_matrix_and_subspace(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Entendiendo LoRA in PyTorch**\n",
    "\n",
    "LoRA (Low-Rank Adaptation) es relativamente sencillo de inicializar en PyTorch. Se inicializa LoRA con las dimensiones de la entrada (`in_dim`), $m$, la salida (`out_dim`), $n$, un rango (`rank`), $r$, y un factor de escala `alpha`. Los par√°metros se inicializan de la siguiente forma:\n",
    "\n",
    "```\n",
    "self.A = torch.nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
    "self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "```\n",
    "\n",
    "El uso de `nn.Parameter` hace que estos valores sean par√°metros aprendibles.\n",
    "\n",
    "En la funci√≥n forward, LoRA usa la notaci√≥n $BAx$. En PyTorch, el vector de entrada es una fila, por lo que la salida se convierte en $x^TA^TB^T$; a partir de ahora omitiremos la transposici√≥n. El paso forward se implementa como:\n",
    "\n",
    "```\n",
    "x = self.alpha * (x @ self.A @ self.B)\n",
    "```\n",
    "\n",
    "El uso de `nn.Parameter` hace que estos valores sean par√°metros aprendibles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        self.A = torch.nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
    "        \n",
    "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.alpha * (x @ self.A @ self.B)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta clase `LinearWithLoRA` copia el modelo lineal original y crea un objeto `LoRALayer`.\n",
    "\n",
    "```\n",
    "self.linear = linear.to(device)\n",
    "self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        ).to(device)\n",
    "```\n",
    "\n",
    "Luego, en el m√©todo forward aplica tanto el modelo lineal original como el modelo LoRA a la entrada `x` y los suma: `self.linear(x) + self.lora(x)`. Esto corresponde a:\n",
    "\n",
    "$x W_0 + x A B$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearWithLoRA(torch.nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear.to(device)\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.linear(x) + self.lora(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Configuraci√≥n adicionales**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importar las librer√≠as requeridas\n",
    "\n",
    "El siguiente bloque importa todas las librer√≠as necesarias:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import accumulate\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "torch.set_num_threads(1)\n",
    "from torch import nn\n",
    "import os\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator, GloVe, Vectors\n",
    "from torchtext.datasets import IMDB\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "import pickle\n",
    "\n",
    "from urllib.request import urlopen\n",
    "import io\n",
    "\n",
    "import tarfile\n",
    "import tempfile\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Tambi√©n puedes usar esta secci√≥n para suprimir las advertencias generadas por tu c√≥digo:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Funciones auxiliares**\n",
    "\n",
    "El siguiente c√≥digo muestra algunas funciones de ayuda para graficar, guardar y cargar archivos. Estas funciones no son el foco principal del cuaderno, pero debes ejecutar estas celdas para poder usarlas m√°s adelante.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(COST,ACC):\n",
    "    \"\"\"\n",
    "    Grafica la evoluci√≥n de la p√©rdida total y la precisi√≥n por √©poca.\n",
    "\n",
    "    Par√°metros:\n",
    "        COST (list): Lista con el valor de la p√©rdida total en cada √©poca.\n",
    "        ACC (list): Lista con la precisi√≥n en cada √©poca.\n",
    "    \"\"\"\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    color = 'tab:red'\n",
    "    ax1.plot(COST, color=color)\n",
    "    ax1.set_xlabel('epoca', color=color)\n",
    "    ax1.set_ylabel('Perdida total', color=color)\n",
    "    ax1.tick_params(axis='y', color=color)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('accuracy', color=color)  \n",
    "    ax2.plot(ACC, color=color)\n",
    "    ax2.tick_params(axis='y', color=color)\n",
    "    fig.tight_layout() \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list_to_file(lst, filename):\n",
    "    \"\"\"\n",
    "    Guarda una lista en un archivo usando serializaci√≥n con pickle.\n",
    "\n",
    "    Par√°metros:\n",
    "        lst (list): La lista que se guardar√°.\n",
    "        filename (str): El nombre del archivo donde se guardar√° la lista.\n",
    "\n",
    "    Retorna:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(lst, file)\n",
    "\n",
    "def load_list_from_file(filename):\n",
    "    \"\"\"\n",
    "    Carga una lista desde un archivo usando deserializaci√≥n con pickle.\n",
    "\n",
    "    Par√°metros:\n",
    "        filename (str): El nombre del archivo desde el cual se cargar√° la lista.\n",
    "\n",
    "    Retorna:\n",
    "        list: La lista cargada.\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as file:\n",
    "        loaded_list = pickle.load(file)\n",
    "    return loaded_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Pipeline de datos**\n",
    "\n",
    "**Tokenizador**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for  _,text in data_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nota: los embeddings de GloVe normalmente se descargan usando:\n",
    "# glove_embedding = GloVe(name=\"6B\", dim=100)\n",
    "# Sin embargo, el servidor de GloVe frecuentemente est√° inactivo.\n",
    "# El c√≥digo siguiente ofrece una soluci√≥n alternativa.\n",
    "\n",
    "class GloVe_override(Vectors):\n",
    "    url = {\n",
    "        \"6B\": \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/tQdezXocAJMBMPfUJx_iUg/glove-6B.zip\",\n",
    "    }\n",
    "\n",
    "    def __init__(self, name=\"6B\", dim=100, **kwargs) -> None:\n",
    "        url = self.url[name]\n",
    "        name = \"glove.{}.{}d.txt\".format(name, str(dim))\n",
    "        #name = \"glove.{}/glove.{}.{}d.txt\".format(name, name, str(dim))\n",
    "        super(GloVe_override, self).__init__(name, url=url, **kwargs)\n",
    "\n",
    "class GloVe_override2(Vectors):\n",
    "    url = {\n",
    "        \"6B\": \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/tQdezXocAJMBMPfUJx_iUg/glove-6B.zip\",\n",
    "    }\n",
    "\n",
    "    def __init__(self, name=\"6B\", dim=100, **kwargs) -> None:\n",
    "        url = self.url[name]\n",
    "        #name = \"glove.{}.{}d.txt\".format(name, str(dim))\n",
    "        name = \"glove.{}/glove.{}.{}d.txt\".format(name, name, str(dim))\n",
    "        super(GloVe_override2, self).__init__(name, url=url, **kwargs)\n",
    "\n",
    "try:\n",
    "    glove_embedding = GloVe_override(name=\"6B\", dim=100)\n",
    "except:\n",
    "    try:\n",
    "        glove_embedding = GloVe_override2(name=\"6B\", dim=100)\n",
    "    except:\n",
    "        glove_embedding = GloVe(name=\"6B\", dim=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Importamos el conjunto de datos IMDB**\n",
    "\n",
    "El siguiente c√≥digo carga y manejo del conjunto de datos IMDB. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/35t-FeC-2uN1ozOwPs7wFg.gz')\n",
    "tar = tarfile.open(fileobj=io.BytesIO(urlopened.read()))\n",
    "tempdir = tempfile.TemporaryDirectory()\n",
    "tar.extractall(tempdir.name)\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, root_dir, train=True):\n",
    "        \"\"\"\n",
    "        root_dir: El directorio base del conjunto de datos IMDB.\n",
    "        train: Indicado booleano que indica si se debe usar datos de entrenamiento o de prueba.\n",
    "        \"\"\"\n",
    "        self.root_dir = os.path.join(root_dir, \"train\" if train else \"test\")\n",
    "        self.neg_files = [os.path.join(self.root_dir, \"neg\", f) for f in os.listdir(os.path.join(self.root_dir, \"neg\")) if f.endswith('.txt')]\n",
    "        self.pos_files = [os.path.join(self.root_dir, \"pos\", f) for f in os.listdir(os.path.join(self.root_dir, \"pos\")) if f.endswith('.txt')]\n",
    "        self.files = self.neg_files + self.pos_files\n",
    "        self.labels = [0] * len(self.neg_files) + [1] * len(self.pos_files)\n",
    "        self.pos_inx=len(self.pos_files)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.files[idx]\n",
    "        label = self.labels[idx]\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        return label, content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente c√≥digo utiliza la clase `IMDBDataset` definida anteriormente para crear iteradores de los conjuntos de datos de entrenamiento y prueba. Luego, muestra 20 ejemplos del conjunto de entrenamiento:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = tempdir.name + '/' + 'imdb_dataset'\n",
    "train_iter = IMDBDataset(root_dir=root_dir, train=True)  # Para datos de entrenamiento\n",
    "test_iter = IMDBDataset(root_dir=root_dir, train=False)  # Para datos de prueba\n",
    "\n",
    "start=train_iter.pos_inx\n",
    "for i in range(-10,10):\n",
    "    print(train_iter[start+i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente fragmento define el mapeo de etiquetas num√©ricas a rese√±as negativas y positivas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_label = {0: \" negative review\", 1: \"positive review\"}\n",
    "imdb_label[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por √∫ltimo, este c√≥digo verifica que en el conjunto de entrenamiento existan exactamente dos clases:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "num_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente c√≥digo carga un tokenizador b√°sico en ingl√©s y define una funci√≥n llamada `yield_tokens` que utiliza el tokenizador para descomponer datos de texto proporcionados por un iterador en tokens:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    \"\"\"Devuelve tokens para cada muestra de datos.\"\"\"\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente c√≥digo carga un modelo de embeddings de palabras preentrenado llamado GloVe en una variable llamada `glove_embedding`:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente c√≥digo construye un objeto vocabulario a partir de un modelo de incrustaciones de palabras GloVe preentrenado y establece el √≠ndice predeterminado en el token `<unk>`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe,vocab\n",
    "# Construye el vocabulario a partir de glove_embedding.stoi\n",
    "vocab = vocab(glove_embedding .stoi, 0,specials=('<unk>', '<pad>'))\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contemos el n√∫mero de palabras en el vocabulario:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probemos la funci√≥n `vocab`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab([\"age\",\"hello\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **Divisi√≥n del conjunto de datos**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convierte los iteradores de entrenamiento y prueba en conjuntos de datos de estilo map.\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "# Determina el n√∫mero de muestras para entrenamiento y validaci√≥n (5 % para validaci√≥n).\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "\n",
    "# Divide aleatoriamente el conjunto de entrenamiento en entrenamiento y validaci√≥n.\n",
    "# El conjunto de entrenamiento tendr√° el 95 % de las muestras y el de validaci√≥n el 5 % restante.\n",
    "split_train_, split_valid_ = random_split(\n",
    "    train_dataset,\n",
    "    [num_train, len(train_dataset) - num_train]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Para simular el proceso como si se tuviera GPU, reducimos a√∫n m√°s el tama√±o del conjunto de entrenamiento. Si deseas usar el conjunto IMDB completo, comenta o elimina las dos l√≠neas siguientes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = int(len(train_dataset) * 0.05)\n",
    "split_train_, _ = random_split(split_train_, [num_train, len(split_train_) - num_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente c√≥digo verifica si hay una GPU compatible con CUDA disponible usando PyTorch. Si existe, asigna `device = \"cuda\"`, de lo contrario `device = \"cpu\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cargador de datos**\n",
    "\n",
    "El siguiente c√≥digo prepara el pipeline de procesamiento de texto con el tokenizador y el vocabulario. \n",
    "\n",
    "La funci√≥n `text_pipeline` primero tokeniza el texto de entrada y luego aplica `vocab` para obtener los √≠ndices de los tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_pipeline(x):\n",
    "    return vocab(tokenizer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En PyTorch, la funci√≥n **`collate_fn`** se utiliza junto con los data loaders para personalizar la forma en que se crean los lotes a partir de muestras individuales. El c√≥digo proporcionado define una funci√≥n `collate_batch` en PyTorch, que se emplea con los data loaders para ajustar la creaci√≥n de lotes a partir de muestras individuales. Esta funci√≥n procesa un lote de datos, incluyendo etiquetas y secuencias de texto. \n",
    "\n",
    "Aplica la funci√≥n `text_pipeline` para preprocesar el texto. Los datos resultantes se convierten en tensores de PyTorch y se devuelven como una tupla que contiene el tensor de etiquetas, el tensor de texto y un tensor de offsets que representa las posiciones iniciales de cada secuencia de texto dentro del tensor combinado.\n",
    "\n",
    "Adem√°s, la funci√≥n se asegura de que los tensores generados se muevan al dispositivo especificado (por ejemplo, GPU) para un c√°lculo m√°s eficiente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for _label, _text in batch:\n",
    "\n",
    "        label_list.append(_label)\n",
    "        text_list.append(torch.tensor(text_pipeline(_text), dtype=torch.int64))\n",
    "\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = pad_sequence(text_list, batch_first=True)\n",
    "\n",
    "    return label_list.to(device), text_list.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes convertir estos conjuntos de datos en data loaders aplicando `collate_fn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Comprobemos qu√© generan estos data loaders:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label,seqence=next(iter(valid_dataloader))\n",
    "label,seqence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Red neuronal**\n",
    "\n",
    "Este c√≥digo define una clase llamada `TextClassifier` que representa un clasificador de texto sencillo que utiliza:\n",
    "\n",
    "* una capa de embedding,\n",
    "* una capa lineal oculta con activaci√≥n ReLU,\n",
    "* y una capa lineal de salida.\n",
    "\n",
    "El constructor recibe los siguientes argumentos:\n",
    "\n",
    "* `num_class`: el n√∫mero de clases a clasificar.\n",
    "* `freeze`: indica si se debe congelar la capa de embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, freeze=False):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        # Capa de embedding precargada con vectores GloVe; opcionalmente congelada\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            glove_embedding.vectors.to(device),\n",
    "            freeze=freeze\n",
    "        )\n",
    "        # Ejemplo de capa adicional: capa lineal intermedia\n",
    "        self.fc1 = nn.Linear(in_features=100, out_features=128)\n",
    "        # Activaci√≥n ReLU tras la capa lineal\n",
    "        self.relu = nn.ReLU()\n",
    "        # Capa de salida que produce las puntuaciones (logits) para cada clase\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pasa la secuencia de √≠ndices por la capa de embedding\n",
    "        x = self.embedding(x)\n",
    "        # Aplica un pooling por media sobre la dimensi√≥n de secuencia\n",
    "        x = torch.mean(x, dim=1)\n",
    "        # Pasa los embeddings agrupados por la capa lineal intermedia‚Ä¶\n",
    "        x = self.fc1(x)\n",
    "        # ‚Ä¶y aplica la activaci√≥n ReLU\n",
    "        x = self.relu(x)\n",
    "        # Devuelve los logits finales para cada clase\n",
    "        return self.fc2(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Entrenamiento del modelo sobre todo el conjunto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo=TextClassifier(num_classes=2,freeze=True)\n",
    "modelo.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.eval()\n",
    "predicted_label=modelo(seqence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicted_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente funci√≥n **`predict`** toma como entradas un texto, una secuencia de texto y un modelo. Utiliza un modelo preentrenado que se pasa como par√°metro para predecir la etiqueta del texto para su clasificaci√≥n en el conjunto de datos IMDB:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, text_pipeline, modelo):\n",
    "    with torch.no_grad():\n",
    "        text = torch.unsqueeze(torch.tensor(text_pipeline(text)),0).to(device)\n",
    "        modelo.to(device)\n",
    "        output = modelo(text)\n",
    "        return imdb_label[output.argmax(1).item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"I like sports and stuff\", text_pipeline, modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Podemos crear una funci√≥n para evaluar la exactitud del modelo en un conjunto de datos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, modelo, device):\n",
    "    modelo.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for label, text in dataloader:\n",
    "            label, text = label.to(device), text.to(device)\n",
    "            outputs = modelo(text)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente c√≥digo eval√∫a el rendimiento del modelo (puede tardar unos 4 minutos en CPU). **Para mayor eficiencia, no ejecutaremos esta celda ahora**, pero puede descomentarse si se desea comprobar que el modelo sin entrenar no rinde mejor que el azar:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate(test_dataloader, modelo, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten en cuenta que el rendimiento actual del modelo no es mejor que el promedio. Este resultado es esperado, considerando que el modelo a√∫n no ha recibido ning√∫n entrenamiento.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Entrenamiento**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente c√≥digo define la funci√≥n de entrenamiento que se utiliza para entrenar tu modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(modelo, optimizer, criterion, train_dataloader, valid_dataloader, epochs=100, model_name=\"mi_modeldrop\"):\n",
    "    cum_loss_list = []\n",
    "    acc_epoch = []\n",
    "    best_acc = 0\n",
    "    file_name = model_name\n",
    "    \n",
    "    for epoch in tqdm(range(1, epochs + 1)):\n",
    "        modelo.train()\n",
    "        cum_loss = 0\n",
    "        for _, (label, text) in enumerate(train_dataloader):            \n",
    "            optimizer.zero_grad()\n",
    "            predicted_label = modelo(text)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(modelo.parameters(), 0.1)\n",
    "            optimizer.step()\n",
    "            cum_loss += loss.item()\n",
    "        #print(\"Funcion de perdida:\", cum_loss)\n",
    "        cum_loss_list.append(cum_loss)\n",
    "        acc_val = evaluate(valid_dataloader, modelo, device)\n",
    "        acc_epoch.append(acc_val)\n",
    "        \n",
    "        if acc_val > best_acc:\n",
    "            best_acc = acc_val\n",
    "            print(f\"Nueva mejor exactituf: {acc_val:.4f}\")\n",
    "            #torch.save(modelo.state_dict(), f\"{model_name}.pth\")\n",
    "    \n",
    "    #save_list_to_file(cum_loss_list, f\"{model_name}_loss.pkl\")\n",
    "    #save_list_to_file(acc_epoch, f\"{model_name}_acc.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "El siguiente c√≥digo establece la tasa de aprendizaje (LR) en 1, que determina el tama√±o del paso con el que el optimizador actualiza los par√°metros del modelo durante el entrenamiento. El criterio `CrossEntropyLoss` se utiliza para calcular la p√©rdida entre las salidas predichas por el modelo y las etiquetas reales. Esta funci√≥n de p√©rdida se emplea com√∫nmente en tareas de clasificaci√≥n multiclase.\n",
    "\n",
    "El optimizador elegido es Stochastic Gradient Descent (SGD), que ajusta los par√°metros del modelo en funci√≥n de los gradientes calculados respecto a la funci√≥n de p√©rdida. El optimizador SGD utiliza la tasa de aprendizaje especificada para controlar el tama√±o de las actualizaciones de los pesos.\n",
    "\n",
    "Adem√°s, se define un programador de tasa de aprendizaje mediante `StepLR`. Este programador ajusta la tasa de aprendizaje durante el entrenamiento, reduci√©ndola en un factor (`gamma`) de 0.1 despu√©s de cada √©poca (paso) para mejorar la convergencia y afinar el rendimiento del modelo. Estos componentes forman, en conjunto, la configuraci√≥n esencial para entrenar una red neuronal usando la tasa de aprendizaje, el criterio de p√©rdida, el optimizador y el programador de tasa de aprendizaje especificados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR=1\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(modelo.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se a preentrenado el modelo para 300 √©pocas con una GPU y se ha guardado para tu comodidad. Sin embargo, para ilustrar el proceso de entrenamiento, se ha incluido el siguiente c√≥digo que entrena el modelo solo para dos √©pocas. Ten en cuenta que ha limitado el n√∫mero de √©pocas a dos, ya que el entrenamiento en una CPU puede llevar mucho tiempo. Incluso con solo dos √©pocas, el siguiente c√≥digo puede tardar aproximadamente un minuto en ejecutarse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"model_imdb_freeze_true2\"\n",
    "train_model(modelo, optimizer, criterion, train_dataloader, valid_dataloader, epochs=2, model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lugar de evaluar el modelo entrenado en 2 √©pocas, carguemos el modelo preentrenado que fue entrenado durante 300 √©pocas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/ZvhVWJU0flC7BmU1jjYxjg/model-imdb-freeze-true2.pth\n",
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/2RdN-JG4Rm5Gx3UNtOP4NA/model-imdb-freeze-true2-acc.pkl\n",
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/8qoGvWk0BdXRGoFAOT-dAw/model-imdb-freeze-true2-loss.pkl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grafiquemos el costo y la exactitud de cada √©poca para el modelo preentrenado, que se entren√≥ para 300 √©pocas. El gr√°fico muestra que, con solo unas pocas √©pocas, la exactitud presenta una volatilidad significativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_loss_list=load_list_from_file(model_name.replace('_','-') + \"-loss.pkl\")\n",
    "acc_epoch=load_list_from_file(model_name.replace('_','-') + \"-acc.pkl\")\n",
    "plot(cum_loss_list,acc_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el modelo que se ha entrenado. Si deseas entrenar el modelo tu mismo comente estas l√≠neas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.load_state_dict(torch.load(model_name.replace('_','-') + \".pth\", map_location=device))\n",
    "modelo.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuaci√≥n, se eval√∫a el modelo con los datos de prueba. El modelo preentrenado alcanza una exactitud del 66 %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(test_dataloader , modelo, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Aplicando LoRA**\n",
    "\n",
    "Para afinar con LoRA, primero carga un modelo **TextClassifier** preentrenado con LoRA (mientras congelas sus capas), carga su estado preentrenado desde un archivo y luego deshabilita las actualizaciones de gradiente para todos sus par√°metros para evitar entrenamiento adicional.\n",
    "\n",
    "Aqu√≠, cargare,ps un modelo que fue preentrenado en el conjunto de datos AG NEWS, el cual tiene 4 clases. \n",
    "\n",
    "Observa que cuando inicializas este modelo, estableces `num_classes` en 4. Adem√°s, el modelo AG\\_News preentrenado se entren√≥ con la capa de embedding sin congelar. Por lo tanto, inicializar√°s el modelo con `freeze=False`. Aunque est√©s inicializando el modelo con capas sin congelar y con un n√∫mero incorrecto de clases para tu tarea, har√°s modificaciones m√°s adelante que corregir√°n esto:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import io\n",
    "\n",
    "model_lora=TextClassifier(num_classes=4,freeze=False)\n",
    "model_lora.to(device)\n",
    "\n",
    "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/uGC04Pom651hQs1XrZ0NsQ/my-model-freeze-false.pth')\n",
    "\n",
    "stream = io.BytesIO(urlopened.read())\n",
    "state_dict = torch.load(stream, map_location=device)\n",
    "model_lora.load_state_dict(state_dict)\n",
    "\n",
    "# Aqu√≠, congelas todas las capas:\n",
    "for parm in model_lora.parameters():\n",
    "    parm.requires_grad=False\n",
    "model_lora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten en cuenta que el bucle `for` en el c√≥digo anterior congel√≥ todas las capas de la red neuronal, incluyendo la capa de embedding.\n",
    "\n",
    "Adem√°s, observa que el modelo original resolv√≠a un problema de clasificaci√≥n con cuatro clases, mientras que el conjunto de datos IMDB solo tiene 2 clases. Para tener esto en cuenta, reemplazamos la capa final por una nueva capa lineal donde el n√∫mero de salidas sea igual a 2:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lora.fc2=nn.Linear(in_features=128, out_features=2, bias=True).to(device)\n",
    "model_lora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos todos los m√≥dulos en el objeto:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tu tarea ahora es reemplazar la capa oculta por una capa LoRA. Puedes acceder a la capa oculta de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lora.fc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo siguiente reemplaza esta capa por una capa LoRA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lora.fc1=LinearWithLoRA(model_lora.fc1,rank=2, alpha=0.1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos nuevamente la capa oculta para asegurarnos de que efectivamente se haya convertido en una capa LoRA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lora.fc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto, entrenar el modelo es similar, con la √∫nica diferencia de que, salvo la capa de salida, solo los par√°metros aprendibles\n",
    "`A` y `B` se actualizar√°n. El c√≥digo para seleccionar los valores de `r` y `alpha`, que no se ejecuta, se proporciona aqu√≠ para tu conveniencia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lora.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details> \n",
    "<summary><b>Haz clic aqu√≠ para ver el c√≥digo para seleccionar r y alpha</b></summary>\n",
    "\n",
    "```python \n",
    "ranks = [1, 2, 5, 10]\n",
    "alphas = [0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "\n",
    "results = []\n",
    "accuracy_old = 0\n",
    "# Itera sobre cada combinaci√≥n de 'r' y 'alpha'\n",
    "for r in ranks:\n",
    "    for alpha in alphas:\n",
    "        print(f\"Pruebas con rank = {r} and alpha = {alpha}\")\n",
    "        model_name = f\"model_lora_rank{r}_alpha{alpha}_AGtoIBDM_final_adam_\"\n",
    "        \n",
    "        model_lora = TextClassifier(num_classes=4, freeze=False)\n",
    "        model_lora.to(device)\n",
    "        \n",
    "        urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/uGC04Pom651hQs1XrZ0NsQ/my-model-freeze-false.pth')\n",
    "        \n",
    "        stream = io.BytesIO(urlopened.read())\n",
    "        state_dict = torch.load(stream, map_location=device)\n",
    "        model_lora.load_state_dict(state_dict)\n",
    "        \n",
    "        for parm in model_lora.parameters():\n",
    "            parm.requires_grad = False\n",
    "        \n",
    "        model_lora.fc2 = nn.Linear(in_features=128, out_features=2, bias=True)\n",
    "        model_lora.fc1 = LinearWithLoRA(model_lora.fc1, rank=r, alpha=alpha)\n",
    "        optimizer = torch.optim.Adam(model_lora.parameters(), lr=LR)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n",
    "        \n",
    "        model_lora.to(device)\n",
    "        \n",
    "        train_model(model_lora, optimizer, criterion, train_dataloader, valid_dataloader, epochs=300, model_name=model_name)\n",
    "        \n",
    "        accuracy = evaluate(valid_dataloader, model_lora, device)\n",
    "        result = {\n",
    "            'rank': r,\n",
    "            'alpha': alpha,\n",
    "            'accuracy': accuracy\n",
    "        }\n",
    "\n",
    "        # Agrega el diccionario a la lista de resultados\n",
    "        results.append(result)\n",
    "        \n",
    "        if accuracy > accuracy_old:\n",
    "            print(f\"Pruebas con rank = {r} and alpha = {alpha}\")\n",
    "            print(f\"Exactitud: {accuracy} accuracy_old: {accuracy_old}\")\n",
    "            accuracy_old = accuracy\n",
    "            torch.save(model.state_dict(), f\"{model_name}.pth\")\n",
    "            save_list_to_file(cum_loss_list, f\"{model_name}_loss.pkl\")\n",
    "            save_list_to_file(acc_epoch, f\"{model_name}_acc.pkl\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, configuremos los componentes de entrenamiento para el modelo `model_lora`, definiendo una tasa de aprendizaje de 1, usando la p√©rdida de entrop√≠a cruzada como criterio, optimizando con descenso de gradiente estoc√°stico (SGD) y programando la tasa de aprendizaje para que decaiga en un factor de 0.1 en cada √©poca:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR=1\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_lora.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ha preentrenado un modelo usando un procedimiento id√©ntico durante 300 √©pocas y lo guardaste para tu conveniencia. Sin embargo, para que veas c√≥mo funciona el entrenamiento en la pr√°ctica, ejecuta el siguiente c√≥digo para entrenar el modelo solo durante 2 √©pocas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"model_lora_final2\"\n",
    "train_model(model_lora,optimizer, criterion, train_dataloader, valid_dataloader, epochs=2, model_name=model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lugar de evaluar el modelo que acabas de entrenar por 2 √©pocas, veamos el modelo LoRA preentrenado con 300 √©pocas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/JWPRb1RMhKLRMUWOKw9pxA/model-lora-final2.pth\n",
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/_dm02rLyTrwsXEQh2r32sQ/model-lora-final2-acc.pkl\n",
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/OZbVqKjoqOSIwnET8AB1KA/model-lora-final2-loss.pkl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo siguiente muestra la progresi√≥n del entrenamiento de este modelo durante 300 √©pocas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_loss_list=load_list_from_file(model_name.replace('_','-') + \"-loss.pkl\")\n",
    "acc_epoch=load_list_from_file(model_name.replace('_','-') + \"-acc.pkl\")\n",
    "plot(cum_loss_list,acc_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, carguemos realmente el modelo en `model_lora`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lora.load_state_dict(torch.load(model_name.replace('_','-') + \".pth\", map_location=device))\n",
    "model_lora.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y evaluemos su rendimiento en los datos de prueba:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtienes una mejora del 3% sobre un modelo entrenado desde cero al usar LoRA. Ten en cuenta que esto ocurre a pesar de que el modelo afinado con LoRA actualiz√≥ menos par√°metros que el modelo entrenado desde cero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El atributo `model_lora.fc1` representa `LinearWithLoRA`, que contiene tanto la capa est√°ndar `Linear` (`linear`) como una capa adicional `LoRA` (`lora`), la cual representa `LoRALayer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lora.fc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desde `model_lora.fc1.lora` puedes obtener los par√°metros aprendibles **A** y **B**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B=model_lora.fc1.lora.B\n",
    "print(\"B\",B)\n",
    "print(\"\\n Numero de elementos en el tensor B\",B.numel())\n",
    "torch.save(B, 'B.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=model_lora.fc1.lora.A\n",
    "print(\"A\",A)\n",
    "print(\"\\n Numero de elementos en el tensor A\",A.numel())\n",
    "torch.save(A, 'A.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A y B tienen aproximadamente 450 par√°metros. Si guardaras la capa lineal completa, tendr√≠as 12,800 par√°metros, lo que es alrededor de 28 veces m√°s. \n",
    "\n",
    "Recuerda, este es posiblemente el modelo m√°s simple que puedas tener."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Numero de elementos en el tensor A\",model_lora.fc1.linear.weight.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alfa y la capa de salida tambi√©n se guardan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alfa_=model_lora.fc1.lora.alpha\n",
    "torch.save(alfa_, 'alfa_.pth')\n",
    "torch.save(model_lora.fc2.state_dict(), 'out_layer.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cargando el modelo**\n",
    "\n",
    "La principal ventaja de LoRA es que para el fine-tuning solo necesitas guardar los par√°metros entrenables A y B, el factor Œ±, y la capa de salida en tu ejemplo de clasificaci√≥n.\n",
    "\n",
    "Los archivos guardados se convierten en tensores y la capa lineal, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.load('A.pth')\n",
    "print(\"A:\",A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = torch.load('B.pth')\n",
    "print(\"B:\",B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alfa_ = torch.load('alfa_.pth')\n",
    "alfa_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La capa de salida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer=nn.Linear(in_features=128, out_features=2, bias=True)\n",
    "output_layer.load_state_dict(torch.load('out_layer.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea el objeto del modelo y se cargan los par√°metros preentrenados:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_load_lora = TextClassifier(num_classes=4,freeze=False)\n",
    "model_load_lora.to(device)\n",
    "\n",
    "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/uGC04Pom651hQs1XrZ0NsQ/my-model-freeze-false.pth')\n",
    "\n",
    "stream = io.BytesIO(urlopened.read())\n",
    "state_dict = torch.load(stream, map_location=device)\n",
    "model_load_lora.load_state_dict(state_dict)\n",
    "\n",
    "model_load_lora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se a√±ade la capa LoRA a la capa oculta original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_load_lora.fc1=LinearWithLoRA(model_load_lora.fc1,rank=2, alpha=0.1)\n",
    "model_load_lora.fc2=nn.Linear(in_features=128, out_features=2, bias=True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se incorporan los par√°metros obtenidos del fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_load_lora.fc1.lora.A=A\n",
    "model_load_lora.fc1.lora.B=B\n",
    "model_load_lora.fc1.lora.alpha=alfa_ \n",
    "model_load_lora.fc2=output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_load_lora.to(device)\n",
    "model_load_lora.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(test_dataloader , model_load_lora, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto confirma que el modelo se carg√≥ correctamente. A√∫n obtienes una mejora del 3 % en exactitud!."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se muestra c√≥mo hacer una predicci√≥n sobre el siguiente art√≠culo utilizando la funci√≥n **`predict`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article=\"\"\"This was a lacklustre movie with very little going for it. I was not impressed.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este contenido en Markdown genera un recuadro con fondo gris claro y padding. Contiene un encabezado `<h3>` que muestra el contenido de la variable `article` y un `<h4>` que indica la categor√≠a predicha del art√≠culo, proporcionada por la variable `result`. Los marcadores `{article}` y `{result}` se reemplazar√°n din√°micamente con los valores reales al renderizarse el Markdown.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict(article, model_load_lora, text_pipeline)\n",
    "\n",
    "markdown_content = f'''\n",
    "<div style=\"background-color: lightgray; padding: 10px;\">\n",
    "    <h3>{article}</h3>\n",
    "    <h4>The category of the news article: {result}</h4>\n",
    "</div>\n",
    "'''\n",
    "\n",
    "md(markdown_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Ejercicio: Aplicar LoRA a una red distinta para reconocimiento de letras**\n",
    "\n",
    "Partiendo de la clase `NNet` (dise√±ada originalmente para d√≠gitos 0‚Äì9), tu tarea es:\n",
    "\n",
    "1. **Reemplazar la √∫ltima capa** para que la red tenga 26 salidas (una por cada letra del alfabeto ingl√©s).\n",
    "2. **Congelar todos los par√°metros originales** y **aplicar LoRA** √∫nicamente a la segunda capa lineal (`fc2`), usando rango 2 y factor de escala $\\alpha=0.1$.\n",
    "3. \n",
    "4. Verificar que s√≥lo los par√°metros de las capas LoRA y de la nueva capa de salida est√©n marcados para entrenamiento (`requires_grad=True`).\n",
    "\n",
    "**C√≥digo base con la secci√≥n de LoRA ya integrada**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Asumimos que LinearWithLoRA est√° definido/importado correctamente:\n",
    "# from lora import LinearWithLoRA\n",
    "\n",
    "class NNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNet, self).__init__()\n",
    "        # Capa de convoluci√≥n C1: 1 canal de entrada, 6 de salida, kernel 5x5\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        # Capa de convoluci√≥n C3: 6 canales de entrada, 16 de salida, kernel 5x5\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # Capa totalmente conectada F5: de 400 a 120 neuronas\n",
    "        self.fc1  = nn.Linear(16 * 5 * 5, 120)\n",
    "        # Capa totalmente conectada F6: de 120 a 84 neuronas\n",
    "        self.fc2  = nn.Linear(120, 84)\n",
    "        # Capa de salida original (se reemplazar√° despu√©s)\n",
    "        self.fc3  = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # C1 + ReLU -> tama√±o (N, 6, 28, 28)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        # S2 Pooling 2x2 -> tama√±o (N, 6, 14, 14)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        # C3 + ReLU -> tama√±o (N, 16, 10, 10)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        # S4 Pooling 2x2 -> tama√±o (N, 16, 5, 5)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        # Aplanamiento -> tama√±o (N, 400)\n",
    "        x = torch.flatten(x, 1)\n",
    "        # F5 + ReLU -> tama√±o (N, 120)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # F6 + ReLU -> tama√±o (N, 84)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # Capa de salida -> tama√±o (N, 26) despu√©s del reemplazo\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Instanciar y mover a dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_exercise = NNet().to(device)\n",
    "\n",
    "print('Modelo antes de aplicar LoRA:')\n",
    "print(model_exercise)\n",
    "print(\"\\n###############\\n\")\n",
    "\n",
    "# 1) Congela todos los par√°metros originales\n",
    "for param in model_exercise.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 2) Reemplaza la capa final por una con 26 salidas (A‚ÄìZ)\n",
    "model_exercise.fc3 = nn.Linear(in_features=84, out_features=26, bias=True).to(device)\n",
    "\n",
    "# 3) Aplica LoRA solo a la segunda capa lineal (fc2)\n",
    "### REEMPLAZAR AQU√ç ###\n",
    "# Sustituimos fc2 por una versi√≥n que incluye adaptadores LoRA\n",
    "model_exercise.fc2 = LinearWithLoRA(\n",
    "    original_linear = model_exercise.fc2,\n",
    "    rank            = 2,\n",
    "    alpha           = 0.1\n",
    ").to(device)\n",
    "### FIN REEMPLAZO ###\n",
    "\n",
    "print('Modelo despu√©s de aplicar LoRA:')\n",
    "print(model_exercise)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tu respuesta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "prev_pub_hash": "76d39c215db0c382687ee9cbd1add55a12e9d224c61198bd34619b63d8828b90"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
