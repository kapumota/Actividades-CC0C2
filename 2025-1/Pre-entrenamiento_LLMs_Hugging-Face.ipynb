{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Preentrenamiento de LLMs con Hugging Face**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuración\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalación de librerías requeridas\n",
    "\n",
    "Las siguientes librerías están **preinstaladas** en el entorno del curso. Sin embargo, si ejecutas estos comandos en otro entorno de Jupyter (por ejemplo, Watson Studio o Anaconda), deberás quitar el `#` antes de `!pip` en las celdas de código para instalarlas:\n",
    "\n",
    "*PS: Para ejecutar este cuaderno en tu propio entorno, ten en cuenta que las versiones de las librerías pueden variar según las dependencias.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todas las bibliotecas necesarias para este laboratorio están listadas a continuación. \n",
    "# Las bibliotecas preinstaladas  están comentadas.\n",
    "# !pip install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 torch==2.1.0+cu118\n",
    "# - Actualizar un paquete específico\n",
    "# !pip install pmdarima -U\n",
    "# - Actualar un paquete a una versión concreta\n",
    "# !pip install --upgrade pmdarima==2.0.2\n",
    "# Nota: Si tu entorno no soporta \"!pip install\", usa \"!mamba install\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las siguientes librerías **no** están preinstaladas. **Debes ejecutar la siguiente celda** para instalarlas:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers==4.40.0 \n",
    "#!pip install -U git+https://github.com/huggingface/transformers\n",
    "#!pip install datasets # 2.15.0\n",
    "#!pip install portalocker>=2/0.0\n",
    "#!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "#!pip install torch==2.3.0\n",
    "#!pip install -U torchvision\n",
    "#!pip install protobuf==3.20.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importación de librerías requeridas\n",
    "\n",
    "*Se recomienda importar todas las librerías necesarias en un solo lugar (aquí):*\n",
    "\n",
    "* **Nota**: si obtienes un error tras ejecutar la celda, intenta reiniciar el kernel; algunos paquetes necesitan reinicio para surtir efecto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoConfig,AutoModelForCausalLM,AutoModelForSequenceClassification,BertConfig,BertForMaskedLM,TrainingArguments, Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer,BertTokenizerFast,TextDataset,DataCollatorForLanguageModeling\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "# Sección para suprimir advertencias generadas por el código:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desactiva el paralelismo de los tokenizadores para evitar bloqueos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establece la variable de entorno TOKENIZERS_PARALLELISM a 'false'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preentrenamiento y Fine-Tuning auto-supervisado**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El preentrenamiento es una técnica en procesamiento de lenguaje natural (NLP) que entrena LLMs en un gran corpus de texto no etiquetado. El objetivo es capturar patrones generales y relaciones semánticas del lenguaje natural, permitiendo al modelo comprender en profundidad la estructura y significado del lenguaje.\n",
    "\n",
    "La motivación de preentrenar transformers es superar las limitaciones de enfoques tradicionales de NLP, que requieren muchos datos etiquetados para cada tarea. Al aprovechar la abundancia de texto no etiquetado, el preentrenamiento permite al modelo aprender habilidades lingüísticas fundamentales mediante objetivos auto-supervisados, facilitando el aprendizaje por transferencia.\n",
    "\n",
    "Objetivos como el *masked language modeling* (MLM) y la *next sentence prediction* (NSP) son clave en el éxito de los transformers. Los modelos preentrenados pueden afinarse (fine-tuning) con datos sin etiquetar de un dominio específico (self-supervised fine-tuning) o con datos etiquetados para tareas concretas (supervised fine-tuning), mejorando aún más su rendimiento.\n",
    "\n",
    "En las siguientes secciones, explorarás los objetivos de preentrenamiento, cómo cargar modelos preentrenados, la preparación de datos y el proceso de fine-tuning. Al finalizar, comprenderás a fondo el preentrenamiento y el fine-tuning auto-supervisado, y estarás listo para aplicar estas técnicas en problemas reales de NLP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comencemos cargando un modelo preentrenado de Hugging Face y realizando una inferencia:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=modelo,tokenizer=tokenizer)\n",
    "print(pipe(\"This movie was really\")[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Objetivos de preentrenamiento**\n",
    "\n",
    "Los objetivos de preentrenamiento definen las tareas en las que el modelo se entrena durante esta fase, permitiéndole aprender representaciones contextuales profundas. Tres objetivos comunes son:\n",
    "\n",
    "1. **Masked Language Modeling (MLM)**\n",
    "   Consiste en enmascarar aleatoriamente algunas palabras en una oración y entrenar al modelo para predecirlas según el contexto circundante. El objetivo es que el modelo aprenda comprensión contextual y rellene la información faltante.\n",
    "\n",
    "2. **Next Sentence Prediction (NSP)**\n",
    "   Entrena al modelo para determinar si dos oraciones son consecutivas en el texto original o han sido emparejadas aleatoriamente. Ayuda a captar relaciones a nivel de oración y coherencia entre ellas.\n",
    "\n",
    "3. **Predicción del Siguiente Token (Next Token Prediction)**\n",
    "   El modelo recibe una secuencia de texto y aprende a predecir cuál es el siguiente token más probable basándose en el contexto anterior.\n",
    "\n",
    "Diferentes modelos pueden usar variaciones o combinaciones de estos objetivos según su arquitectura y configuración de entrenamiento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Entrenamiento auto-supervisado de un modelo BERT**\n",
    "\n",
    "Entrenar un modelo BERT es un proceso complejo y que requiere un gran corpus de texto sin etiquetar y recursos computacionales significativos. A continuación te presentamos un ejercicio simplificado para ilustrar los pasos involucrados en el preentrenamiento de un modelo BERT usando el objetivo de MLM (Masked Language Modeling).\n",
    "\n",
    "En este ejercicio utilizaremos la biblioteca Hugging Face Transformers, que provee modelos BERT ya implementados y herramientas para el preentrenamiento. Se te indicará que realices las siguientes tareas:\n",
    "\n",
    "* Preparar el conjunto de datos de entrenamiento\n",
    "* Entrenar un tokenizador\n",
    "* Preprocesar el conjunto de datos\n",
    "* Preentrenar BERT usando una tarea MLM\n",
    "* Evaluar el modelo entrenado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Importación de los conjuntos de datos necesarios**\n",
    "\n",
    "El conjunto de datos WikiText es un benchmark muy utilizado en procesamiento de lenguaje natural (NLP). Contiene texto extraído de artículos de Wikipedia, limpiado para eliminar formato, enlaces y metadatos, resultando en un corpus de texto \"crudo\".\n",
    "\n",
    "WikiText ofrece 4 configuraciones diferentes y se divide en tres partes: entrenamiento, validación y prueba. El conjunto de entrenamiento sirve para entrenar los modelos de lenguaje, mientras que los de validación y prueba se utilizan para evaluar su desempeño. Primero, carguemos los datos y concatenémoslos para crear un único conjunto de datos.\n",
    "\n",
    "*Nota: El BERT original se preentrenó sobre los conjuntos de datos de Wikipedia y BookCorpus.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga el conjunto de datos\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos la estructura del conjunto de datos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisemos un registro de ejemplo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Revisa un registro de ejemplo\n",
    "dataset[\"train\"][400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este conjunto contiene 36 718 filas de datos de entrenamiento. Si no dispones de un entorno con GPU, tal vez necesites reducir el tamaño del dataset. Puedes descomentar las siguientes líneas para seleccionar solo una parte:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset[\"train\"] = dataset[\"train\"].select([i for i in range(1000)])\n",
    "#dataset[\"test\"] = dataset[\"test\"].select([i for i in range(200)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, guardamos los textos en archivos de texto para crear objetos `TextDataset`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rutas para guardar los datasets en archivos de texto\n",
    "output_file_train = \"wikitext_dataset_train.txt\"\n",
    "output_file_test  = \"wikitext_dataset_test.txt\"\n",
    "\n",
    "# Guarda el conjunto de entrenamiento en un archivo de texto\n",
    "with open(output_file_train, \"w\", encoding=\"utf-8\") as f:\n",
    "    for example in dataset[\"train\"]:\n",
    "        # Escribe cada texto en una nueva línea\n",
    "        f.write(example[\"text\"] + \"\\n\")\n",
    "\n",
    "# Guarda el conjunto de prueba en un archivo de texto\n",
    "with open(output_file_test, \"w\", encoding=\"utf-8\") as f:\n",
    "    for example in dataset[\"test\"]:\n",
    "        # Escribe cada texto en una nueva línea\n",
    "        f.write(example[\"text\"] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debes definir un tokenizador para convertir tu texto en tokens numéricos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea un tokenizador BERT reutilizando tokens especiales de uno preentrenado\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "modelo = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, is_decoder=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Entrenamiento de un tokenizador (opcional)**\n",
    "\n",
    "En la celda anterior creaste una instancia de un tokenizador a partir de un tokenizador BERT preentrenado. Si quieres entrenar el tokenizador con tu propio conjunto de datos, puedes descomentar el código que aparece a continuación. Esto es especialmente útil cuando usas Transformers en áreas específicas, como la medicina, donde los tokens son de algún modo diferentes a los tokens generales en los que se basan los tokenizadores preexistentes. \n",
    "\n",
    "(Puedes omitir este paso si no deseas entrenar el tokenizador con tus datos específicos).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## crea un generador de Python para cargar los datos de forma dinámica\n",
    "def batch_iterator(batch_size=10000):\n",
    "    for i in tqdm(range(0, len(dataset), batch_size)):\n",
    "        yield dataset['train'][i : i + batch_size][\"text\"]\n",
    "\n",
    "## crea un tokenizador a partir de uno existente para reutilizar los tokens especiales\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "## entrena el tokenizador usando nuestro propio conjunto de datos\n",
    "bert_tokenizer = bert_tokenizer.train_new_from_iterator(text_iterator=batch_iterator(), vocab_size=30522)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preentrenamiento**\n",
    "\n",
    "En este paso, definimos la configuración del modelo BERT y creamos el modelo:\n",
    "\n",
    "#### Definir la configuración de BERT\n",
    "\n",
    "Aquí definimos los parámetros de configuración de un modelo BERT usando `BertConfig`. Esto incluye ajustar varios parámetros relacionados con la arquitectura del modelo:\n",
    "\n",
    "* **vocab\\_size=30522**: Especifica el tamaño del vocabulario. Este número debe coincidir con el tamaño de vocabulario usado por el tokenizador.\n",
    "* **hidden\\_size=768**: Establece el tamaño de las capas ocultas.\n",
    "* **num\\_hidden\\_layers=12**: Determina el número de capas ocultas en el modelo Transformer.\n",
    "* **num\\_attention\\_heads=12**: Establece el número de cabeceras de atención en cada capa de atención.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define la configuración de BERT\n",
    "config = BertConfig(\n",
    "    vocab_size=len(bert_tokenizer.get_vocab()),  # Especifica el tamaño del vocabulario (asegúrate de que este número sea igual al vocab_size del tokenizador)\n",
    "    hidden_size=768,                            # Establece el tamaño de las capas ocultas\n",
    "    num_hidden_layers=12,                       # Establece el número de capas ocultas\n",
    "    num_attention_heads=12,                     # Establece el número de cabeceras de atención\n",
    "    intermediate_size=3072,                     # Establece el tamaño de la capa intermedia\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crea el modelo BERT para preentrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea el modelo BERT para preentrenamiento\n",
    "modelo = BertForMaskedLM(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifica la configuración del modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifica la configuración del modelo\n",
    "modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Definición del conjunto de datos de entrenamiento**\n",
    "\n",
    "Aquí definimos un conjunto de datos de entrenamiento usando la clase `TextDataset`, que sirve para cargar y procesar texto para entrenar modelos de lenguaje. Esta configuración típicamente implica:\n",
    "\n",
    "* **tokenizer=bert\\_tokenizer**: Especifica el tokenizador a usar. `bert_tokenizer` convierte el texto en tokens comprensibles por el modelo.\n",
    "* **file\\_path=\"wikitext\\_dataset\\_train.txt\"**: Ruta al archivo de datos de preentrenamiento.\n",
    "* **block\\_size=128**: Define la longitud de las secuencias en las que el modelo será entrenado.\n",
    "\n",
    "La clase `TextDataset` está diseñada para tomar grandes fragmentos de texto (como los que se encuentran en el archivo especificado), tokenizarlos y procesarlos de manera eficiente en bloques manejables del tamaño indicado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara los datos de preentrenamiento como un TextDataset\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=bert_tokenizer,\n",
    "    file_path=\"wikitext_dataset_train.txt\",  # Ruta al archivo de datos de preentrenamiento\n",
    "    block_size=128                           # Establece el tamaño de bloque deseado para el entrenamiento\n",
    ")\n",
    "test_dataset = TextDataset(\n",
    "    tokenizer=bert_tokenizer,\n",
    "    file_path=\"wikitext_dataset_test.txt\",   # Ruta al archivo de datos de prueba\n",
    "    block_size=128                           # Establece el tamaño de bloque deseado para el entrenamiento\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al examinar una muestra, los índices de los tokens se muestran aquí con el tamaño de bloque:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, preparamos los datos para la tarea MLM (enmascaramiento de tokens aleatorios):\n",
    "\n",
    "#### **Definir el data collator para modelado de lenguaje**\n",
    "\n",
    "Esta línea de código configura un `DataCollatorForLanguageModeling` de la librería Hugging Face Transformers. Un *data collator* se utiliza durante el entrenamiento para crear lotes de datos de forma dinámica. \n",
    "\n",
    "Para el modelado de lenguaje, especialmente para modelos como BERT que emplean *masked language modeling* (MLM), este collator prepara los lotes de entrenamiento enmascarando automáticamente tokens según una probabilidad especificada. A continuación, los detalles de los parámetros utilizados:\n",
    "\n",
    "* **tokenizer=bert\\_tokenizer**: Especifica el tokenizador que usará el data collator. El `bert_tokenizer` se encarga de tokenizar el texto y convertirlo al formato que espera el modelo.\n",
    "* **mlm=True**: Indica que el data collator debe enmascarar tokens para el entrenamiento de masked language modeling. Al activarse, el collator enmascara aleatoriamente algunos tokens de los datos de entrada, los cuales el modelo intentará predecir.\n",
    "* **mlm\\_probability=0.15**: Establece la probabilidad con la que se enmascararán los tokens. Una probabilidad de 0.15 significa que, en promedio, el 15 % de los tokens de cada secuencia serán reemplazados por el token de máscara.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara el data collator para modelado de lenguaje\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=bert_tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica cómo el collator transforma un registro de datos de ejemplo\n",
    "data_collator([train_dataset[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora entrenamos el modelo BERT usando el módulo `Trainer`. (Para ver la lista completa de argumentos de entrenamiento, consulta [aquí](https://huggingface.co/docs/transformers/v4.33.2/en/main_classes/trainer#transformers.TrainingArguments)):\n",
    "\n",
    "Esta sección configura el proceso de entrenamiento indicando diversos parámetros que controlan cómo se entrena, evalúa y guarda el modelo:\n",
    "\n",
    "* **output\\_dir=\"./trained\\_model\"**: Especifica el directorio donde se guardará el modelo entrenado y otros archivos de salida.\n",
    "* **overwrite\\_output\\_dir=True**: Si se establece en `True`, sobrescribirá el contenido del directorio de salida si ya existe. Esto resulta útil al ejecutar experimentos varias veces.\n",
    "* **do\\_eval=True**: Habilita la evaluación del modelo. Si es `True`, el modelo se evaluará en los intervalos especificados.\n",
    "* **evaluation\\_strategy=\"epoch\"**: Define cuándo debe evaluarse el modelo. Al ponerlo en `\"epoch\"`, se evaluará al final de cada época.\n",
    "* **learning\\_rate=5e-5**: Establece la tasa de aprendizaje para entrenar el modelo. Es un valor típico para ajustar modelos tipo BERT.\n",
    "* **num\\_train\\_epochs=10**: Especifica el número de épocas de entrenamiento. Cada época corresponde a un pase completo sobre los datos de entrenamiento.\n",
    "* **per\\_device\\_train\\_batch\\_size=2**: Fija el tamaño de lote para el entrenamiento en cada dispositivo. Debe ajustarse según la memoria disponible de tu hardware.\n",
    "* **save\\_total\\_limit=2**: Limita el número total de puntos de control (checkpoints) que se guardarán. Solo se conservarán los dos más recientes.\n",
    "* **logging\\_steps=20**: Determina cada cuántos pasos de entrenamiento se registrará información, lo cual ayuda a supervisar el proceso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Define los argumentos de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./trained_model\",      # Especificar el directorio de salida para el modelo entrenado\n",
    "    overwrite_output_dir=True,         # Sobrescribir el contenido del directorio de salida si ya existe\n",
    "    do_eval=True,                      # Realizar evaluación durante el entrenamiento\n",
    "    evaluation_strategy=\"epoch\",       # Estrategia de evaluación: al final de cada época\n",
    "    learning_rate=5e-5,                # Tasa de aprendizaje\n",
    "    num_train_epochs=10,               # Especificar el número de épocas de entrenamiento\n",
    "    per_device_train_batch_size=2,     # Tamaño de lote por dispositivo durante el entrenamiento\n",
    "    save_total_limit=2,                # Límite máximo de puntos de control guardados\n",
    "    logging_steps=20                   # Registrar información cada 20 pasos\n",
    ")\n",
    "\n",
    "# Instancia el Trainer\n",
    "trainer = Trainer(\n",
    "    model=modelo,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Inicia el preentrenamiento\n",
    "trainer.train()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Evaluación del rendimiento del modelo**\n",
    "\n",
    "Comprobemos cómo se desempeña el modelo entrenado. La **perplejidad** (perplexity) se usa comúnmente para comparar diferentes modelos de lenguaje o configuraciones de un mismo modelo. Después del entrenamiento, la perplejidad se puede calcular sobre un conjunto de evaluación reservado para medir el rendimiento. Se calcula alimentando el conjunto de evaluación al modelo y comparando las probabilidades predichas de los tokens objetivo con los valores reales de los tokens enmascarados.\n",
    "\n",
    "Una puntuación de perplejidad más baja indica que el modelo entiende mejor el lenguaje y es más eficaz prediciendo los tokens enmascarados. Esto sugiere que el modelo ha aprendido representaciones útiles y puede generalizar bien a datos no vistos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''eval_results = trainer.evaluate()\n",
    "print(f\"Perplejidad: {math.exp(eval_results['eval_loss']):.2f}\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga del modelo guardado\n",
    "\n",
    "Si deseas omitir el entrenamiento y cargar el modelo que entrenaste durante 10 épocas, descomenta la siguiente celda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/BeXRxFT2EyQAmBHvxVaMYQ/bert-scratch-model.pt'\n",
    "#modelo.load_state_dict(torch.load('bert-scratch-model.pt',map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La forma más sencilla de probar el modelo en modo inferencia es usarlo en un `pipeline()`. Instancia un pipeline para la tarea *fill-mask* con tu modelo y pásale el texto. Si lo deseas, puedes usar el parámetro `top_k` para especificar cuántas predicciones devolver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define el texto de entrada con un token enmascarado\n",
    "text = \"This is a [MASK] movie!\"\n",
    "\n",
    "# Crea un pipeline para la tarea \"fill-mask\"\n",
    "mask_filler = pipeline(\"fill-mask\", model=modelo, tokenizer=bert_tokenizer)\n",
    "\n",
    "# Genera predicciones rellenando el token enmascarado\n",
    "results = mask_filler(text)  # Se puede especificar top_k\n",
    "\n",
    "# Imprime las secuencias predichas\n",
    "for result in results:\n",
    "    print(f\"Token predicho: {result['token_str']}, Confianza: {result['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verás que `[MASK]` se reemplaza por el token más frecuente. Este rendimiento limitado puede deberse a un entrenamiento insuficiente, falta de datos, arquitectura del modelo o a no ajustar correctamente los hiperparámetros. \n",
    "\n",
    "Probemos ahora con un modelo preentrenado de Hugging Face:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Inferencia con un modelo BERT preentrenado**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo y tokenizador BERT preentrenados\n",
    "pretrained_model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "pretrained_tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Definir el texto de entrada con un token enmascarado\n",
    "text = \"This is a [MASK] movie!\"\n",
    "\n",
    "# Crear el pipeline para \"fill-mask\"\n",
    "mask_filler = pipeline(task='fill-mask', model=pretrained_model, tokenizer=pretrained_tokenizer)\n",
    "\n",
    "# Realizar inferencia usando el pipeline\n",
    "results = mask_filler(text)\n",
    "for result in results:\n",
    "    print(f\"Token predicho: {result['token_str']}, Confianza: {result['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este modelo preentrenado funciona mucho mejor que el modelo que entrenaste solo unas pocas épocas con un único conjunto de datos. Aun así, los modelos preentrenados no están diseñados para tareas específicas como extracción de sentimiento o clasificación de secuencias. Por eso se introducen métodos de **fine-tuning supervisado**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicios**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Crea un modelo y un tokenizador usando la librería Hugging Face.\n",
    "2. Visita este [enlace](https://huggingface.co/datasets?task_categories=task_categories:text-classification&sort=trending).\n",
    "3. Elige un conjunto de datos de clasificación de texto que puedas cargar, por ejemplo `stanfordnlp/snli`.\n",
    "4. Utiliza ese conjunto de datos para entrenar tu modelo (ten en cuenta los recursos disponibles para el entrenamiento) y evalúalo.\n",
    "\n",
    "> **Nota:** El entorno del cuaderno no dispone de recursos suficientes para soportar entrenamientos pesados y esto podría causar que el kernel deje de funcionar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "prev_pub_hash": "d29458bb8fd401d00186b91d1862005b641aee6fb66e97a6a065470e2b2b2981"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
